{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CjRWziAVU2lZ"
   },
   "source": [
    "# Unit 4: Programmieren Sie Ihren ersten Deep Reinforcement Learning Algorithmus mit PyTorch: Reinforce. Und teste seine Robustheit 💪.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/thumbnail.png\" alt=\"thumbnail\"/>\n",
    "\n",
    "\n",
    "In diesem Notizbuch werden Sie Ihren ersten Deep Reinforcement Learning-Algorithmus von Grund auf programmieren: Reinforce (auch Monte Carlo Policy Gradient genannt).\n",
    "\n",
    "Reinforce ist eine *Policy-basierte Methode*: ein Deep Reinforcement Learning-Algorithmus, der versucht, **die Policy direkt zu optimieren, ohne eine Action-Value-Funktion** zu verwenden.\n",
    "\n",
    "Genauer gesagt ist Reinforce eine *Policy-Gradient-Methode*, eine Unterklasse von *Policy-basierten Methoden*, die darauf abzielt, **die Policy direkt zu optimieren, indem sie die Gewichte der optimalen Policy mithilfe des Gradientenanstiegs schätzt**.\n",
    "\n",
    "Um ihre Robustheit zu testen, werden wir sie in 2 verschiedenen einfachen Umgebungen trainieren:\n",
    "- Cartpole-v1\n",
    "- Pixelcopter-Umgebung\n",
    "\n",
    "⬇️ Hier ist ein Beispiel für das, was **am Ende dieses Notizbuchs erreicht wird** ⬇️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4rBom2sbo7S"
   },
   "source": [
    "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/envs.gif\" alt=\"Umgebungen\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BPLwsPajb1f8"
   },
   "source": [
    "### 🎮 Umgebungen:\n",
    "\n",
    "- [CartPole-v1](https://www.gymlibrary.dev/environments/classic_control/cart_pole/)\n",
    "- [PixelCopter](https://pygame-learning-environment.readthedocs.io/en/latest/user/games/pixelcopter.html)\n",
    "\n",
    "### 📚 RL-Library:\n",
    "\n",
    "- Python\n",
    "- PyTorch\n",
    "\n",
    "\n",
    "Wir versuchen ständig, unsere Tutorials zu verbessern. **Wenn Sie also Fehler in diesem Notizbuch** finden, öffnen Sie bitte [einen Fehler im GitHub Repo](https://github.com/huggingface/deep-rl-class/issues)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_WSo0VUV99t"
   },
   "source": [
    "## Ziele dieses Notizbuchs 🏆\n",
    "Am Ende des Notizbuchs werden Sie:\n",
    "- In der Lage sein, **einen Reinforce-Algorithmus mit PyTorch von Grund auf zu programmieren.**\n",
    "- In der Lage sein, **die Robustheit deines Agenten unter Verwendung einfacher Umgebungen zu testen.**\n",
    "- In der Lage sein, **Ihren trainierten Agenten auf den Hub** zu pushen, mit einer schönen Videowiedergabe und einem Bewertungsergebnis 🔥."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lEPrZg2eWa4R"
   },
   "source": [
    "## Dieses Notizbuch stammt aus dem Deep Reinforcement Learning Kurs.\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/deep-rl-course-illustration.jpg\" alt=\"Deep RL Course illustration\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6p5HnEefISCB"
   },
   "source": [
    "In diesem kostenlosen Kurs lernen Sie:\n",
    "\n",
    "- 📖 Deep Reinforcement Learning in **Theorie und Praxis** studieren.\n",
    "- 🧑‍💻 Lernen Sie, **berühmte Deep RL-Bibliotheken** wie Stable Baselines3, RL Baselines3 Zoo, CleanRL und Sample Factory 2.0 zu verwenden.\n",
    "- 🤖 Trainieren Sie **Agenten in einzigartigen Umgebungen**.\n",
    "\n",
    "Und mehr, siehe 📚 den Lehrplan 👉 https://simoninithomas.github.io/deep-rl-course\n",
    "\n",
    "Vergessen Sie nicht, sich **<a href=\"http://eepurl.com/ic5ZUD\">für den Kurs anzumelden</a>** (wir sammeln Ihre E-Mail, um Ihnen **die Links zu senden, wenn die einzelnen Einheiten veröffentlicht werden, und Sie über die Herausforderungen und Aktualisierungen zu informieren).**\n",
    "\n",
    "\n",
    "Der beste Weg, um in Kontakt zu bleiben, ist, unserem Discord-Server beizutreten, um sich mit der Community und mit uns auszutauschen 👉🏻 https://discord.gg/ydHrjt3WP5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Was sind die richtlinienbasierten Methoden?\n",
    "\n",
    "Das Hauptziel des Reinforcement Learning ist es, **die optimale Strategie zu finden, die die erwartete kumulative Belohnung maximiert**.\n",
    "Da das Reinforcement Learning auf der *Belohnungshypothese* basiert: **alle Ziele können als Maximierung der erwarteten kumulativen Belohnung beschrieben werden**.\n",
    "\n",
    "Bei einem Fußballspiel zum Beispiel (bei dem man die Agenten in zwei Einheiten trainiert) ist das Ziel, das Spiel zu gewinnen. Wir können dieses Ziel beim Reinforcement Learning wie folgt beschreiben\n",
    "**Maximierung der Anzahl der erzielten Tore** (wenn der Ball die Torlinie überquert) in die Fußballtore des Gegners. Und **Minimierung der Anzahl der Tore in den eigenen Fußballtoren**.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/soccer.jpg\" alt=\"Fußball\" />\n",
    "\n",
    "## Wertbasierte, richtlinienbasierte und akteurskritische Methoden\n",
    "\n",
    "In der ersten Einheit haben wir zwei Methoden kennengelernt, um die optimale Policy zu finden (oder sich ihr in den meisten Fällen zu nähern).\n",
    "\n",
    "- Bei *wertbasierten Methoden* lernen wir eine Wertfunktion.\n",
    "  - Die Idee ist, dass eine optimale Wertfunktion zu einer optimalen Policy \\\\(\\pi^{*}\\) führt.\n",
    "  - Unser Ziel ist es, **den Verlust zwischen dem vorhergesagten und dem Zielwert zu minimieren**, um die wahre Aktions-Wert-Funktion zu approximieren.\n",
    "  - Wir haben eine Strategie, aber sie ist implizit, da sie **direkt aus der Wertfunktion** generiert wird. Beim Q-Learning haben wir zum Beispiel eine (epsilon-)gierige Strategie verwendet.\n",
    "\n",
    "- Im Gegensatz dazu lernen wir bei *policy-basierten Methoden* direkt, \\\\(\\pi^{*}\\) zu approximieren, ohne eine Wertfunktion lernen zu müssen.\n",
    "  - Die Idee ist **die Parametrisierung der Policy**. Wenn man beispielsweise ein neuronales Netz \\\\(\\pi_\\theta\\\\) verwendet, wird diese Strategie eine Wahrscheinlichkeitsverteilung über Aktionen ausgeben (stochastische Strategie).\n",
    "  - <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/stochastic_policy.png\" alt=\"stochastic policy\" />\n",
    "  - Unser Ziel ist dann **die Maximierung der Leistung der parametrisierten Policy unter Verwendung des Gradientenanstiegs**.\n",
    "  - Zu diesem Zweck kontrollieren wir den Parameter \\\\(\\theta\\), der die Verteilung der Aktionen über einen Zustand beeinflussen wird.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/policy_based.png\" alt=\"Policy based\" />\n",
    "\n",
    "- Nächstes Mal werden wir uns mit der *Actor-Critic*-Methode befassen, die eine Kombination aus wertbasierten und richtlinienbasierten Methoden darstellt.\n",
    "\n",
    "Dank der Policybasierten Methoden können wir unsere Policy \\\\(\\pi_\\theta\\\\) direkt optimieren, um eine Wahrscheinlichkeitsverteilung über Aktionen \\\\(\\pi_\\theta(a|s)\\\\\\) auszugeben, die zur besten kumulativen Rendite führt.\n",
    "Dazu definieren wir eine Zielfunktion \\\\(J(\\theta)\\\\), d.h. die erwartete kumulative Belohnung, und wir **möchten den Wert \\\\(\\theta\\\\) finden, der diese Zielfunktion maximiert**.\n",
    "\n",
    "## Der Unterschied zwischen richtlinienbasierten und richtliniengradientenbasierten Methoden\n",
    "\n",
    "Policy-Gradient-Methoden, die wir in dieser Einheit untersuchen werden, sind eine Unterklasse der Policy-basierten Methoden. Bei richtlinienbasierten Methoden erfolgt die Optimierung die meiste Zeit *auf der Grundlage von Richtlinien*, da wir für jede Aktualisierung nur Daten (Trajektorien) verwenden, die **durch unsere letzte Version von** gesammelt wurden.\n",
    "\n",
    "Der Unterschied zwischen diesen beiden Methoden **liegt darin, wie wir den Parameter** \\\\(\\theta\\\\) optimieren:\n",
    "\n",
    "- Bei *Policy-basierten Methoden* suchen wir direkt nach der optimalen Policy. Wir können den Parameter \\\\(\\theta\\\\) **indirekt** optimieren, indem wir die lokale Approximation der Zielfunktion mit Techniken wie Hill Climbing, Simulated Annealing oder Evolutionsstrategien maximieren.\n",
    "- Bei *Policy-Gradient-Methoden*, die eine Unterklasse der Policy-basierten Methoden sind, suchen wir direkt nach der optimalen Policy. Wir optimieren jedoch den Parameter \\\\(\\theta\\) **direkt**, indem wir den Gradientenanstieg auf die Leistung der Zielfunktion \\\\(J(\\theta)\\\\) anwenden.\n",
    "\n",
    "Bevor wir uns näher mit der Funktionsweise von Policy-Gradient-Methoden befassen (Zielfunktion, Policy-Gradient-Theorem, Gradientenanstieg usw.), wollen wir die Vor- und Nachteile von Policy-basierten Methoden untersuchen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Die Vor- und Nachteile der Policy-Gradient-Methoden\n",
    "\n",
    "An dieser Stelle werden Sie vielleicht fragen: \"Aber Deep Q-Learning ist hervorragend! Warum also Policy-Gradient-Methoden verwenden?\". Um diese Frage zu beantworten, lassen Sie uns die **Vor- und Nachteile von Policy-Gradient-Methoden** untersuchen.\n",
    "\n",
    "## Vorteile\n",
    "\n",
    "Es gibt mehrere Vorteile gegenüber wertbasierten Methoden. Sehen wir uns einige von ihnen an:\n",
    "\n",
    "### Die Einfachheit der Integration\n",
    "\n",
    "Wir können die Policy direkt schätzen, ohne zusätzliche Daten (Aktionswerte) zu speichern.\n",
    "\n",
    "### Policy-Gradient-Methoden können eine stochastische Policy lernen\n",
    "\n",
    "Policy-Gradient-Methoden können **eine stochastische Policy erlernen, während Wertfunktionen dies nicht können**.\n",
    "\n",
    "Dies hat zwei Konsequenzen:\n",
    "\n",
    "1. Wir **müssen einen Kompromiss zwischen Exploration und Ausbeutung nicht von Hand implementieren**. Da wir eine Wahrscheinlichkeitsverteilung über Aktionen ausgeben, erkundet der Agent **den Zustandsraum, ohne immer dieselbe Flugbahn zu nehmen**.\n",
    "\n",
    "2. Wir beseitigen auch das Problem des **perceptual aliasing**. Perceptual aliasing bedeutet, dass zwei Zustände gleich erscheinen (oder sind), aber unterschiedliche Aktionen erfordern.\n",
    "\n",
    "Nehmen wir ein Beispiel: Wir haben einen intelligenten Staubsauger, dessen Ziel es ist, den Staub zu saugen und die Hamster nicht zu töten.\n",
    "\n",
    "<figure class=\"image table text-center m-0 w-full\">\n",
    "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/hamster1.jpg\" alt=\"Hamster 1\"/>\n",
    "</figure>\n",
    "\n",
    "Unser Staubsauger kann nur wahrnehmen, wo die Wände sind.\n",
    "\n",
    "Das Problem ist, dass die **zwei roten (farbigen) Zustände Aliasing-Zustände sind, weil der Agent jeweils eine obere und untere Wand wahrnimmt**.\n",
    "\n",
    "<figure class=\"image table text-center m-0 w-full\">\n",
    "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/hamster2.jpg\" alt=\"Hamster 1\"/>\n",
    "</figure>\n",
    "\n",
    "Bei einer deterministischen Strategie wird der Agent entweder immer nach rechts gehen, wenn er sich im roten Bereich befindet, oder immer nach links. **In beiden Fällen bleibt unser Agent stecken und kann sich nicht aus dem Staub machen**.\n",
    "\n",
    "Mit einem wertbasierten Verstärkungslernalgorithmus lernen wir eine **quasi-deterministische Strategie** (\"gierige Epsilon-Strategie\"). Folglich kann unser Agent **viel Zeit damit verbringen, den Staub zu finden**.\n",
    "\n",
    "Andererseits bewegt sich eine optimale stochastische Strategie **in roten (farbigen) Zuständen zufällig nach links oder rechts**. Folglich **bleibt er nicht stecken und erreicht den Zielzustand mit einer hohen Wahrscheinlichkeit**.\n",
    "\n",
    "<figure class=\"image table text-center m-0 w-full\">\n",
    "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/hamster3.jpg\" alt=\"Hamster 1\"/>\n",
    "</figure>\n",
    "\n",
    "### Policy-Gradient-Methoden sind in hochdimensionalen Aktionsräumen und kontinuierlichen Aktionsräumen effektiver.\n",
    "\n",
    "Das Problem mit Deep Q-learning ist, dass ihre **Vorhersagen in jedem Zeitschritt eine Punktzahl (maximale erwartete zukünftige Belohnung) für jede mögliche Aktion** zuweisen, wenn der aktuelle Zustand gegeben ist.\n",
    "\n",
    "Was aber, wenn es unendlich viele Handlungsmöglichkeiten gibt?\n",
    "\n",
    "Bei einem selbstfahrenden Auto zum Beispiel kann man in jedem Zustand eine (fast) unendliche Auswahl an Aktionen haben (das Lenkrad um 15°, 17,2°, 19,4° drehen, hupen usw.). **Wir müssen für jede mögliche Aktion einen Q-Wert ausgeben**! Und **die maximale Aktion einer kontinuierlichen Ausgabe zu wählen, ist selbst ein Optimierungsproblem**!\n",
    "\n",
    "Mit Policy-Gradient-Methoden geben wir stattdessen eine **Wahrscheinlichkeitsverteilung über Aktionen** aus.\n",
    "\n",
    "### Policy-Gradient-Methoden haben bessere Konvergenzeigenschaften\n",
    "\n",
    "Bei wertbasierten Methoden verwenden wir einen aggressiven Operator, um **die Wertfunktion zu ändern: wir nehmen das Maximum über Q-Schätzungen**.\n",
    "Folglich können sich die Aktionswahrscheinlichkeiten bei einer beliebig kleinen Änderung der geschätzten Aktionswerte drastisch ändern, wenn diese Änderung dazu führt, dass eine andere Aktion den maximalen Wert hat.\n",
    "\n",
    "Wenn z. B. während des Trainings die beste Aktion links war (mit einem Q-Wert von 0,22) und im nächsten Trainingsschritt rechts (da der rechte Q-Wert 0,23 wird), haben wir die Strategie drastisch geändert, da die Strategie nun die meiste Zeit rechts statt links wählt.\n",
    "\n",
    "Andererseits ändern sich bei Policy-Gradient-Methoden die stochastischen Handlungspräferenzen (Wahrscheinlichkeit, eine Handlung vorzunehmen) **mit der Zeit gleichmäßig**.\n",
    "\n",
    "## Nachteile\n",
    "\n",
    "Natürlich haben Policy-Gradient-Methoden auch einige Nachteile:\n",
    "\n",
    "- **Häufig konvergiert die Policy-Gradient-Methode zu einem lokalen Maximum anstatt zu einem globalen Optimum**.\n",
    "- Policy-Gradient-Methoden gehen langsamer vor, **Schritt für Schritt: es kann länger dauern, sie zu trainieren (ineffizient).**\n",
    "- Policy-Gradient kann eine hohe Varianz haben. Wir werden in der Einheit zur Akteurskritik sehen, warum das so ist, und wie wir dieses Problem lösen können.\n",
    "\n",
    "👉 Wenn Sie sich eingehender mit den Vor- und Nachteilen von Policy-Gradient-Methoden beschäftigen möchten, [können Sie sich dieses Video ansehen] (https://youtu.be/y3oqOjHilio).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vertiefung der Policy-Gradient-Methoden\n",
    "\n",
    "## Das große Ganze im Blick\n",
    "\n",
    "Wir haben gerade gelernt, dass Policy-Gradient-Methoden darauf abzielen, Parameter \\\\( \\theta \\\\) zu finden, die **den erwarteten Ertrag** maximieren.\n",
    "\n",
    "Die Idee ist, dass wir eine *parametrisierte stochastische Strategie* haben. In unserem Fall gibt ein neuronales Netz eine Wahrscheinlichkeitsverteilung über Aktionen aus. Die Wahrscheinlichkeit, jede Aktion durchzuführen, wird auch als *Aktionspräferenz* bezeichnet.\n",
    "\n",
    "Nehmen wir das Beispiel von CartPole-v1:\n",
    "- Als Eingabe haben wir einen Zustand.\n",
    "- Als Ausgabe haben wir eine Wahrscheinlichkeitsverteilung über Aktionen in diesem Zustand.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/policy_based.png\" alt=\"Policy based\" />\n",
    "\n",
    "Unser Ziel mit Policy-Gradient ist es, **die Wahrscheinlichkeitsverteilung von Aktionen** zu steuern, indem wir die Policy so abstimmen, dass **gute Aktionen (die den Ertrag maximieren) in der Zukunft häufiger abgerufen werden**.\n",
    "Jedes Mal, wenn der Agent mit der Umwelt interagiert, verändern wir die Parameter so, dass gute Aktionen in der Zukunft häufiger durchgeführt werden.\n",
    "\n",
    "Aber **wie sollen wir die Gewichte anhand des erwarteten Ertrags** optimieren?\n",
    "\n",
    "Die Idee ist, dass wir den Agenten **während einer Episode interagieren lassen**. Wenn wir die Episode gewinnen, gehen wir davon aus, dass jede getätigte Aktion gut war und in Zukunft häufiger durchgeführt werden muss\n",
    "da sie zum Sieg führen.\n",
    "\n",
    "Wir wollen also für jedes Zustands-Aktions-Paar die \\\\(P(a|s)\\\\) erhöhen: die Wahrscheinlichkeit, diese Aktion in diesem Zustand durchzuführen. Oder verringern, wenn wir verloren haben.\n",
    "\n",
    "Der Policy-Gradient-Algorithmus (vereinfacht) sieht wie folgt aus:\n",
    "<figure class=\"image table text-center m-0 w-full\">\n",
    "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/pg_bigpicture.jpg\" alt=\"Policy Gradient Big Picture\"/>\n",
    "</figure>\n",
    "\n",
    "Jetzt, wo wir das große Bild haben, wollen wir tiefer in die Methoden des Policy-Gradienten eintauchen.\n",
    "\n",
    "## Vertiefung der Policy-Gradient-Methoden\n",
    "\n",
    "Wir haben unsere stochastische Policy \\\\(\\pi\\\\), die einen Parameter \\\\(\\theta\\\\) hat. Dieses \\\\(\\pi\\\\) gibt bei einem Zustand **eine Wahrscheinlichkeitsverteilung von Aktionen** aus.\n",
    "\n",
    "<figure class=\"image table text-center m-0 w-full\">\n",
    "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/stochastic_policy.png\" alt=\"Policy\"/>\n",
    "</figure>\n",
    "\n",
    "Wobei \\\\(\\pi_\\theta(a_t|s_t)\\\\\\) die Wahrscheinlichkeit ist, dass der Agent die Aktion \\\\(a_t\\\\) aus dem Zustand \\\\(s_t\\\\\\) auswählt, wenn unsere Policy gegeben ist.\n",
    "\n",
    "**Aber woher wissen wir, ob unsere Strategie gut ist?** Wir brauchen eine Möglichkeit, sie zu messen. Um das zu wissen, definieren wir eine Punktzahl/Zielfunktion namens \\\\(J(\\theta)\\\\).\n",
    "\n",
    "### Die Zielfunktion\n",
    "\n",
    "Die *Zielfunktion* gibt uns die **Leistung des Agenten** bei einer Trajektorie (Zustandsaktionsfolge ohne Berücksichtigung der Belohnung (im Gegensatz zu einer Episode)), und sie gibt die *erwartete kumulative Belohnung* aus.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/objective.jpg\" alt=\"Return\"/>\n",
    "\n",
    "Lassen Sie uns diese Formel etwas genauer erläutern:\n",
    "- Die *erwartete Rendite* (auch *erwartete kumulative Belohnung* genannt), ist der gewichtete Durchschnitt (wobei die Gewichte durch \\\\(P(\\tau;\\theta)\\\\) aller möglichen Werte gegeben sind, die die Rendite \\\\(R(\\tau)\\\\) annehmen kann).\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/expected_reward.png\" alt=\"Return\"/>\n",
    "\n",
    "\n",
    "- \\\\(R(\\tau)\\\\) :  Die Rückkehr von einer beliebigen Flugbahn. Um diese Größe zur Berechnung der erwarteten Rendite zu verwenden, müssen wir sie mit der Wahrscheinlichkeit jeder möglichen Trajektorie multiplizieren.\n",
    "\n",
    "- \\\\(P(\\tau;\\theta)\\\\) : Wahrscheinlichkeit jeder möglichen Trajektorie \\\\(\\tau\\\\) (diese Wahrscheinlichkeit hängt von \\\\( \\theta\\\\) ab, da sie die Policy definiert, die sie verwendet, um die Aktionen der Trajektorie auszuwählen, die einen Einfluss auf die besuchten Zustände hat).\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/probability.png\" alt=\"Probability\"/>\n",
    "\n",
    "- \\\\(J(\\theta)\\\\) : Die erwartete Rendite berechnen wir, indem wir für alle Trajektorien die Wahrscheinlichkeit, diese Trajektorie zu nehmen, wenn \\\\(\\theta \\\\) mit der Rendite dieser Trajektorie multipliziert wird.\n",
    "\n",
    "Unser Ziel ist es dann, die erwartete kumulative Belohnung zu maximieren, indem wir das \\\\(\\theta \\\\) finden, das die besten Aktionswahrscheinlichkeitsverteilungen ergibt:\n",
    "\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/max_objective.png\" alt=\"Max objective\"/>\n",
    "\n",
    "## Gradientenanstieg und das Policy-Gradienten-Theorem\n",
    "\n",
    "Policy-Gradient ist ein Optimierungsproblem: Wir wollen die Werte von \\\\(\\theta\\) finden, die unsere Zielfunktion \\\\(J(\\theta)\\\\) maximieren, also müssen wir **Gradienten-Anstieg** verwenden. Dies ist die Umkehrung von *Gradient-Descent*, da sie die Richtung des steilsten Anstiegs von \\\\(J(\\theta)\\\\) angibt.\n",
    "\n",
    "(Wenn Sie eine Auffrischung des Unterschieds zwischen Gradientenabstieg und Gradientenaufstieg benötigen, lesen Sie [dies](https://www.baeldung.com/cs/gradient-descent-vs-ascent) und [dies](https://stats.stackexchange.com/questions/258721/gradient-ascent-vs-gradient-descent-in-logistic-regression)).\n",
    "\n",
    "Unser Aktualisierungsschritt für den Gradienten-Abstieg ist:\n",
    "\n",
    "\\\\( \\theta \\leftarrow \\theta + \\alpha * \\nabla_\\theta J(\\theta) \\\\\\)\n",
    "\n",
    "Wir können diese Aktualisierung wiederholt anwenden, in der Hoffnung, dass \\\\\\(\\theta \\\\) zu dem Wert konvergiert, der \\\\\\(J(\\theta)\\\\\\) maximiert.\n",
    "\n",
    "Es gibt jedoch zwei Probleme bei der Berechnung der Ableitung von \\\\(J(\\theta)\\\\):\n",
    "1. Wir können die wahre Steigung der Zielfunktion nicht berechnen, da dies die Berechnung der Wahrscheinlichkeit jeder möglichen Flugbahn erfordert, was rechnerisch sehr aufwendig ist.\n",
    "Wir wollen also eine **Gradientenschätzung mit einer stichprobenbasierten Schätzung berechnen (einige Trajektorien sammeln)**.\n",
    "\n",
    "2. Wir haben ein weiteres Problem, das ich im nächsten optionalen Abschnitt erkläre. Um diese Zielfunktion zu differenzieren, müssen wir die Zustandsverteilung, die sogenannte Markov-Entscheidungsprozess-Dynamik, differenzieren. Diese ist mit der Umwelt verbunden. Sie gibt uns die Wahrscheinlichkeit, dass die Umgebung in den nächsten Zustand übergeht, wenn der aktuelle Zustand und die vom Agenten durchgeführte Aktion gegeben sind. Das Problem ist, dass wir sie nicht differenzieren können, weil wir sie möglicherweise nicht kennen.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/probability.png\" alt=\"Wahrscheinlichkeit\"/>\n",
    "\n",
    "Glücklicherweise werden wir eine Lösung namens Policy Gradient Theorem verwenden, die uns helfen wird, die Zielfunktion in eine differenzierbare Funktion umzuformulieren, die keine Differenzierung der Zustandsverteilung erfordert.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/policy_gradient_theorem.png\" alt=\"Policy Gradient\"/>\n",
    "\n",
    "Wenn Sie verstehen wollen, wie wir diese Formel für die Annäherung an den Gradienten herleiten, lesen Sie den nächsten (optionalen) Abschnitt.\n",
    "\n",
    "## Der Reinforce-Algorithmus (Monte Carlo Reinforce)\n",
    "\n",
    "Der Reinforce-Algorithmus, auch Monte-Carlo-Policy-Gradient genannt, ist ein Policy-Gradient-Algorithmus, der **eine geschätzte Rendite aus einer ganzen Episode zur Aktualisierung des Policy-Parameters** verwendet:\n",
    "\n",
    "In einer Schleife:\n",
    "- Verwenden Sie die Richtlinie \\\\(\\pi_\\theta\\\\), um eine Episode zu sammeln \\\\(\\tau\\\\)\n",
    "- Verwenden Sie die Episode, um den Gradienten zu schätzen \\\\(\\hat{g} = \\nabla_\\theta J(\\theta)\\\\\\)\n",
    "\n",
    " <figure class=\"image table text-center m-0 w-full\">\n",
    "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/policy_gradient_one.png\" alt=\"Policy Gradient\"/>\n",
    "</figure>\n",
    "\n",
    "- Aktualisieren Sie die Gewichte der Richtlinie: \\\\(\\theta \\leftarrow \\theta + \\alpha \\hat{g}\\\\)\n",
    "\n",
    "Wir können diese Aktualisierung wie folgt interpretieren:\n",
    "- \\\\(\\nabla_\\theta log \\pi_\\theta(a_t|s_t)\\\\\\) ist die Richtung des **stärksten Anstiegs der (log)-Wahrscheinlichkeit** der Auswahl der Aktion at aus dem Zustand st.\n",
    "Dies sagt uns, **wie wir die Gewichte der Policy ändern sollten**, wenn wir die logarithmische Wahrscheinlichkeit der Auswahl einer Aktion \\\\(a_t\\\\) im Zustand \\\\(s_t\\\\) erhöhen/verringern wollen.\n",
    "- \\\\(R(\\tau)\\\\): ist die Bewertungsfunktion:\n",
    "  - Wenn die Rendite hoch ist, **erhöht sie die Wahrscheinlichkeiten** der Kombinationen (Zustand, Aktion).\n",
    "  - Wenn die Rendite niedrig ist, **verringert sie die Wahrscheinlichkeiten** der (Zustand, Handlung) Kombinationen.\n",
    "\n",
    "\n",
    "Wir können auch **mehrere Episoden (Trajektorien)** sammeln, um den Gradienten zu schätzen:\n",
    "<figure class=\"image table text-center m-0 w-full\">\n",
    " <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/policy_gradient_multiple.png\" alt=\"Policy Gradient\"/>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glossar \n",
    "\n",
    "- **Deep Q-Learning:** Ein wertbasierter Deep Reinforcement Learning-Algorithmus, der ein tiefes neuronales Netz verwendet, um Q-Werte für Aktionen in einem bestimmten Zustand zu approximieren. Das Ziel von Deep Q-Learning ist es, die optimale Strategie zu finden, die die erwartete kumulative Belohnung durch Lernen der Aktionswerte maximiert.\n",
    "\n",
    "- **Wertbasierte Methoden:** Reinforcement-Learning-Methoden, die eine Wertfunktion als Zwischenschritt zur Ermittlung einer optimalen Strategie schätzen.\n",
    "\n",
    "- **Policybasierte Methoden:** Reinforcement Learning Methoden, die direkt lernen, die optimale Policy zu approximieren, ohne eine Wertfunktion zu lernen. In der Praxis geben sie eine Wahrscheinlichkeitsverteilung über Aktionen aus. \n",
    "\n",
    "    Die Vorteile der Verwendung von Policy-Gradienten-Methoden gegenüber wertbasierten Methoden sind unter anderem: \n",
    "    - Einfachheit der Integration: keine Notwendigkeit, Aktionswerte zu speichern;\n",
    "    - die Fähigkeit, eine stochastische Strategie zu erlernen: der Agent erkundet den Zustandsraum, ohne immer dieselbe Flugbahn einzuschlagen, und vermeidet das Problem des Wahrnehmungs-Alias;\n",
    "    - Effektivität in hochdimensionalen und kontinuierlichen Handlungsräumen; und\n",
    "    - verbesserte Konvergenzeigenschaften.\n",
    "\n",
    "- **Policy Gradient:** Eine Teilmenge der Policy-basierten Methoden, bei denen das Ziel darin besteht, die Leistung einer parametrisierten Policy mithilfe des Gradientenanstiegs zu maximieren. Das Ziel eines Policy-Gradienten ist es, die Wahrscheinlichkeitsverteilung von Aktionen zu kontrollieren, indem die Policy so eingestellt wird, dass gute Aktionen (die den Ertrag maximieren) in Zukunft häufiger abgerufen werden. \n",
    "\n",
    "- **Monte Carlo Reinforce:** Ein Policy-Gradient-Algorithmus, der eine geschätzte Rendite aus einer ganzen Episode verwendet, um den Policy-Parameter zu aktualisieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mjY-eq3eWh9O"
   },
   "source": [
    "## Voraussetzungen 🏗️\n",
    "Bevor Sie sich mit dem Notebook beschäftigen, müssen Sie:\n",
    "\n",
    "🔲 📚 [Studieren Sie Policy Gradients durch Lesen von Unit 4](https://huggingface.co/deep-rl-course/unit4/introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bsh4ZAamchSl"
   },
   "source": [
    "# Wir programmieren den Reinforce-Algorithmus von Grund auf neu 🔥.\n",
    "\n",
    "\n",
    "Um diese praktische Übung für den Zertifizierungsprozess zu validieren, müssen Sie Ihre trainierten Modelle an den Hub übertragen.\n",
    "\n",
    "- Erhalten Sie ein Ergebnis von >= 350 für \"Cartpole-v1\".\n",
    "- Erhalte ein Ergebnis von >= 5 für \"PixelCopter\".\n",
    "\n",
    "Um dein Ergebnis zu finden, gehe zum Leaderboard und suche dein Modell, **das Ergebnis = mean_reward - std of reward**. **Wenn du dein Modell nicht auf der Bestenliste siehst, gehe unten auf der Bestenlistenseite und klicke auf die Schaltfläche \"Aktualisieren \"**.\n",
    "\n",
    "Weitere Informationen über den Zertifizierungsprozess finden Sie in diesem Abschnitt 👉 https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JoTC9o2SczNn"
   },
   "source": [
    "## Ein Ratschlag 💡\n",
    "Es ist besser, dieses Colab in einer Kopie auf Ihrem Google Drive auszuführen, so dass Sie **bei Zeitüberschreitungen** immer noch das gespeicherte Notizbuch auf Ihrem Google Drive haben und nicht alles von Grund auf neu ausfüllen müssen.\n",
    "\n",
    "Dazu kannst du entweder \"Strg + S\" oder \"Datei > Kopie in Google Drive speichern\" verwenden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bTpYcVZVMzUI"
   },
   "source": [
    "## Erstellen einer virtuellen Anzeige 🖥\n",
    "\n",
    "Während der Arbeit mit dem Notebook müssen wir ein Wiederholungsvideo erstellen. Dazu benötigen wir mit colab **einen virtuellen Bildschirm, um die Umgebung zu rendern** (und somit die Bilder aufzunehmen).\n",
    "\n",
    "Daher wird die folgende Zelle die Librairies installieren und einen virtuellen Bildschirm erstellen und starten 🖥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jV6wjQ7Be7p5"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!apt install python-opengl\n",
    "!apt install ffmpeg\n",
    "!apt install xvfb\n",
    "!pip install pyvirtualdisplay\n",
    "!pip install pyglet==1.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sr-Nuyb1dBm0"
   },
   "outputs": [],
   "source": [
    "# Virtual display\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "virtual_display = Display(visible=0, size=(1400, 900))\n",
    "virtual_display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tjrLfPFIW8XK"
   },
   "source": [
    "## Installieren Sie die Abhängigkeiten 🔽.\n",
    "Der erste Schritt besteht darin, die Abhängigkeiten zu installieren. Wir werden mehrere davon installieren:\n",
    "\n",
    "- `gym`\n",
    "- `gym-games`: Zusätzliche Gym-Umgebungen, die mit PyGame erstellt wurden.\n",
    "- `huggingface_hub`: 🤗 dient als zentraler Ort, an dem jeder Modelle und Datensätze teilen und erforschen kann. Es bietet Versionierung, Metriken, Visualisierungen und andere Funktionen, die eine einfache Zusammenarbeit mit anderen ermöglichen.\n",
    "\n",
    "Sie fragen sich vielleicht, warum wir gym und nicht gymnasium, eine neuere Version von gym, installieren? **Weil die Gym-Spiele, die wir verwenden, noch nicht mit Gym aktualisiert sind**.\n",
    "\n",
    "Die Unterschiede, die Sie hier finden werden:\n",
    "- In `gym` haben wir kein `terminated` und `truncated` sondern nur `done`.\n",
    "- In `gym` liefert die Verwendung von `env.step()` die Ergebnisse `state, reward, done, info`.\n",
    "\n",
    "Mehr über die Unterschiede zwischen Gym und Gymnasium kannst du hier erfahren 👉 https://gymnasium.farama.org/content/migration-guide/\n",
    "\n",
    "\n",
    "Hier können Sie alle verfügbaren Reinforce-Modelle sehen 👉 https://huggingface.co/models?other=reinforce\n",
    "\n",
    "Und hier finden Sie alle Deep Reinforcement Learning-Modelle 👉 https://huggingface.co/models?pipeline_tag=reinforcement-learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e8ZVi-uydpgL"
   },
   "outputs": [],
   "source": [
    "!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AAHAq6RZW3rn"
   },
   "source": [
    "## Importieren der Pakete 📦\n",
    "Zusätzlich zum Import der installierten Bibliotheken, importieren wir auch:\n",
    "\n",
    "- `imageio`: Eine Bibliothek, die uns helfen wird, ein Wiedergabevideo zu erzeugen\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V8oadoJSWp7C"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# Gym\n",
    "import gym\n",
    "import gym_pygame\n",
    "\n",
    "# Hugging Face Hub\n",
    "from huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RfxJYdMeeVgv"
   },
   "source": [
    "## Prüfen, ob wir eine GPU haben\n",
    "\n",
    "- Prüfen wir, ob wir eine GPU haben\n",
    "- Wenn dies der Fall ist, sollten Sie `Gerät:cuda0` sehen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kaJu5FeZxXGY"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U5TNYa14aRav"
   },
   "outputs": [],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBPecCtBL_pZ"
   },
   "source": [
    "Wir sind nun bereit, unseren Reinforce-Algorithmus zu implementieren 🔥."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8KEyKYo2ZSC-"
   },
   "source": [
    "# Erster Agent: Playing CartPole-v1 🤖"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "haLArKURMyuF"
   },
   "source": [
    "## Erstellen Sie die CartPole-Umgebung und verstehen Sie, wie sie funktioniert.\n",
    "### [Die Umgebung 🎮](https://www.gymlibrary.dev/environments/classic_control/cart_pole/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AH_TaLKFXo_8"
   },
   "source": [
    "### Warum verwenden wir eine einfache Umgebung wie CartPole-v1?\n",
    "Wie in [Tipps und Tricks zum Verstärkungslernen] (https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html) erläutert, müssen Sie bei der Neuimplementierung Ihres Agenten **sich vergewissern, dass er korrekt funktioniert, und mit einfachen Umgebungen Fehler finden, bevor Sie tiefer gehen**. Denn die Fehlersuche ist in einfachen Umgebungen viel einfacher.\n",
    "\n",
    "\n",
    "> Versuchen Sie, bei Spielzeugproblemen ein \"Lebenszeichen\" zu geben.\n",
    "\n",
    "\n",
    "> Validieren Sie die Implementierung, indem Sie sie auf immer schwierigeren Umgebungen laufen lassen (Sie können die Ergebnisse mit dem RL-Zoo vergleichen). Für diesen Schritt müssen Sie normalerweise eine Hyperparameter-Optimierung durchführen.\n",
    "___\n",
    "### Die CartPole-v1-Umgebung\n",
    "\n",
    "> Eine Stange ist über ein unbetätigtes Gelenk an einem Wagen befestigt, der sich auf einer reibungsfreien Bahn bewegt. Das Pendel wird aufrecht auf dem Wagen platziert und das Ziel ist es, die Stange auszubalancieren, indem Kräfte in die linke und rechte Richtung auf den Wagen ausgeübt werden.\n",
    "\n",
    "\n",
    "\n",
    "Wir beginnen also mit CartPole-v1. Das Ziel ist es, den Wagen nach links oder rechts zu schieben, **so dass der Pol im Gleichgewicht bleibt.**\n",
    "\n",
    "Die Episode endet, wenn:\n",
    "- der Winkel der Stange größer als ±12° ist\n",
    "- Die Position des Wagens ist größer als ±2,4\n",
    "- Die Episodenlänge ist größer als 500\n",
    "\n",
    "Wir erhalten eine Belohnung 💰 von +1 für jeden Zeitschritt, den der Pol im Gleichgewicht bleibt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "POOOk15_K6KA"
   },
   "outputs": [],
   "source": [
    "env_id = \"CartPole-v1\"\n",
    "# Create the env\n",
    "env = gym.make(env_id)\n",
    "\n",
    "# Create the evaluation env\n",
    "eval_env = gym.make(env_id)\n",
    "\n",
    "# Get the state space and action space\n",
    "s_size = env.observation_space.shape[0]\n",
    "a_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FMLFrjiBNLYJ"
   },
   "outputs": [],
   "source": [
    "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
    "print(\"The State Space is: \", s_size)\n",
    "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lu6t4sRNNWkN"
   },
   "outputs": [],
   "source": [
    "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
    "print(\"The Action Space is: \", a_size)\n",
    "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7SJMJj3WaFOz"
   },
   "source": [
    "## Bauen wir die Reinforce-Architektur auf\n",
    "Diese Implementierung basiert auf zwei Implementierungen:\n",
    "- [PyTorch official Reinforcement Learning example](https://github.com/pytorch/examples/blob/main/reinforcement_learning/reinforce.py)\n",
    "- [Udacity Reinforce](https://github.com/udacity/deep-reinforcement-learning/blob/master/reinforce/REINFORCE.ipynb)\n",
    "- [Verbesserung der Integration durch Chris1nexus](https://github.com/huggingface/deep-rl-class/pull/95)\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/reinforce.png\" alt=\"Reinforce\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "49kogtxBODX8"
   },
   "source": [
    "Wir wollen also:\n",
    "- Zwei vollständig verbundene Schichten (fc1 und fc2).\n",
    "- Verwendung von ReLU als Aktivierungsfunktion von fc1\n",
    "- Verwendung von Softmax zur Ausgabe einer Wahrscheinlichkeitsverteilung über Aktionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w2LHcHhVZvPZ"
   },
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, s_size, a_size, h_size):\n",
    "        super(Policy, self).__init__()\n",
    "        # Create two fully connected layers\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Define the forward pass\n",
    "        # state goes to fc1 then we apply ReLU activation function\n",
    "\n",
    "        # fc1 outputs goes to fc2\n",
    "\n",
    "        # We output the softmax\n",
    "    \n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "        Given a state, take action\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        probs = self.forward(state).cpu()\n",
    "        m = Categorical(probs)\n",
    "        action = np.argmax(m)\n",
    "        return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rOMrdwSYOWSC"
   },
   "source": [
    "### Lösung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jGdhRSVrOV4K"
   },
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, s_size, a_size, h_size):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(s_size, h_size)\n",
    "        self.fc2 = nn.Linear(h_size, a_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "    \n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        probs = self.forward(state).cpu()\n",
    "        m = Categorical(probs)\n",
    "        action = np.argmax(m)\n",
    "        return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZTGWL4g2eM5B"
   },
   "source": [
    "Ich habe einen Fehler gemacht, können Sie erraten, wo?\n",
    "\n",
    "- Um das herauszufinden, machen wir einen Vorwärtspass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lwnqGBCNePor"
   },
   "outputs": [],
   "source": [
    "debug_policy = Policy(s_size, a_size, 64).to(device)\n",
    "debug_policy.act(env.reset())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14UYkoxCPaor"
   },
   "source": [
    "- Hier sehen wir, dass die Fehlermeldung `ValueError: Das Wertargument von log_prob muss ein Tensor sein\".\n",
    "\n",
    "- Das bedeutet, dass `action` in `m.log_prob(action)` ein Tensor sein muss **aber das ist er nicht.**\n",
    "\n",
    "- Wissen Sie, warum? Prüfen Sie die Funktion act und versuchen Sie herauszufinden, warum sie nicht funktioniert.\n",
    "\n",
    "Hinweis 💡: Irgendetwas ist in dieser Implementierung falsch. Erinnern Sie sich daran, dass wir mit der Funktion act **eine Aktion aus der Wahrscheinlichkeitsverteilung über Aktionen** ziehen wollen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gfGJNZBUP7Vn"
   },
   "source": [
    "### (Real) Lösung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ho_UHf49N9i4"
   },
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, s_size, a_size, h_size):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(s_size, h_size)\n",
    "        self.fc2 = nn.Linear(h_size, a_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "    \n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        probs = self.forward(state).cpu()\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgJWQFU_eUYw"
   },
   "source": [
    "Durch die Verwendung von CartPole war es einfacher zu debuggen, da **wir wissen, dass der Fehler von unserer Integration kommt und nicht von unserer einfachen Umgebung**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-20i7Pk0l1T"
   },
   "source": [
    "- Da **wir eine Aktion aus der Wahrscheinlichkeitsverteilung über Aktionen** auswählen wollen, können wir nicht `Aktion = np.argmax(m)` verwenden, da dies immer die Aktion mit der höchsten Wahrscheinlichkeit ausgibt.\n",
    "\n",
    "- Wir müssen es durch `action = m.sample()` ersetzen, das eine Aktion aus der Wahrscheinlichkeitsverteilung P(.|s) auswählt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4MXoqetzfIoW"
   },
   "source": [
    "### Bauen wir den Reinforce-Trainingsalgorithmus auf\n",
    "Dies ist der Pseudocode des Reinforce-Algorithmus:\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/pg_pseudocode.png\" alt=\"Policy gradient pseudocode\"/>\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QmcXG-9i2Qu2"
   },
   "source": [
    "- Bei der Berechnung der Belohnung Gt (Zeile 6) sehen wir, dass wir die Summe der Rabattierten Belohnungn **ausgehend vom Zeitschritt t** berechnen.\n",
    "\n",
    "- Warum? Weil unsere Policy nur **Handlungen auf der Grundlage der Konsequenzen verstärken sollte**: Belohnungen, die vor einer Handlung erzielt wurden, sind also nutzlos (da sie nicht auf die Handlung zurückzuführen sind), **nur die Belohnungen, die nach der Handlung kommen, zählen**.\n",
    "\n",
    "- Bevor Sie dies kodieren, sollten Sie diesen Abschnitt lesen [lassen Sie sich nicht von der Vergangenheit ablenken] (https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#don-t-let-the-past-distract-you), in dem erklärt wird, warum wir den Reward-to-Go-Gradienten verwenden.\n",
    "\n",
    "Wir verwenden eine interessante Technik, die von [Chris1nexus](https://github.com/Chris1nexus) kodiert wurde, um **die Belohnung in jedem Zeitschritt effizient zu berechnen**. In den Kommentaren wird das Verfahren erläutert. Zögern Sie nicht, auch [die PR-Erklärung zu lesen](https://github.com/huggingface/deep-rl-class/pull/95)\n",
    "Aber im Großen und Ganzen geht es darum, **die Belohnung in jedem Zeitschritt effizient zu berechnen**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O554nUGPpcoq"
   },
   "source": [
    "Die zweite Frage, die Sie sich stellen können, ist **Warum minimieren wir den Verlust**? Sie sprachen von Gradientenaufstieg und nicht von Gradientenabstieg?\n",
    "\n",
    "- Wir wollen unsere Nutzenfunktion $J(\\theta)$ maximieren, aber in PyTorch wie in Tensorflow ist es besser, eine Zielfunktion zu **minimieren**.\n",
    "    - Nehmen wir also an, wir wollen Aktion 3 in einem bestimmten Zeitschritt verstärken. Vor dem Training dieser Aktion ist P 0.25.\n",
    "    - Wir wollen also $\\theta$ so verändern, dass $\\pi_\\theta(a_3|s; \\theta) > 0.25$\n",
    "    - Da sich alle P zu 1 summieren müssen, wird max $\\pi_\\theta(a_3|s; \\theta)$ die **Minimierung der anderen Aktionswahrscheinlichkeit$ bewirken.\n",
    "    - Wir sollten PyTorch also anweisen, $1 - \\pi_\\theta(a_3|s; \\theta)$ zu minimieren.\n",
    "    - Diese Verlustfunktion nähert sich 0, wenn $\\pi_\\theta(a_3|s; \\theta)$ sich 1 nähert.\n",
    "    - Wir ermutigen also den Gradienten zu maximal $\\pi_\\theta(a_3|s; \\theta)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iOdv8Q9NfLK7"
   },
   "outputs": [],
   "source": [
    "def reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):\n",
    "    # Help us to calculate the score during the training\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    # Line 3 of pseudocode\n",
    "    for i_episode in range(1, n_training_episodes+1):\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state = # TODO: reset the environment\n",
    "        # Line 4 of pseudocode\n",
    "        for t in range(max_t):\n",
    "            action, log_prob = # TODO get the action\n",
    "            saved_log_probs.append(log_prob)\n",
    "            state, reward, done, _ = # TODO: take an env step\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break \n",
    "        scores_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "        \n",
    "        # Line 6 of pseudocode: calculate the return\n",
    "        returns = deque(maxlen=max_t) \n",
    "        n_steps = len(rewards) \n",
    "        # Compute the discounted returns at each timestep,\n",
    "        # as the sum of the gamma-discounted return at time t (G_t) + the reward at time t\n",
    "        \n",
    "        # In O(N) time, where N is the number of time steps\n",
    "        # (this definition of the discounted return G_t follows the definition of this quantity \n",
    "        # shown at page 44 of Sutton&Barto 2017 2nd draft)\n",
    "        # G_t = r_(t+1) + r_(t+2) + ...\n",
    "        \n",
    "        # Given this formulation, the returns at each timestep t can be computed \n",
    "        # by re-using the computed future returns G_(t+1) to compute the current return G_t\n",
    "        # G_t = r_(t+1) + gamma*G_(t+1)\n",
    "        # G_(t-1) = r_t + gamma* G_t\n",
    "        # (this follows a dynamic programming approach, with which we memorize solutions in order \n",
    "        # to avoid computing them multiple times)\n",
    "        \n",
    "        # This is correct since the above is equivalent to (see also page 46 of Sutton&Barto 2017 2nd draft)\n",
    "        # G_(t-1) = r_t + gamma*r_(t+1) + gamma*gamma*r_(t+2) + ...\n",
    "        \n",
    "        \n",
    "        ## Given the above, we calculate the returns at timestep t as: \n",
    "        #               gamma[t] * return[t] + reward[t]\n",
    "        #\n",
    "        ## We compute this starting from the last timestep to the first, in order\n",
    "        ## to employ the formula presented above and avoid redundant computations that would be needed \n",
    "        ## if we were to do it from first to last.\n",
    "        \n",
    "        ## Hence, the queue \"returns\" will hold the returns in chronological order, from t=0 to t=n_steps\n",
    "        ## thanks to the appendleft() function which allows to append to the position 0 in constant time O(1)\n",
    "        ## a normal python list would instead require O(N) to do this.\n",
    "        for t in range(n_steps)[::-1]:\n",
    "            disc_return_t = (returns[0] if len(returns)>0 else 0)\n",
    "            returns.appendleft(    ) # TODO: complete here        \n",
    "       \n",
    "        ## standardization of the returns is employed to make training more stable\n",
    "        eps = np.finfo(np.float32).eps.item()\n",
    "        \n",
    "        ## eps is the smallest representable float, which is \n",
    "        # added to the standard deviation of the returns to avoid numerical instabilities\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "        \n",
    "        # Line 7:\n",
    "        policy_loss = []\n",
    "        for log_prob, disc_return in zip(saved_log_probs, returns):\n",
    "            policy_loss.append(-log_prob * disc_return)\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "        \n",
    "        # Line 8: PyTorch prefers gradient descent \n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i_episode % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YB0Cxrw1StrP"
   },
   "source": [
    "#### Lösung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NCNvyElRStWG"
   },
   "outputs": [],
   "source": [
    "def reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):\n",
    "    # Help us to calculate the score during the training\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    # Line 3 of pseudocode\n",
    "    for i_episode in range(1, n_training_episodes+1):\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state = env.reset()\n",
    "        # Line 4 of pseudocode\n",
    "        for t in range(max_t):\n",
    "            action, log_prob = policy.act(state)\n",
    "            saved_log_probs.append(log_prob)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break \n",
    "        scores_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "        \n",
    "        # Line 6 of pseudocode: calculate the return\n",
    "        returns = deque(maxlen=max_t) \n",
    "        n_steps = len(rewards) \n",
    "        # Compute the discounted returns at each timestep,\n",
    "        # as \n",
    "        #      the sum of the gamma-discounted return at time t (G_t) + the reward at time t\n",
    "        #\n",
    "        # In O(N) time, where N is the number of time steps\n",
    "        # (this definition of the discounted return G_t follows the definition of this quantity \n",
    "        # shown at page 44 of Sutton&Barto 2017 2nd draft)\n",
    "        # G_t = r_(t+1) + r_(t+2) + ...\n",
    "        \n",
    "        # Given this formulation, the returns at each timestep t can be computed \n",
    "        # by re-using the computed future returns G_(t+1) to compute the current return G_t\n",
    "        # G_t = r_(t+1) + gamma*G_(t+1)\n",
    "        # G_(t-1) = r_t + gamma* G_t\n",
    "        # (this follows a dynamic programming approach, with which we memorize solutions in order \n",
    "        # to avoid computing them multiple times)\n",
    "        \n",
    "        # This is correct since the above is equivalent to (see also page 46 of Sutton&Barto 2017 2nd draft)\n",
    "        # G_(t-1) = r_t + gamma*r_(t+1) + gamma*gamma*r_(t+2) + ...\n",
    "        \n",
    "        \n",
    "        ## Given the above, we calculate the returns at timestep t as: \n",
    "        #               gamma[t] * return[t] + reward[t]\n",
    "        #\n",
    "        ## We compute this starting from the last timestep to the first, in order\n",
    "        ## to employ the formula presented above and avoid redundant computations that would be needed \n",
    "        ## if we were to do it from first to last.\n",
    "        \n",
    "        ## Hence, the queue \"returns\" will hold the returns in chronological order, from t=0 to t=n_steps\n",
    "        ## thanks to the appendleft() function which allows to append to the position 0 in constant time O(1)\n",
    "        ## a normal python list would instead require O(N) to do this.\n",
    "        for t in range(n_steps)[::-1]:\n",
    "            disc_return_t = (returns[0] if len(returns)>0 else 0)\n",
    "            returns.appendleft( gamma*disc_return_t + rewards[t]   )    \n",
    "            \n",
    "        ## standardization of the returns is employed to make training more stable\n",
    "        eps = np.finfo(np.float32).eps.item()\n",
    "        ## eps is the smallest representable float, which is \n",
    "        # added to the standard deviation of the returns to avoid numerical instabilities        \n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "        \n",
    "        # Line 7:\n",
    "        policy_loss = []\n",
    "        for log_prob, disc_return in zip(saved_log_probs, returns):\n",
    "            policy_loss.append(-log_prob * disc_return)\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "        \n",
    "        # Line 8: PyTorch prefers gradient descent \n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i_episode % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RIWhQyJjfpEt"
   },
   "source": [
    "## Train it\n",
    "- Wir sind nun bereit, unseren Agenten zu trainieren.\n",
    "- Aber zuerst definieren wir eine Variable, die alle Trainingshyperparameter enthält.\n",
    "- Sie können die Trainingsparameter ändern (und sollten 😉 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "utRe1NgtVBYF"
   },
   "outputs": [],
   "source": [
    "cartpole_hyperparameters = {\n",
    "    \"h_size\": 16,\n",
    "    \"n_training_episodes\": 1000,\n",
    "    \"n_evaluation_episodes\": 10,\n",
    "    \"max_t\": 1000,\n",
    "    \"gamma\": 1.0,\n",
    "    \"lr\": 1e-2,\n",
    "    \"env_id\": env_id,\n",
    "    \"state_space\": s_size,\n",
    "    \"action_space\": a_size,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D3lWyVXBVfl6"
   },
   "outputs": [],
   "source": [
    "# Create policy and place it to the device\n",
    "cartpole_policy = Policy(cartpole_hyperparameters[\"state_space\"], cartpole_hyperparameters[\"action_space\"], cartpole_hyperparameters[\"h_size\"]).to(device)\n",
    "cartpole_optimizer = optim.Adam(cartpole_policy.parameters(), lr=cartpole_hyperparameters[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uGf-hQCnfouB"
   },
   "outputs": [],
   "source": [
    "scores = reinforce(cartpole_policy,\n",
    "                   cartpole_optimizer,\n",
    "                   cartpole_hyperparameters[\"n_training_episodes\"], \n",
    "                   cartpole_hyperparameters[\"max_t\"],\n",
    "                   cartpole_hyperparameters[\"gamma\"], \n",
    "                   100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qajj2kXqhB3g"
   },
   "source": [
    "## Bewertungsmethode definieren 📝\n",
    "- Hier definieren wir die Auswertungsmethode, mit der wir unseren Reinforce-Agenten testen werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3FamHmxyhBEU"
   },
   "outputs": [],
   "source": [
    "def evaluate_agent(env, max_steps, n_eval_episodes, policy):\n",
    "  \"\"\"\n",
    "  Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n",
    "  :param env: The evaluation environment\n",
    "  :param n_eval_episodes: Number of episode to evaluate the agent\n",
    "  :param policy: The Reinforce agent\n",
    "  \"\"\"\n",
    "  episode_rewards = []\n",
    "  for episode in range(n_eval_episodes):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards_ep = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "      action, _ = policy.act(state)\n",
    "      new_state, reward, done, info = env.step(action)\n",
    "      total_rewards_ep += reward\n",
    "        \n",
    "      if done:\n",
    "        break\n",
    "      state = new_state\n",
    "    episode_rewards.append(total_rewards_ep)\n",
    "  mean_reward = np.mean(episode_rewards)\n",
    "  std_reward = np.std(episode_rewards)\n",
    "\n",
    "  return mean_reward, std_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xdH2QCrLTrlT"
   },
   "source": [
    "## Bewerten Sie unseren Agenten 📈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ohGSXDyHh0xx"
   },
   "outputs": [],
   "source": [
    "evaluate_agent(eval_env, \n",
    "               cartpole_hyperparameters[\"max_t\"], \n",
    "               cartpole_hyperparameters[\"n_evaluation_episodes\"],\n",
    "               cartpole_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7CoeLkQ7TpO8"
   },
   "source": [
    "### Veröffentlichen Sie unser trainiertes Modell auf dem Hub 🔥\n",
    "Da wir nun gesehen haben, dass wir nach dem Training gute Ergebnisse erzielt haben, können wir unser trainiertes Modell mit einer Zeile Code auf dem Hub 🤗 veröffentlichen.\n",
    "\n",
    "Hier ist ein Beispiel für eine Model Card:\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/modelcard.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jmhs1k-cftIq"
   },
   "source": [
    "### Push zum Hub\n",
    "#### Diesen Code nicht verändern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LIVsvlW_8tcw"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, snapshot_download\n",
    "from huggingface_hub.repocard import metadata_eval_result, metadata_save\n",
    "\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import json\n",
    "import imageio\n",
    "\n",
    "import tempfile\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lo4JH45if81z"
   },
   "outputs": [],
   "source": [
    "def record_video(env, policy, out_directory, fps=30):\n",
    "  \"\"\"\n",
    "  Generate a replay video of the agent\n",
    "  :param env\n",
    "  :param Qtable: Qtable of our agent\n",
    "  :param out_directory\n",
    "  :param fps: how many frame per seconds (with taxi-v3 and frozenlake-v1 we use 1)\n",
    "  \"\"\"\n",
    "  images = []  \n",
    "  done = False\n",
    "  state = env.reset()\n",
    "  img = env.render(mode='rgb_array')\n",
    "  images.append(img)\n",
    "  while not done:\n",
    "    # Take the action (index) that have the maximum expected future reward given that state\n",
    "    action, _ = policy.act(state)\n",
    "    state, reward, done, info = env.step(action) # We directly put next_state = state for recording logic\n",
    "    img = env.render(mode='rgb_array')\n",
    "    images.append(img)\n",
    "  imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_TPdq47D7_f_"
   },
   "outputs": [],
   "source": [
    "def push_to_hub(repo_id, \n",
    "                model,\n",
    "                hyperparameters,\n",
    "                eval_env,\n",
    "                video_fps=30\n",
    "                ):\n",
    "  \"\"\"\n",
    "  Evaluate, Generate a video and Upload a model to Hugging Face Hub.\n",
    "  This method does the complete pipeline:\n",
    "  - It evaluates the model\n",
    "  - It generates the model card\n",
    "  - It generates a replay video of the agent\n",
    "  - It pushes everything to the Hub\n",
    "\n",
    "  :param repo_id: repo_id: id of the model repository from the Hugging Face Hub\n",
    "  :param model: the pytorch model we want to save\n",
    "  :param hyperparameters: training hyperparameters\n",
    "  :param eval_env: evaluation environment\n",
    "  :param video_fps: how many frame per seconds to record our video replay \n",
    "  \"\"\"\n",
    "\n",
    "  _, repo_name = repo_id.split(\"/\")\n",
    "  api = HfApi()\n",
    "  \n",
    "  # Step 1: Create the repo\n",
    "  repo_url = api.create_repo(\n",
    "        repo_id=repo_id,\n",
    "        exist_ok=True,\n",
    "  )\n",
    "\n",
    "  with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    local_directory = Path(tmpdirname)\n",
    "  \n",
    "    # Step 2: Save the model\n",
    "    torch.save(model, local_directory / \"model.pt\")\n",
    "\n",
    "    # Step 3: Save the hyperparameters to JSON\n",
    "    with open(local_directory / \"hyperparameters.json\", \"w\") as outfile:\n",
    "      json.dump(hyperparameters, outfile)\n",
    "    \n",
    "    # Step 4: Evaluate the model and build JSON\n",
    "    mean_reward, std_reward = evaluate_agent(eval_env, \n",
    "                                            hyperparameters[\"max_t\"],\n",
    "                                            hyperparameters[\"n_evaluation_episodes\"], \n",
    "                                            model)\n",
    "    # Get datetime\n",
    "    eval_datetime = datetime.datetime.now()\n",
    "    eval_form_datetime = eval_datetime.isoformat()\n",
    "\n",
    "    evaluate_data = {\n",
    "          \"env_id\": hyperparameters[\"env_id\"], \n",
    "          \"mean_reward\": mean_reward,\n",
    "          \"n_evaluation_episodes\": hyperparameters[\"n_evaluation_episodes\"],\n",
    "          \"eval_datetime\": eval_form_datetime,\n",
    "    }\n",
    "\n",
    "    # Write a JSON file\n",
    "    with open(local_directory / \"results.json\", \"w\") as outfile:\n",
    "        json.dump(evaluate_data, outfile)\n",
    "\n",
    "    # Step 5: Create the model card\n",
    "    env_name = hyperparameters[\"env_id\"]\n",
    "    \n",
    "    metadata = {}\n",
    "    metadata[\"tags\"] = [\n",
    "          env_name,\n",
    "          \"reinforce\",\n",
    "          \"reinforcement-learning\",\n",
    "          \"custom-implementation\",\n",
    "          \"deep-rl-class\"\n",
    "      ]\n",
    "\n",
    "    # Add metrics\n",
    "    eval = metadata_eval_result(\n",
    "        model_pretty_name=repo_name,\n",
    "        task_pretty_name=\"reinforcement-learning\",\n",
    "        task_id=\"reinforcement-learning\",\n",
    "        metrics_pretty_name=\"mean_reward\",\n",
    "        metrics_id=\"mean_reward\",\n",
    "        metrics_value=f\"{mean_reward:.2f} +/- {std_reward:.2f}\",\n",
    "        dataset_pretty_name=env_name,\n",
    "        dataset_id=env_name,\n",
    "      )\n",
    "\n",
    "    # Merges both dictionaries\n",
    "    metadata = {**metadata, **eval}\n",
    "\n",
    "    model_card = f\"\"\"\n",
    "  # **Reinforce** Agent playing **{env_id}**\n",
    "  This is a trained model of a **Reinforce** agent playing **{env_id}** .\n",
    "  To learn to use this model and train yours check Unit 4 of the Deep Reinforcement Learning Course: https://huggingface.co/deep-rl-course/unit4/introduction\n",
    "  \"\"\"\n",
    "\n",
    "    readme_path = local_directory / \"README.md\"\n",
    "    readme = \"\"\n",
    "    if readme_path.exists():\n",
    "        with readme_path.open(\"r\", encoding=\"utf8\") as f:\n",
    "          readme = f.read()\n",
    "    else:\n",
    "      readme = model_card\n",
    "\n",
    "    with readme_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "      f.write(readme)\n",
    "\n",
    "    # Save our metrics to Readme metadata\n",
    "    metadata_save(readme_path, metadata)\n",
    "\n",
    "    # Step 6: Record a video\n",
    "    video_path =  local_directory / \"replay.mp4\"\n",
    "    record_video(env, model, video_path, video_fps)\n",
    "\n",
    "    # Step 7. Push everything to the Hub\n",
    "    api.upload_folder(\n",
    "          repo_id=repo_id,\n",
    "          folder_path=local_directory,\n",
    "          path_in_repo=\".\",\n",
    "    )\n",
    "\n",
    "    print(f\"Your model is pushed to the Hub. You can view your model here: {repo_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w17w8CxzoURM"
   },
   "source": [
    "### .\n",
    "\n",
    "Mit \"push_to_hub\" **werten Sie aus, zeichnen ein Replay auf, generieren eine Modellkarte Ihres Agenten und schieben sie an den Hub**.\n",
    "\n",
    "This way:\n",
    "- Sie können **unsere Arbeit vorführen** 🔥.\n",
    "- Sie können **Ihren Agenten beim Spielen visualisieren** 👀\n",
    "- Du kannst **einen Agenten mit der Community teilen, den andere benutzen können** 💾\n",
    "- Sie können **auf eine Bestenliste 🏆 zugreifen, um zu sehen, wie gut Ihr Agent im Vergleich zu Ihren Klassenkameraden abschneidet** 👉 https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cWnFC0iZooTw"
   },
   "source": [
    "Um Ihr Modell mit der Gemeinschaft teilen zu können, sind drei weitere Schritte erforderlich:\n",
    "\n",
    "1️⃣ (Falls noch nicht geschehen) Erstellen Sie ein Konto für HF ➡ https://huggingface.co/join\n",
    "\n",
    "2️⃣ Melde dich an und speichere dann dein Authentifizierungs-Token von der Hugging Face Website.\n",
    "- Erstellen Sie ein neues Token (https://huggingface.co/settings/tokens) **mit Schreibrolle**\n",
    "\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/create-token.jpg\" alt=\"HF-Token erstellen\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QB5nIcxR8paT"
   },
   "outputs": [],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GyWc1x3-o3xG"
   },
   "source": [
    "Wenn Sie kein Google Colab oder ein Jupyter Notebook verwenden möchten, müssen Sie stattdessen diesen Befehl verwenden: `huggingface-cli login` (oder `login`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-D-zhbRoeOm"
   },
   "source": [
    "3️⃣ Wir sind jetzt bereit, unseren trainierten Agenten mit der Funktion `package_to_hub()` an den 🤗 Hub 🔥 zu übertragen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UNwkTS65Uq3Q"
   },
   "outputs": [],
   "source": [
    "repo_id = \"\" #TODO Define your repo id {username/Reinforce-{model-id}}\n",
    "push_to_hub(repo_id,\n",
    "                cartpole_policy, # The model we want to save\n",
    "                cartpole_hyperparameters, # Hyperparameters\n",
    "                eval_env, # Evaluation environment\n",
    "                video_fps=30\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jrnuKH1gYZSz"
   },
   "source": [
    "Nachdem wir nun die Robustheit unserer Implementierung getestet haben, wollen wir eine komplexere Umgebung ausprobieren: PixelCopter 🚁.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JNLVmKKVKA6j"
   },
   "source": [
    "## Zweiter Agent: PixelCopter 🚁.\n",
    "\n",
    "### Studiere die PixelCopter-Umgebung 👀\n",
    "- [Die Umgebungsdokumentation](https://pygame-learning-environment.readthedocs.io/en/latest/user/games/pixelcopter.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JBSc8mlfyin3"
   },
   "outputs": [],
   "source": [
    "env_id = \"Pixelcopter-PLE-v0\"\n",
    "env = gym.make(env_id)\n",
    "eval_env = gym.make(env_id)\n",
    "s_size = env.observation_space.shape[0]\n",
    "a_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L5u_zAHsKBy7"
   },
   "outputs": [],
   "source": [
    "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
    "print(\"The State Space is: \", s_size)\n",
    "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D7yJM9YXKNbq"
   },
   "outputs": [],
   "source": [
    "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
    "print(\"The Action Space is: \", a_size)\n",
    "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NNWvlyvzalXr"
   },
   "source": [
    "Der Beobachtungsraum (7) 👀:\n",
    "- Spieler-Y-Position\n",
    "- Spieler-Geschwindigkeit\n",
    "- Abstand des Spielers zum Boden\n",
    "- Abstand des Spielers zur Decke\n",
    "- nächster Block x Abstand zum Spieler\n",
    "- nächste Blöcke obere y-Position\n",
    "- nächste Blöcke untere y-Position\n",
    "\n",
    "Der Aktionsraum(2) 🎮:\n",
    "- Nach oben (Gaspedal drücken)\n",
    "- Nichts tun (nicht auf den Beschleuniger drücken)\n",
    "\n",
    "Die Belohnungsfunktion 💰:\n",
    "- Für jeden vertikalen Block, den er durchläuft, erhält er eine positive Belohnung von +1. Jedes Mal, wenn ein Endzustand erreicht wird, erhält er eine negative Belohnung von -1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aV1466QP8crz"
   },
   "source": [
    "### Definieren Sie die neue Richtlinie 🧠\n",
    "- Wir brauchen ein tieferes neuronales Netz, da die Umgebung komplexer ist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I1eBkCiX2X_S"
   },
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, s_size, a_size, h_size):\n",
    "        super(Policy, self).__init__()\n",
    "        # Define the three layers here\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Define the forward process here\n",
    "        return F.softmax(x, dim=1)\n",
    "    \n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        probs = self.forward(state).cpu()\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47iuAFqV8Ws-"
   },
   "source": [
    "#### Lösung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wrNuVcHC8Xu7"
   },
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, s_size, a_size, h_size):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(s_size, h_size)\n",
    "        self.fc2 = nn.Linear(h_size, h_size*2)\n",
    "        self.fc3 = nn.Linear(h_size*2, a_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "    \n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        probs = self.forward(state).cpu()\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SM1QiGCSbBkM"
   },
   "source": [
    "### Definieren Sie die Hyperparameter ⚙️\n",
    "- Denn diese Umgebung ist komplexer.\n",
    "- Insbesondere für die versteckte Größe benötigen wir mehr Neuronen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y0uujOR_ypB6"
   },
   "outputs": [],
   "source": [
    "pixelcopter_hyperparameters = {\n",
    "    \"h_size\": 64,\n",
    "    \"n_training_episodes\": 50000,\n",
    "    \"n_evaluation_episodes\": 10,\n",
    "    \"max_t\": 10000,\n",
    "    \"gamma\": 0.99,\n",
    "    \"lr\": 1e-4,\n",
    "    \"env_id\": env_id,\n",
    "    \"state_space\": s_size,\n",
    "    \"action_space\": a_size,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wyvXTJWm9GJG"
   },
   "source": [
    "### Train it\n",
    "- Wir sind jetzt bereit, unseren Agenten zu trainieren 🔥."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7mM2P_ckysFE"
   },
   "outputs": [],
   "source": [
    "# Create policy and place it to the device\n",
    "# torch.manual_seed(50)\n",
    "pixelcopter_policy = Policy(pixelcopter_hyperparameters[\"state_space\"], pixelcopter_hyperparameters[\"action_space\"], pixelcopter_hyperparameters[\"h_size\"]).to(device)\n",
    "pixelcopter_optimizer = optim.Adam(pixelcopter_policy.parameters(), lr=pixelcopter_hyperparameters[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v1HEqP-fy-Rf"
   },
   "outputs": [],
   "source": [
    "scores = reinforce(pixelcopter_policy,\n",
    "                   pixelcopter_optimizer,\n",
    "                   pixelcopter_hyperparameters[\"n_training_episodes\"], \n",
    "                   pixelcopter_hyperparameters[\"max_t\"],\n",
    "                   pixelcopter_hyperparameters[\"gamma\"], \n",
    "                   1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8kwFQ-Ip85BE"
   },
   "source": [
    "### Veröffentlichen Sie unser trainiertes Modell auf dem Hub 🔥."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6PtB7LRbTKWK"
   },
   "outputs": [],
   "source": [
    "repo_id = \"\" #TODO Define your repo id {username/Reinforce-{model-id}}\n",
    "push_to_hub(repo_id,\n",
    "                pixelcopter_policy, # The model we want to save\n",
    "                pixelcopter_hyperparameters, # Hyperparameters\n",
    "                eval_env, # Evaluation environment\n",
    "                video_fps=30\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7VDcJ29FcOyb"
   },
   "source": [
    "## Einige zusätzliche Herausforderungen 🏆\n",
    "Die beste Art zu lernen **ist, Dinge selbst auszuprobieren**! Wie Sie gesehen haben, ist der derzeitige Agent nicht besonders gut. Als ersten Vorschlag können Sie für mehr Schritte trainieren. Versuchen Sie aber auch, bessere Parameter zu finden.\n",
    "\n",
    "In der [Rangliste] (https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) finden Sie Ihre Agenten. Können Sie sich an die Spitze setzen?\n",
    "\n",
    "Hier sind einige Ideen, um dies zu erreichen:\n",
    "* Trainiere mehr Schritte\n",
    "* Probiere verschiedene Hyperparameter aus, indem du dir ansiehst, was deine Klassenkameraden gemacht haben 👉 https://huggingface.co/models?other=reinforce\n",
    "* **Pushen Sie Ihr neu trainiertes Modell** auf dem Hub 🔥\n",
    "* **Verbesserung der Implementierung für komplexere Umgebungen** (wie wäre es z.B., das Netzwerk in ein Convolutional Neural Network zu ändern, um\n",
    "Frames als Beobachtung zu behandeln)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x62pP0PHdA-y"
   },
   "source": [
    "________________________________________________________________________\n",
    "\n",
    "**Glückwunsch zum Abschluss dieser Einheit**! Es gab eine Menge Informationen.\n",
    "Und herzlichen Glückwunsch zum Abschluss des Tutorials. Du hast gerade deinen ersten Deep Reinforcement Learning-Agenten von Grund auf mit PyTorch programmiert und ihn im Hub geteilt 🥳.\n",
    "\n",
    "Zögern Sie nicht, diese Einheit zu iterieren **durch Verbesserung der Implementierung für komplexere Umgebungen** (wie wäre es zum Beispiel, das Netzwerk in ein Convolutional Neural Network zu ändern, um\n",
    "Frames als Beobachtung zu behandeln)?\n",
    "\n",
    "In der nächsten Einheit **werden wir mehr über Unity MLAgents** lernen, indem wir Agenten in Unity-Umgebungen trainieren. Auf diese Weise werden Sie bereit sein, an den **AI vs. AI Herausforderungen teilzunehmen, bei denen Sie Ihre Agenten trainieren werden\n",
    "gegen andere Agenten in einer Schneeballschlacht und einem Fußballspiel antreten**.\n",
    "\n",
    "Klingt lustig? Bis zum nächsten Mal!\n",
    "\n",
    "Zum Schluss würden wir gerne **hören, was Sie über den Kurs denken und wie wir ihn verbessern können**. Wenn Sie also ein Feedback haben, bitte 👉 [füllen Sie dieses Formular aus] (https://forms.gle/BzKXWzLAGZESGNaE9)\n",
    "\n",
    "Wir sehen uns in Referat 5! 🔥\n",
    "\n",
    "### Keep Learning, stay awesome 🤗\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "BPLwsPajb1f8",
    "L_WSo0VUV99t",
    "mjY-eq3eWh9O",
    "JoTC9o2SczNn",
    "gfGJNZBUP7Vn",
    "YB0Cxrw1StrP",
    "47iuAFqV8Ws-",
    "x62pP0PHdA-y"
   ],
   "include_colab_link": true,
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
