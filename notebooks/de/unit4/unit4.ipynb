{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CjRWziAVU2lZ"
   },
   "source": [
    "# Unit 4: Programmieren Sie Ihren ersten Deep Reinforcement Learning Algorithmus mit PyTorch: Reinforce. Und teste seine Robustheit üí™.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/thumbnail.png\" alt=\"thumbnail\"/>\n",
    "\n",
    "\n",
    "In diesem Notizbuch werden Sie Ihren ersten Deep Reinforcement Learning-Algorithmus von Grund auf programmieren: Reinforce (auch Monte Carlo Policy Gradient genannt).\n",
    "\n",
    "Reinforce ist eine *Policy-basierte Methode*: ein Deep Reinforcement Learning-Algorithmus, der versucht, **die Policy direkt zu optimieren, ohne eine Action-Value-Funktion** zu verwenden.\n",
    "\n",
    "Genauer gesagt ist Reinforce eine *Policy-Gradient-Methode*, eine Unterklasse von *Policy-basierten Methoden*, die darauf abzielt, **die Policy direkt zu optimieren, indem sie die Gewichte der optimalen Policy mithilfe des Gradientenanstiegs sch√§tzt**.\n",
    "\n",
    "Um ihre Robustheit zu testen, werden wir sie in 2 verschiedenen einfachen Umgebungen trainieren:\n",
    "- Cartpole-v1\n",
    "- Pixelcopter-Umgebung\n",
    "\n",
    "‚¨áÔ∏è Hier ist ein Beispiel f√ºr das, was **am Ende dieses Notizbuchs erreicht wird** ‚¨áÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4rBom2sbo7S"
   },
   "source": [
    "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/envs.gif\" alt=\"Umgebungen\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BPLwsPajb1f8"
   },
   "source": [
    "### üéÆ Umgebungen:\n",
    "\n",
    "- [CartPole-v1](https://www.gymlibrary.dev/environments/classic_control/cart_pole/)\n",
    "- [PixelCopter](https://pygame-learning-environment.readthedocs.io/en/latest/user/games/pixelcopter.html)\n",
    "\n",
    "### üìö RL-Library:\n",
    "\n",
    "- Python\n",
    "- PyTorch\n",
    "\n",
    "\n",
    "Wir versuchen st√§ndig, unsere Tutorials zu verbessern. **Wenn Sie also Fehler in diesem Notizbuch** finden, √∂ffnen Sie bitte [einen Fehler im GitHub Repo](https://github.com/huggingface/deep-rl-class/issues)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_WSo0VUV99t"
   },
   "source": [
    "## Ziele dieses Notizbuchs üèÜ\n",
    "Am Ende des Notizbuchs werden Sie:\n",
    "- In der Lage sein, **einen Reinforce-Algorithmus mit PyTorch von Grund auf zu programmieren.**\n",
    "- In der Lage sein, **die Robustheit deines Agenten unter Verwendung einfacher Umgebungen zu testen.**\n",
    "- In der Lage sein, **Ihren trainierten Agenten auf den Hub** zu pushen, mit einer sch√∂nen Videowiedergabe und einem Bewertungsergebnis üî•."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lEPrZg2eWa4R"
   },
   "source": [
    "## Dieses Notizbuch stammt aus dem Deep Reinforcement Learning Kurs.\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/deep-rl-course-illustration.jpg\" alt=\"Deep RL Course illustration\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6p5HnEefISCB"
   },
   "source": [
    "In diesem kostenlosen Kurs lernen Sie:\n",
    "\n",
    "- üìñ Deep Reinforcement Learning in **Theorie und Praxis** studieren.\n",
    "- üßë‚Äçüíª Lernen Sie, **ber√ºhmte Deep RL-Bibliotheken** wie Stable Baselines3, RL Baselines3 Zoo, CleanRL und Sample Factory 2.0 zu verwenden.\n",
    "- ü§ñ Trainieren Sie **Agenten in einzigartigen Umgebungen**.\n",
    "\n",
    "Und mehr, siehe üìö den Lehrplan üëâ https://simoninithomas.github.io/deep-rl-course\n",
    "\n",
    "Vergessen Sie nicht, sich **<a href=\"http://eepurl.com/ic5ZUD\">f√ºr den Kurs anzumelden</a>** (wir sammeln Ihre E-Mail, um Ihnen **die Links zu senden, wenn die einzelnen Einheiten ver√∂ffentlicht werden, und Sie √ºber die Herausforderungen und Aktualisierungen zu informieren).**\n",
    "\n",
    "\n",
    "Der beste Weg, um in Kontakt zu bleiben, ist, unserem Discord-Server beizutreten, um sich mit der Community und mit uns auszutauschen üëâüèª https://discord.gg/ydHrjt3WP5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Was sind die richtlinienbasierten Methoden?\n",
    "\n",
    "Das Hauptziel des Reinforcement Learning ist es, **die optimale Strategie zu finden, die die erwartete kumulative Belohnung maximiert**.\n",
    "Da das Reinforcement Learning auf der *Belohnungshypothese* basiert: **alle Ziele k√∂nnen als Maximierung der erwarteten kumulativen Belohnung beschrieben werden**.\n",
    "\n",
    "Bei einem Fu√üballspiel zum Beispiel (bei dem man die Agenten in zwei Einheiten trainiert) ist das Ziel, das Spiel zu gewinnen. Wir k√∂nnen dieses Ziel beim Reinforcement Learning wie folgt beschreiben\n",
    "**Maximierung der Anzahl der erzielten Tore** (wenn der Ball die Torlinie √ºberquert) in die Fu√üballtore des Gegners. Und **Minimierung der Anzahl der Tore in den eigenen Fu√üballtoren**.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/soccer.jpg\" alt=\"Fu√üball\" />\n",
    "\n",
    "## Wertbasierte, richtlinienbasierte und akteurskritische Methoden\n",
    "\n",
    "In der ersten Einheit haben wir zwei Methoden kennengelernt, um die optimale Policy zu finden (oder sich ihr in den meisten F√§llen zu n√§hern).\n",
    "\n",
    "- Bei *wertbasierten Methoden* lernen wir eine Wertfunktion.\n",
    "  - Die Idee ist, dass eine optimale Wertfunktion zu einer optimalen Policy \\\\(\\pi^{*}\\) f√ºhrt.\n",
    "  - Unser Ziel ist es, **den Verlust zwischen dem vorhergesagten und dem Zielwert zu minimieren**, um die wahre Aktions-Wert-Funktion zu approximieren.\n",
    "  - Wir haben eine Strategie, aber sie ist implizit, da sie **direkt aus der Wertfunktion** generiert wird. Beim Q-Learning haben wir zum Beispiel eine (epsilon-)gierige Strategie verwendet.\n",
    "\n",
    "- Im Gegensatz dazu lernen wir bei *policy-basierten Methoden* direkt, \\\\(\\pi^{*}\\) zu approximieren, ohne eine Wertfunktion lernen zu m√ºssen.\n",
    "  - Die Idee ist **die Parametrisierung der Policy**. Wenn man beispielsweise ein neuronales Netz \\\\(\\pi_\\theta\\\\) verwendet, wird diese Strategie eine Wahrscheinlichkeitsverteilung √ºber Aktionen ausgeben (stochastische Strategie).\n",
    "  - <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/stochastic_policy.png\" alt=\"stochastic policy\" />\n",
    "  - Unser Ziel ist dann **die Maximierung der Leistung der parametrisierten Policy unter Verwendung des Gradientenanstiegs**.\n",
    "  - Zu diesem Zweck kontrollieren wir den Parameter \\\\(\\theta\\), der die Verteilung der Aktionen √ºber einen Zustand beeinflussen wird.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/policy_based.png\" alt=\"Policy based\" />\n",
    "\n",
    "- N√§chstes Mal werden wir uns mit der *Actor-Critic*-Methode befassen, die eine Kombination aus wertbasierten und richtlinienbasierten Methoden darstellt.\n",
    "\n",
    "Dank der Policybasierten Methoden k√∂nnen wir unsere Policy \\\\(\\pi_\\theta\\\\) direkt optimieren, um eine Wahrscheinlichkeitsverteilung √ºber Aktionen \\\\(\\pi_\\theta(a|s)\\\\\\) auszugeben, die zur besten kumulativen Rendite f√ºhrt.\n",
    "Dazu definieren wir eine Zielfunktion \\\\(J(\\theta)\\\\), d.h. die erwartete kumulative Belohnung, und wir **m√∂chten den Wert \\\\(\\theta\\\\) finden, der diese Zielfunktion maximiert**.\n",
    "\n",
    "## Der Unterschied zwischen richtlinienbasierten und richtliniengradientenbasierten Methoden\n",
    "\n",
    "Policy-Gradient-Methoden, die wir in dieser Einheit untersuchen werden, sind eine Unterklasse der Policy-basierten Methoden. Bei richtlinienbasierten Methoden erfolgt die Optimierung die meiste Zeit *auf der Grundlage von Richtlinien*, da wir f√ºr jede Aktualisierung nur Daten (Trajektorien) verwenden, die **durch unsere letzte Version von** gesammelt wurden.\n",
    "\n",
    "Der Unterschied zwischen diesen beiden Methoden **liegt darin, wie wir den Parameter** \\\\(\\theta\\\\) optimieren:\n",
    "\n",
    "- Bei *Policy-basierten Methoden* suchen wir direkt nach der optimalen Policy. Wir k√∂nnen den Parameter \\\\(\\theta\\\\) **indirekt** optimieren, indem wir die lokale Approximation der Zielfunktion mit Techniken wie Hill Climbing, Simulated Annealing oder Evolutionsstrategien maximieren.\n",
    "- Bei *Policy-Gradient-Methoden*, die eine Unterklasse der Policy-basierten Methoden sind, suchen wir direkt nach der optimalen Policy. Wir optimieren jedoch den Parameter \\\\(\\theta\\) **direkt**, indem wir den Gradientenanstieg auf die Leistung der Zielfunktion \\\\(J(\\theta)\\\\) anwenden.\n",
    "\n",
    "Bevor wir uns n√§her mit der Funktionsweise von Policy-Gradient-Methoden befassen (Zielfunktion, Policy-Gradient-Theorem, Gradientenanstieg usw.), wollen wir die Vor- und Nachteile von Policy-basierten Methoden untersuchen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Die Vor- und Nachteile der Policy-Gradient-Methoden\n",
    "\n",
    "An dieser Stelle werden Sie vielleicht fragen: \"Aber Deep Q-Learning ist hervorragend! Warum also Policy-Gradient-Methoden verwenden?\". Um diese Frage zu beantworten, lassen Sie uns die **Vor- und Nachteile von Policy-Gradient-Methoden** untersuchen.\n",
    "\n",
    "## Vorteile\n",
    "\n",
    "Es gibt mehrere Vorteile gegen√ºber wertbasierten Methoden. Sehen wir uns einige von ihnen an:\n",
    "\n",
    "### Die Einfachheit der Integration\n",
    "\n",
    "Wir k√∂nnen die Policy direkt sch√§tzen, ohne zus√§tzliche Daten (Aktionswerte) zu speichern.\n",
    "\n",
    "### Policy-Gradient-Methoden k√∂nnen eine stochastische Policy lernen\n",
    "\n",
    "Policy-Gradient-Methoden k√∂nnen **eine stochastische Policy erlernen, w√§hrend Wertfunktionen dies nicht k√∂nnen**.\n",
    "\n",
    "Dies hat zwei Konsequenzen:\n",
    "\n",
    "1. Wir **m√ºssen einen Kompromiss zwischen Exploration und Ausbeutung nicht von Hand implementieren**. Da wir eine Wahrscheinlichkeitsverteilung √ºber Aktionen ausgeben, erkundet der Agent **den Zustandsraum, ohne immer dieselbe Flugbahn zu nehmen**.\n",
    "\n",
    "2. Wir beseitigen auch das Problem des **perceptual aliasing**. Perceptual aliasing bedeutet, dass zwei Zust√§nde gleich erscheinen (oder sind), aber unterschiedliche Aktionen erfordern.\n",
    "\n",
    "Nehmen wir ein Beispiel: Wir haben einen intelligenten Staubsauger, dessen Ziel es ist, den Staub zu saugen und die Hamster nicht zu t√∂ten.\n",
    "\n",
    "<figure class=\"image table text-center m-0 w-full\">\n",
    "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/hamster1.jpg\" alt=\"Hamster 1\"/>\n",
    "</figure>\n",
    "\n",
    "Unser Staubsauger kann nur wahrnehmen, wo die W√§nde sind.\n",
    "\n",
    "Das Problem ist, dass die **zwei roten (farbigen) Zust√§nde Aliasing-Zust√§nde sind, weil der Agent jeweils eine obere und untere Wand wahrnimmt**.\n",
    "\n",
    "<figure class=\"image table text-center m-0 w-full\">\n",
    "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/hamster2.jpg\" alt=\"Hamster 1\"/>\n",
    "</figure>\n",
    "\n",
    "Bei einer deterministischen Strategie wird der Agent entweder immer nach rechts gehen, wenn er sich im roten Bereich befindet, oder immer nach links. **In beiden F√§llen bleibt unser Agent stecken und kann sich nicht aus dem Staub machen**.\n",
    "\n",
    "Mit einem wertbasierten Verst√§rkungslernalgorithmus lernen wir eine **quasi-deterministische Strategie** (\"gierige Epsilon-Strategie\"). Folglich kann unser Agent **viel Zeit damit verbringen, den Staub zu finden**.\n",
    "\n",
    "Andererseits bewegt sich eine optimale stochastische Strategie **in roten (farbigen) Zust√§nden zuf√§llig nach links oder rechts**. Folglich **bleibt er nicht stecken und erreicht den Zielzustand mit einer hohen Wahrscheinlichkeit**.\n",
    "\n",
    "<figure class=\"image table text-center m-0 w-full\">\n",
    "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/hamster3.jpg\" alt=\"Hamster 1\"/>\n",
    "</figure>\n",
    "\n",
    "### Policy-Gradient-Methoden sind in hochdimensionalen Aktionsr√§umen und kontinuierlichen Aktionsr√§umen effektiver.\n",
    "\n",
    "Das Problem mit Deep Q-learning ist, dass ihre **Vorhersagen in jedem Zeitschritt eine Punktzahl (maximale erwartete zuk√ºnftige Belohnung) f√ºr jede m√∂gliche Aktion** zuweisen, wenn der aktuelle Zustand gegeben ist.\n",
    "\n",
    "Was aber, wenn es unendlich viele Handlungsm√∂glichkeiten gibt?\n",
    "\n",
    "Bei einem selbstfahrenden Auto zum Beispiel kann man in jedem Zustand eine (fast) unendliche Auswahl an Aktionen haben (das Lenkrad um 15¬∞, 17,2¬∞, 19,4¬∞ drehen, hupen usw.). **Wir m√ºssen f√ºr jede m√∂gliche Aktion einen Q-Wert ausgeben**! Und **die maximale Aktion einer kontinuierlichen Ausgabe zu w√§hlen, ist selbst ein Optimierungsproblem**!\n",
    "\n",
    "Mit Policy-Gradient-Methoden geben wir stattdessen eine **Wahrscheinlichkeitsverteilung √ºber Aktionen** aus.\n",
    "\n",
    "### Policy-Gradient-Methoden haben bessere Konvergenzeigenschaften\n",
    "\n",
    "Bei wertbasierten Methoden verwenden wir einen aggressiven Operator, um **die Wertfunktion zu √§ndern: wir nehmen das Maximum √ºber Q-Sch√§tzungen**.\n",
    "Folglich k√∂nnen sich die Aktionswahrscheinlichkeiten bei einer beliebig kleinen √Ñnderung der gesch√§tzten Aktionswerte drastisch √§ndern, wenn diese √Ñnderung dazu f√ºhrt, dass eine andere Aktion den maximalen Wert hat.\n",
    "\n",
    "Wenn z. B. w√§hrend des Trainings die beste Aktion links war (mit einem Q-Wert von 0,22) und im n√§chsten Trainingsschritt rechts (da der rechte Q-Wert 0,23 wird), haben wir die Strategie drastisch ge√§ndert, da die Strategie nun die meiste Zeit rechts statt links w√§hlt.\n",
    "\n",
    "Andererseits √§ndern sich bei Policy-Gradient-Methoden die stochastischen Handlungspr√§ferenzen (Wahrscheinlichkeit, eine Handlung vorzunehmen) **mit der Zeit gleichm√§√üig**.\n",
    "\n",
    "## Nachteile\n",
    "\n",
    "Nat√ºrlich haben Policy-Gradient-Methoden auch einige Nachteile:\n",
    "\n",
    "- **H√§ufig konvergiert die Policy-Gradient-Methode zu einem lokalen Maximum anstatt zu einem globalen Optimum**.\n",
    "- Policy-Gradient-Methoden gehen langsamer vor, **Schritt f√ºr Schritt: es kann l√§nger dauern, sie zu trainieren (ineffizient).**\n",
    "- Policy-Gradient kann eine hohe Varianz haben. Wir werden in der Einheit zur Akteurskritik sehen, warum das so ist, und wie wir dieses Problem l√∂sen k√∂nnen.\n",
    "\n",
    "üëâ Wenn Sie sich eingehender mit den Vor- und Nachteilen von Policy-Gradient-Methoden besch√§ftigen m√∂chten, [k√∂nnen Sie sich dieses Video ansehen] (https://youtu.be/y3oqOjHilio).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vertiefung der Policy-Gradient-Methoden\n",
    "\n",
    "## Das gro√üe Ganze im Blick\n",
    "\n",
    "Wir haben gerade gelernt, dass Policy-Gradient-Methoden darauf abzielen, Parameter \\\\( \\theta \\\\) zu finden, die **den erwarteten Ertrag** maximieren.\n",
    "\n",
    "Die Idee ist, dass wir eine *parametrisierte stochastische Strategie* haben. In unserem Fall gibt ein neuronales Netz eine Wahrscheinlichkeitsverteilung √ºber Aktionen aus. Die Wahrscheinlichkeit, jede Aktion durchzuf√ºhren, wird auch als *Aktionspr√§ferenz* bezeichnet.\n",
    "\n",
    "Nehmen wir das Beispiel von CartPole-v1:\n",
    "- Als Eingabe haben wir einen Zustand.\n",
    "- Als Ausgabe haben wir eine Wahrscheinlichkeitsverteilung √ºber Aktionen in diesem Zustand.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/policy_based.png\" alt=\"Policy based\" />\n",
    "\n",
    "Unser Ziel mit Policy-Gradient ist es, **die Wahrscheinlichkeitsverteilung von Aktionen** zu steuern, indem wir die Policy so abstimmen, dass **gute Aktionen (die den Ertrag maximieren) in der Zukunft h√§ufiger abgerufen werden**.\n",
    "Jedes Mal, wenn der Agent mit der Umwelt interagiert, ver√§ndern wir die Parameter so, dass gute Aktionen in der Zukunft h√§ufiger durchgef√ºhrt werden.\n",
    "\n",
    "Aber **wie sollen wir die Gewichte anhand des erwarteten Ertrags** optimieren?\n",
    "\n",
    "Die Idee ist, dass wir den Agenten **w√§hrend einer Episode interagieren lassen**. Wenn wir die Episode gewinnen, gehen wir davon aus, dass jede get√§tigte Aktion gut war und in Zukunft h√§ufiger durchgef√ºhrt werden muss\n",
    "da sie zum Sieg f√ºhren.\n",
    "\n",
    "Wir wollen also f√ºr jedes Zustands-Aktions-Paar die \\\\(P(a|s)\\\\) erh√∂hen: die Wahrscheinlichkeit, diese Aktion in diesem Zustand durchzuf√ºhren. Oder verringern, wenn wir verloren haben.\n",
    "\n",
    "Der Policy-Gradient-Algorithmus (vereinfacht) sieht wie folgt aus:\n",
    "<figure class=\"image table text-center m-0 w-full\">\n",
    "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/pg_bigpicture.jpg\" alt=\"Policy Gradient Big Picture\"/>\n",
    "</figure>\n",
    "\n",
    "Jetzt, wo wir das gro√üe Bild haben, wollen wir tiefer in die Methoden des Policy-Gradienten eintauchen.\n",
    "\n",
    "## Vertiefung der Policy-Gradient-Methoden\n",
    "\n",
    "Wir haben unsere stochastische Policy \\\\(\\pi\\\\), die einen Parameter \\\\(\\theta\\\\) hat. Dieses \\\\(\\pi\\\\) gibt bei einem Zustand **eine Wahrscheinlichkeitsverteilung von Aktionen** aus.\n",
    "\n",
    "<figure class=\"image table text-center m-0 w-full\">\n",
    "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/stochastic_policy.png\" alt=\"Policy\"/>\n",
    "</figure>\n",
    "\n",
    "Wobei \\\\(\\pi_\\theta(a_t|s_t)\\\\\\) die Wahrscheinlichkeit ist, dass der Agent die Aktion \\\\(a_t\\\\) aus dem Zustand \\\\(s_t\\\\\\) ausw√§hlt, wenn unsere Policy gegeben ist.\n",
    "\n",
    "**Aber woher wissen wir, ob unsere Strategie gut ist?** Wir brauchen eine M√∂glichkeit, sie zu messen. Um das zu wissen, definieren wir eine Punktzahl/Zielfunktion namens \\\\(J(\\theta)\\\\).\n",
    "\n",
    "### Die Zielfunktion\n",
    "\n",
    "Die *Zielfunktion* gibt uns die **Leistung des Agenten** bei einer Trajektorie (Zustandsaktionsfolge ohne Ber√ºcksichtigung der Belohnung (im Gegensatz zu einer Episode)), und sie gibt die *erwartete kumulative Belohnung* aus.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/objective.jpg\" alt=\"Return\"/>\n",
    "\n",
    "Lassen Sie uns diese Formel etwas genauer erl√§utern:\n",
    "- Die *erwartete Rendite* (auch *erwartete kumulative Belohnung* genannt), ist der gewichtete Durchschnitt (wobei die Gewichte durch \\\\(P(\\tau;\\theta)\\\\) aller m√∂glichen Werte gegeben sind, die die Rendite \\\\(R(\\tau)\\\\) annehmen kann).\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/expected_reward.png\" alt=\"Return\"/>\n",
    "\n",
    "\n",
    "- \\\\(R(\\tau)\\\\) :  Die R√ºckkehr von einer beliebigen Flugbahn. Um diese Gr√∂√üe zur Berechnung der erwarteten Rendite zu verwenden, m√ºssen wir sie mit der Wahrscheinlichkeit jeder m√∂glichen Trajektorie multiplizieren.\n",
    "\n",
    "- \\\\(P(\\tau;\\theta)\\\\) : Wahrscheinlichkeit jeder m√∂glichen Trajektorie \\\\(\\tau\\\\) (diese Wahrscheinlichkeit h√§ngt von \\\\( \\theta\\\\) ab, da sie die Policy definiert, die sie verwendet, um die Aktionen der Trajektorie auszuw√§hlen, die einen Einfluss auf die besuchten Zust√§nde hat).\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/probability.png\" alt=\"Probability\"/>\n",
    "\n",
    "- \\\\(J(\\theta)\\\\) : Die erwartete Rendite berechnen wir, indem wir f√ºr alle Trajektorien die Wahrscheinlichkeit, diese Trajektorie zu nehmen, wenn \\\\(\\theta \\\\) mit der Rendite dieser Trajektorie multipliziert wird.\n",
    "\n",
    "Unser Ziel ist es dann, die erwartete kumulative Belohnung zu maximieren, indem wir das \\\\(\\theta \\\\) finden, das die besten Aktionswahrscheinlichkeitsverteilungen ergibt:\n",
    "\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/max_objective.png\" alt=\"Max objective\"/>\n",
    "\n",
    "## Gradientenanstieg und das Policy-Gradienten-Theorem\n",
    "\n",
    "Policy-Gradient ist ein Optimierungsproblem: Wir wollen die Werte von \\\\(\\theta\\) finden, die unsere Zielfunktion \\\\(J(\\theta)\\\\) maximieren, also m√ºssen wir **Gradienten-Anstieg** verwenden. Dies ist die Umkehrung von *Gradient-Descent*, da sie die Richtung des steilsten Anstiegs von \\\\(J(\\theta)\\\\) angibt.\n",
    "\n",
    "(Wenn Sie eine Auffrischung des Unterschieds zwischen Gradientenabstieg und Gradientenaufstieg ben√∂tigen, lesen Sie [dies](https://www.baeldung.com/cs/gradient-descent-vs-ascent) und [dies](https://stats.stackexchange.com/questions/258721/gradient-ascent-vs-gradient-descent-in-logistic-regression)).\n",
    "\n",
    "Unser Aktualisierungsschritt f√ºr den Gradienten-Abstieg ist:\n",
    "\n",
    "\\\\( \\theta \\leftarrow \\theta + \\alpha * \\nabla_\\theta J(\\theta) \\\\\\)\n",
    "\n",
    "Wir k√∂nnen diese Aktualisierung wiederholt anwenden, in der Hoffnung, dass \\\\\\(\\theta \\\\) zu dem Wert konvergiert, der \\\\\\(J(\\theta)\\\\\\) maximiert.\n",
    "\n",
    "Es gibt jedoch zwei Probleme bei der Berechnung der Ableitung von \\\\(J(\\theta)\\\\):\n",
    "1. Wir k√∂nnen die wahre Steigung der Zielfunktion nicht berechnen, da dies die Berechnung der Wahrscheinlichkeit jeder m√∂glichen Flugbahn erfordert, was rechnerisch sehr aufwendig ist.\n",
    "Wir wollen also eine **Gradientensch√§tzung mit einer stichprobenbasierten Sch√§tzung berechnen (einige Trajektorien sammeln)**.\n",
    "\n",
    "2. Wir haben ein weiteres Problem, das ich im n√§chsten optionalen Abschnitt erkl√§re. Um diese Zielfunktion zu differenzieren, m√ºssen wir die Zustandsverteilung, die sogenannte Markov-Entscheidungsprozess-Dynamik, differenzieren. Diese ist mit der Umwelt verbunden. Sie gibt uns die Wahrscheinlichkeit, dass die Umgebung in den n√§chsten Zustand √ºbergeht, wenn der aktuelle Zustand und die vom Agenten durchgef√ºhrte Aktion gegeben sind. Das Problem ist, dass wir sie nicht differenzieren k√∂nnen, weil wir sie m√∂glicherweise nicht kennen.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/probability.png\" alt=\"Wahrscheinlichkeit\"/>\n",
    "\n",
    "Gl√ºcklicherweise werden wir eine L√∂sung namens Policy Gradient Theorem verwenden, die uns helfen wird, die Zielfunktion in eine differenzierbare Funktion umzuformulieren, die keine Differenzierung der Zustandsverteilung erfordert.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/policy_gradient_theorem.png\" alt=\"Policy Gradient\"/>\n",
    "\n",
    "Wenn Sie verstehen wollen, wie wir diese Formel f√ºr die Ann√§herung an den Gradienten herleiten, lesen Sie den n√§chsten (optionalen) Abschnitt.\n",
    "\n",
    "## Der Reinforce-Algorithmus (Monte Carlo Reinforce)\n",
    "\n",
    "Der Reinforce-Algorithmus, auch Monte-Carlo-Policy-Gradient genannt, ist ein Policy-Gradient-Algorithmus, der **eine gesch√§tzte Rendite aus einer ganzen Episode zur Aktualisierung des Policy-Parameters** verwendet:\n",
    "\n",
    "In einer Schleife:\n",
    "- Verwenden Sie die Richtlinie \\\\(\\pi_\\theta\\\\), um eine Episode zu sammeln \\\\(\\tau\\\\)\n",
    "- Verwenden Sie die Episode, um den Gradienten zu sch√§tzen \\\\(\\hat{g} = \\nabla_\\theta J(\\theta)\\\\\\)\n",
    "\n",
    " <figure class=\"image table text-center m-0 w-full\">\n",
    "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/policy_gradient_one.png\" alt=\"Policy Gradient\"/>\n",
    "</figure>\n",
    "\n",
    "- Aktualisieren Sie die Gewichte der Richtlinie: \\\\(\\theta \\leftarrow \\theta + \\alpha \\hat{g}\\\\)\n",
    "\n",
    "Wir k√∂nnen diese Aktualisierung wie folgt interpretieren:\n",
    "- \\\\(\\nabla_\\theta log \\pi_\\theta(a_t|s_t)\\\\\\) ist die Richtung des **st√§rksten Anstiegs der (log)-Wahrscheinlichkeit** der Auswahl der Aktion at aus dem Zustand st.\n",
    "Dies sagt uns, **wie wir die Gewichte der Policy √§ndern sollten**, wenn wir die logarithmische Wahrscheinlichkeit der Auswahl einer Aktion \\\\(a_t\\\\) im Zustand \\\\(s_t\\\\) erh√∂hen/verringern wollen.\n",
    "- \\\\(R(\\tau)\\\\): ist die Bewertungsfunktion:\n",
    "  - Wenn die Rendite hoch ist, **erh√∂ht sie die Wahrscheinlichkeiten** der Kombinationen (Zustand, Aktion).\n",
    "  - Wenn die Rendite niedrig ist, **verringert sie die Wahrscheinlichkeiten** der (Zustand, Handlung) Kombinationen.\n",
    "\n",
    "\n",
    "Wir k√∂nnen auch **mehrere Episoden (Trajektorien)** sammeln, um den Gradienten zu sch√§tzen:\n",
    "<figure class=\"image table text-center m-0 w-full\">\n",
    " <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/policy_gradient_multiple.png\" alt=\"Policy Gradient\"/>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glossar \n",
    "\n",
    "- **Deep Q-Learning:** Ein wertbasierter Deep Reinforcement Learning-Algorithmus, der ein tiefes neuronales Netz verwendet, um Q-Werte f√ºr Aktionen in einem bestimmten Zustand zu approximieren. Das Ziel von Deep Q-Learning ist es, die optimale Strategie zu finden, die die erwartete kumulative Belohnung durch Lernen der Aktionswerte maximiert.\n",
    "\n",
    "- **Wertbasierte Methoden:** Reinforcement-Learning-Methoden, die eine Wertfunktion als Zwischenschritt zur Ermittlung einer optimalen Strategie sch√§tzen.\n",
    "\n",
    "- **Policybasierte Methoden:** Reinforcement Learning Methoden, die direkt lernen, die optimale Policy zu approximieren, ohne eine Wertfunktion zu lernen. In der Praxis geben sie eine Wahrscheinlichkeitsverteilung √ºber Aktionen aus. \n",
    "\n",
    "    Die Vorteile der Verwendung von Policy-Gradienten-Methoden gegen√ºber wertbasierten Methoden sind unter anderem: \n",
    "    - Einfachheit der Integration: keine Notwendigkeit, Aktionswerte zu speichern;\n",
    "    - die F√§higkeit, eine stochastische Strategie zu erlernen: der Agent erkundet den Zustandsraum, ohne immer dieselbe Flugbahn einzuschlagen, und vermeidet das Problem des Wahrnehmungs-Alias;\n",
    "    - Effektivit√§t in hochdimensionalen und kontinuierlichen Handlungsr√§umen; und\n",
    "    - verbesserte Konvergenzeigenschaften.\n",
    "\n",
    "- **Policy Gradient:** Eine Teilmenge der Policy-basierten Methoden, bei denen das Ziel darin besteht, die Leistung einer parametrisierten Policy mithilfe des Gradientenanstiegs zu maximieren. Das Ziel eines Policy-Gradienten ist es, die Wahrscheinlichkeitsverteilung von Aktionen zu kontrollieren, indem die Policy so eingestellt wird, dass gute Aktionen (die den Ertrag maximieren) in Zukunft h√§ufiger abgerufen werden. \n",
    "\n",
    "- **Monte Carlo Reinforce:** Ein Policy-Gradient-Algorithmus, der eine gesch√§tzte Rendite aus einer ganzen Episode verwendet, um den Policy-Parameter zu aktualisieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mjY-eq3eWh9O"
   },
   "source": [
    "## Voraussetzungen üèóÔ∏è\n",
    "Bevor Sie sich mit dem Notebook besch√§ftigen, m√ºssen Sie:\n",
    "\n",
    "üî≤ üìö [Studieren Sie Policy Gradients durch Lesen von Unit 4](https://huggingface.co/deep-rl-course/unit4/introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bsh4ZAamchSl"
   },
   "source": [
    "# Wir programmieren den Reinforce-Algorithmus von Grund auf neu üî•.\n",
    "\n",
    "\n",
    "Um diese praktische √úbung f√ºr den Zertifizierungsprozess zu validieren, m√ºssen Sie Ihre trainierten Modelle an den Hub √ºbertragen.\n",
    "\n",
    "- Erhalten Sie ein Ergebnis von >= 350 f√ºr \"Cartpole-v1\".\n",
    "- Erhalte ein Ergebnis von >= 5 f√ºr \"PixelCopter\".\n",
    "\n",
    "Um dein Ergebnis zu finden, gehe zum Leaderboard und suche dein Modell, **das Ergebnis = mean_reward - std of reward**. **Wenn du dein Modell nicht auf der Bestenliste siehst, gehe unten auf der Bestenlistenseite und klicke auf die Schaltfl√§che \"Aktualisieren \"**.\n",
    "\n",
    "Weitere Informationen √ºber den Zertifizierungsprozess finden Sie in diesem Abschnitt üëâ https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JoTC9o2SczNn"
   },
   "source": [
    "## Ein Ratschlag üí°\n",
    "Es ist besser, dieses Colab in einer Kopie auf Ihrem Google Drive auszuf√ºhren, so dass Sie **bei Zeit√ºberschreitungen** immer noch das gespeicherte Notizbuch auf Ihrem Google Drive haben und nicht alles von Grund auf neu ausf√ºllen m√ºssen.\n",
    "\n",
    "Dazu kannst du entweder \"Strg + S\" oder \"Datei > Kopie in Google Drive speichern\" verwenden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bTpYcVZVMzUI"
   },
   "source": [
    "## Erstellen einer virtuellen Anzeige üñ•\n",
    "\n",
    "W√§hrend der Arbeit mit dem Notebook m√ºssen wir ein Wiederholungsvideo erstellen. Dazu ben√∂tigen wir mit colab **einen virtuellen Bildschirm, um die Umgebung zu rendern** (und somit die Bilder aufzunehmen).\n",
    "\n",
    "Daher wird die folgende Zelle die Librairies installieren und einen virtuellen Bildschirm erstellen und starten üñ•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jV6wjQ7Be7p5"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!apt install python-opengl\n",
    "!apt install ffmpeg\n",
    "!apt install xvfb\n",
    "!pip install pyvirtualdisplay\n",
    "!pip install pyglet==1.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sr-Nuyb1dBm0"
   },
   "outputs": [],
   "source": [
    "# Virtual display\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "virtual_display = Display(visible=0, size=(1400, 900))\n",
    "virtual_display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tjrLfPFIW8XK"
   },
   "source": [
    "## Installieren Sie die Abh√§ngigkeiten üîΩ.\n",
    "Der erste Schritt besteht darin, die Abh√§ngigkeiten zu installieren. Wir werden mehrere davon installieren:\n",
    "\n",
    "- `gym`\n",
    "- `gym-games`: Zus√§tzliche Gym-Umgebungen, die mit PyGame erstellt wurden.\n",
    "- `huggingface_hub`: ü§ó dient als zentraler Ort, an dem jeder Modelle und Datens√§tze teilen und erforschen kann. Es bietet Versionierung, Metriken, Visualisierungen und andere Funktionen, die eine einfache Zusammenarbeit mit anderen erm√∂glichen.\n",
    "\n",
    "Sie fragen sich vielleicht, warum wir gym und nicht gymnasium, eine neuere Version von gym, installieren? **Weil die Gym-Spiele, die wir verwenden, noch nicht mit Gym aktualisiert sind**.\n",
    "\n",
    "Die Unterschiede, die Sie hier finden werden:\n",
    "- In `gym` haben wir kein `terminated` und `truncated` sondern nur `done`.\n",
    "- In `gym` liefert die Verwendung von `env.step()` die Ergebnisse `state, reward, done, info`.\n",
    "\n",
    "Mehr √ºber die Unterschiede zwischen Gym und Gymnasium kannst du hier erfahren üëâ https://gymnasium.farama.org/content/migration-guide/\n",
    "\n",
    "\n",
    "Hier k√∂nnen Sie alle verf√ºgbaren Reinforce-Modelle sehen üëâ https://huggingface.co/models?other=reinforce\n",
    "\n",
    "Und hier finden Sie alle Deep Reinforcement Learning-Modelle üëâ https://huggingface.co/models?pipeline_tag=reinforcement-learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e8ZVi-uydpgL"
   },
   "outputs": [],
   "source": [
    "!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AAHAq6RZW3rn"
   },
   "source": [
    "## Importieren der Pakete üì¶\n",
    "Zus√§tzlich zum Import der installierten Bibliotheken, importieren wir auch:\n",
    "\n",
    "- `imageio`: Eine Bibliothek, die uns helfen wird, ein Wiedergabevideo zu erzeugen\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V8oadoJSWp7C"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# Gym\n",
    "import gym\n",
    "import gym_pygame\n",
    "\n",
    "# Hugging Face Hub\n",
    "from huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RfxJYdMeeVgv"
   },
   "source": [
    "## Pr√ºfen, ob wir eine GPU haben\n",
    "\n",
    "- Pr√ºfen wir, ob wir eine GPU haben\n",
    "- Wenn dies der Fall ist, sollten Sie `Ger√§t:cuda0` sehen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kaJu5FeZxXGY"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U5TNYa14aRav"
   },
   "outputs": [],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBPecCtBL_pZ"
   },
   "source": [
    "Wir sind nun bereit, unseren Reinforce-Algorithmus zu implementieren üî•."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8KEyKYo2ZSC-"
   },
   "source": [
    "# Erster Agent: Playing CartPole-v1 ü§ñ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "haLArKURMyuF"
   },
   "source": [
    "## Erstellen Sie die CartPole-Umgebung und verstehen Sie, wie sie funktioniert.\n",
    "### [Die Umgebung üéÆ](https://www.gymlibrary.dev/environments/classic_control/cart_pole/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AH_TaLKFXo_8"
   },
   "source": [
    "### Warum verwenden wir eine einfache Umgebung wie CartPole-v1?\n",
    "Wie in [Tipps und Tricks zum Verst√§rkungslernen] (https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html) erl√§utert, m√ºssen Sie bei der Neuimplementierung Ihres Agenten **sich vergewissern, dass er korrekt funktioniert, und mit einfachen Umgebungen Fehler finden, bevor Sie tiefer gehen**. Denn die Fehlersuche ist in einfachen Umgebungen viel einfacher.\n",
    "\n",
    "\n",
    "> Versuchen Sie, bei Spielzeugproblemen ein \"Lebenszeichen\" zu geben.\n",
    "\n",
    "\n",
    "> Validieren Sie die Implementierung, indem Sie sie auf immer schwierigeren Umgebungen laufen lassen (Sie k√∂nnen die Ergebnisse mit dem RL-Zoo vergleichen). F√ºr diesen Schritt m√ºssen Sie normalerweise eine Hyperparameter-Optimierung durchf√ºhren.\n",
    "___\n",
    "### Die CartPole-v1-Umgebung\n",
    "\n",
    "> Eine Stange ist √ºber ein unbet√§tigtes Gelenk an einem Wagen befestigt, der sich auf einer reibungsfreien Bahn bewegt. Das Pendel wird aufrecht auf dem Wagen platziert und das Ziel ist es, die Stange auszubalancieren, indem Kr√§fte in die linke und rechte Richtung auf den Wagen ausge√ºbt werden.\n",
    "\n",
    "\n",
    "\n",
    "Wir beginnen also mit CartPole-v1. Das Ziel ist es, den Wagen nach links oder rechts zu schieben, **so dass der Pol im Gleichgewicht bleibt.**\n",
    "\n",
    "Die Episode endet, wenn:\n",
    "- der Winkel der Stange gr√∂√üer als ¬±12¬∞ ist\n",
    "- Die Position des Wagens ist gr√∂√üer als ¬±2,4\n",
    "- Die Episodenl√§nge ist gr√∂√üer als 500\n",
    "\n",
    "Wir erhalten eine Belohnung üí∞ von +1 f√ºr jeden Zeitschritt, den der Pol im Gleichgewicht bleibt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "POOOk15_K6KA"
   },
   "outputs": [],
   "source": [
    "env_id = \"CartPole-v1\"\n",
    "# Create the env\n",
    "env = gym.make(env_id)\n",
    "\n",
    "# Create the evaluation env\n",
    "eval_env = gym.make(env_id)\n",
    "\n",
    "# Get the state space and action space\n",
    "s_size = env.observation_space.shape[0]\n",
    "a_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FMLFrjiBNLYJ"
   },
   "outputs": [],
   "source": [
    "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
    "print(\"The State Space is: \", s_size)\n",
    "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lu6t4sRNNWkN"
   },
   "outputs": [],
   "source": [
    "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
    "print(\"The Action Space is: \", a_size)\n",
    "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7SJMJj3WaFOz"
   },
   "source": [
    "## Bauen wir die Reinforce-Architektur auf\n",
    "Diese Implementierung basiert auf zwei Implementierungen:\n",
    "- [PyTorch official Reinforcement Learning example](https://github.com/pytorch/examples/blob/main/reinforcement_learning/reinforce.py)\n",
    "- [Udacity Reinforce](https://github.com/udacity/deep-reinforcement-learning/blob/master/reinforce/REINFORCE.ipynb)\n",
    "- [Verbesserung der Integration durch Chris1nexus](https://github.com/huggingface/deep-rl-class/pull/95)\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/reinforce.png\" alt=\"Reinforce\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "49kogtxBODX8"
   },
   "source": [
    "Wir wollen also:\n",
    "- Zwei vollst√§ndig verbundene Schichten (fc1 und fc2).\n",
    "- Verwendung von ReLU als Aktivierungsfunktion von fc1\n",
    "- Verwendung von Softmax zur Ausgabe einer Wahrscheinlichkeitsverteilung √ºber Aktionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w2LHcHhVZvPZ"
   },
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, s_size, a_size, h_size):\n",
    "        super(Policy, self).__init__()\n",
    "        # Create two fully connected layers\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Define the forward pass\n",
    "        # state goes to fc1 then we apply ReLU activation function\n",
    "\n",
    "        # fc1 outputs goes to fc2\n",
    "\n",
    "        # We output the softmax\n",
    "    \n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "        Given a state, take action\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        probs = self.forward(state).cpu()\n",
    "        m = Categorical(probs)\n",
    "        action = np.argmax(m)\n",
    "        return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rOMrdwSYOWSC"
   },
   "source": [
    "### L√∂sung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jGdhRSVrOV4K"
   },
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, s_size, a_size, h_size):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(s_size, h_size)\n",
    "        self.fc2 = nn.Linear(h_size, a_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "    \n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        probs = self.forward(state).cpu()\n",
    "        m = Categorical(probs)\n",
    "        action = np.argmax(m)\n",
    "        return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZTGWL4g2eM5B"
   },
   "source": [
    "Ich habe einen Fehler gemacht, k√∂nnen Sie erraten, wo?\n",
    "\n",
    "- Um das herauszufinden, machen wir einen Vorw√§rtspass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lwnqGBCNePor"
   },
   "outputs": [],
   "source": [
    "debug_policy = Policy(s_size, a_size, 64).to(device)\n",
    "debug_policy.act(env.reset())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14UYkoxCPaor"
   },
   "source": [
    "- Hier sehen wir, dass die Fehlermeldung `ValueError: Das Wertargument von log_prob muss ein Tensor sein\".\n",
    "\n",
    "- Das bedeutet, dass `action` in `m.log_prob(action)` ein Tensor sein muss **aber das ist er nicht.**\n",
    "\n",
    "- Wissen Sie, warum? Pr√ºfen Sie die Funktion act und versuchen Sie herauszufinden, warum sie nicht funktioniert.\n",
    "\n",
    "Hinweis üí°: Irgendetwas ist in dieser Implementierung falsch. Erinnern Sie sich daran, dass wir mit der Funktion act **eine Aktion aus der Wahrscheinlichkeitsverteilung √ºber Aktionen** ziehen wollen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gfGJNZBUP7Vn"
   },
   "source": [
    "### (Real) L√∂sung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ho_UHf49N9i4"
   },
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, s_size, a_size, h_size):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(s_size, h_size)\n",
    "        self.fc2 = nn.Linear(h_size, a_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "    \n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        probs = self.forward(state).cpu()\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgJWQFU_eUYw"
   },
   "source": [
    "Durch die Verwendung von CartPole war es einfacher zu debuggen, da **wir wissen, dass der Fehler von unserer Integration kommt und nicht von unserer einfachen Umgebung**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-20i7Pk0l1T"
   },
   "source": [
    "- Da **wir eine Aktion aus der Wahrscheinlichkeitsverteilung √ºber Aktionen** ausw√§hlen wollen, k√∂nnen wir nicht `Aktion = np.argmax(m)` verwenden, da dies immer die Aktion mit der h√∂chsten Wahrscheinlichkeit ausgibt.\n",
    "\n",
    "- Wir m√ºssen es durch `action = m.sample()` ersetzen, das eine Aktion aus der Wahrscheinlichkeitsverteilung P(.|s) ausw√§hlt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4MXoqetzfIoW"
   },
   "source": [
    "### Bauen wir den Reinforce-Trainingsalgorithmus auf\n",
    "Dies ist der Pseudocode des Reinforce-Algorithmus:\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/pg_pseudocode.png\" alt=\"Policy gradient pseudocode\"/>\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QmcXG-9i2Qu2"
   },
   "source": [
    "- Bei der Berechnung der Belohnung Gt (Zeile 6) sehen wir, dass wir die Summe der Rabattierten Belohnungn **ausgehend vom Zeitschritt t** berechnen.\n",
    "\n",
    "- Warum? Weil unsere Policy nur **Handlungen auf der Grundlage der Konsequenzen verst√§rken sollte**: Belohnungen, die vor einer Handlung erzielt wurden, sind also nutzlos (da sie nicht auf die Handlung zur√ºckzuf√ºhren sind), **nur die Belohnungen, die nach der Handlung kommen, z√§hlen**.\n",
    "\n",
    "- Bevor Sie dies kodieren, sollten Sie diesen Abschnitt lesen [lassen Sie sich nicht von der Vergangenheit ablenken] (https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#don-t-let-the-past-distract-you), in dem erkl√§rt wird, warum wir den Reward-to-Go-Gradienten verwenden.\n",
    "\n",
    "Wir verwenden eine interessante Technik, die von [Chris1nexus](https://github.com/Chris1nexus) kodiert wurde, um **die Belohnung in jedem Zeitschritt effizient zu berechnen**. In den Kommentaren wird das Verfahren erl√§utert. Z√∂gern Sie nicht, auch [die PR-Erkl√§rung zu lesen](https://github.com/huggingface/deep-rl-class/pull/95)\n",
    "Aber im Gro√üen und Ganzen geht es darum, **die Belohnung in jedem Zeitschritt effizient zu berechnen**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O554nUGPpcoq"
   },
   "source": [
    "Die zweite Frage, die Sie sich stellen k√∂nnen, ist **Warum minimieren wir den Verlust**? Sie sprachen von Gradientenaufstieg und nicht von Gradientenabstieg?\n",
    "\n",
    "- Wir wollen unsere Nutzenfunktion $J(\\theta)$ maximieren, aber in PyTorch wie in Tensorflow ist es besser, eine Zielfunktion zu **minimieren**.\n",
    "    - Nehmen wir also an, wir wollen Aktion 3 in einem bestimmten Zeitschritt verst√§rken. Vor dem Training dieser Aktion ist P 0.25.\n",
    "    - Wir wollen also $\\theta$ so ver√§ndern, dass $\\pi_\\theta(a_3|s; \\theta) > 0.25$\n",
    "    - Da sich alle P zu 1 summieren m√ºssen, wird max $\\pi_\\theta(a_3|s; \\theta)$ die **Minimierung der anderen Aktionswahrscheinlichkeit$ bewirken.\n",
    "    - Wir sollten PyTorch also anweisen, $1 - \\pi_\\theta(a_3|s; \\theta)$ zu minimieren.\n",
    "    - Diese Verlustfunktion n√§hert sich 0, wenn $\\pi_\\theta(a_3|s; \\theta)$ sich 1 n√§hert.\n",
    "    - Wir ermutigen also den Gradienten zu maximal $\\pi_\\theta(a_3|s; \\theta)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iOdv8Q9NfLK7"
   },
   "outputs": [],
   "source": [
    "def reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):\n",
    "    # Help us to calculate the score during the training\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    # Line 3 of pseudocode\n",
    "    for i_episode in range(1, n_training_episodes+1):\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state = # TODO: reset the environment\n",
    "        # Line 4 of pseudocode\n",
    "        for t in range(max_t):\n",
    "            action, log_prob = # TODO get the action\n",
    "            saved_log_probs.append(log_prob)\n",
    "            state, reward, done, _ = # TODO: take an env step\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break \n",
    "        scores_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "        \n",
    "        # Line 6 of pseudocode: calculate the return\n",
    "        returns = deque(maxlen=max_t) \n",
    "        n_steps = len(rewards) \n",
    "        # Compute the discounted returns at each timestep,\n",
    "        # as the sum of the gamma-discounted return at time t (G_t) + the reward at time t\n",
    "        \n",
    "        # In O(N) time, where N is the number of time steps\n",
    "        # (this definition of the discounted return G_t follows the definition of this quantity \n",
    "        # shown at page 44 of Sutton&Barto 2017 2nd draft)\n",
    "        # G_t = r_(t+1) + r_(t+2) + ...\n",
    "        \n",
    "        # Given this formulation, the returns at each timestep t can be computed \n",
    "        # by re-using the computed future returns G_(t+1) to compute the current return G_t\n",
    "        # G_t = r_(t+1) + gamma*G_(t+1)\n",
    "        # G_(t-1) = r_t + gamma* G_t\n",
    "        # (this follows a dynamic programming approach, with which we memorize solutions in order \n",
    "        # to avoid computing them multiple times)\n",
    "        \n",
    "        # This is correct since the above is equivalent to (see also page 46 of Sutton&Barto 2017 2nd draft)\n",
    "        # G_(t-1) = r_t + gamma*r_(t+1) + gamma*gamma*r_(t+2) + ...\n",
    "        \n",
    "        \n",
    "        ## Given the above, we calculate the returns at timestep t as: \n",
    "        #               gamma[t] * return[t] + reward[t]\n",
    "        #\n",
    "        ## We compute this starting from the last timestep to the first, in order\n",
    "        ## to employ the formula presented above and avoid redundant computations that would be needed \n",
    "        ## if we were to do it from first to last.\n",
    "        \n",
    "        ## Hence, the queue \"returns\" will hold the returns in chronological order, from t=0 to t=n_steps\n",
    "        ## thanks to the appendleft() function which allows to append to the position 0 in constant time O(1)\n",
    "        ## a normal python list would instead require O(N) to do this.\n",
    "        for t in range(n_steps)[::-1]:\n",
    "            disc_return_t = (returns[0] if len(returns)>0 else 0)\n",
    "            returns.appendleft(    ) # TODO: complete here        \n",
    "       \n",
    "        ## standardization of the returns is employed to make training more stable\n",
    "        eps = np.finfo(np.float32).eps.item()\n",
    "        \n",
    "        ## eps is the smallest representable float, which is \n",
    "        # added to the standard deviation of the returns to avoid numerical instabilities\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "        \n",
    "        # Line 7:\n",
    "        policy_loss = []\n",
    "        for log_prob, disc_return in zip(saved_log_probs, returns):\n",
    "            policy_loss.append(-log_prob * disc_return)\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "        \n",
    "        # Line 8: PyTorch prefers gradient descent \n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i_episode % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YB0Cxrw1StrP"
   },
   "source": [
    "#### L√∂sung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NCNvyElRStWG"
   },
   "outputs": [],
   "source": [
    "def reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):\n",
    "    # Help us to calculate the score during the training\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    # Line 3 of pseudocode\n",
    "    for i_episode in range(1, n_training_episodes+1):\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state = env.reset()\n",
    "        # Line 4 of pseudocode\n",
    "        for t in range(max_t):\n",
    "            action, log_prob = policy.act(state)\n",
    "            saved_log_probs.append(log_prob)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break \n",
    "        scores_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "        \n",
    "        # Line 6 of pseudocode: calculate the return\n",
    "        returns = deque(maxlen=max_t) \n",
    "        n_steps = len(rewards) \n",
    "        # Compute the discounted returns at each timestep,\n",
    "        # as \n",
    "        #      the sum of the gamma-discounted return at time t (G_t) + the reward at time t\n",
    "        #\n",
    "        # In O(N) time, where N is the number of time steps\n",
    "        # (this definition of the discounted return G_t follows the definition of this quantity \n",
    "        # shown at page 44 of Sutton&Barto 2017 2nd draft)\n",
    "        # G_t = r_(t+1) + r_(t+2) + ...\n",
    "        \n",
    "        # Given this formulation, the returns at each timestep t can be computed \n",
    "        # by re-using the computed future returns G_(t+1) to compute the current return G_t\n",
    "        # G_t = r_(t+1) + gamma*G_(t+1)\n",
    "        # G_(t-1) = r_t + gamma* G_t\n",
    "        # (this follows a dynamic programming approach, with which we memorize solutions in order \n",
    "        # to avoid computing them multiple times)\n",
    "        \n",
    "        # This is correct since the above is equivalent to (see also page 46 of Sutton&Barto 2017 2nd draft)\n",
    "        # G_(t-1) = r_t + gamma*r_(t+1) + gamma*gamma*r_(t+2) + ...\n",
    "        \n",
    "        \n",
    "        ## Given the above, we calculate the returns at timestep t as: \n",
    "        #               gamma[t] * return[t] + reward[t]\n",
    "        #\n",
    "        ## We compute this starting from the last timestep to the first, in order\n",
    "        ## to employ the formula presented above and avoid redundant computations that would be needed \n",
    "        ## if we were to do it from first to last.\n",
    "        \n",
    "        ## Hence, the queue \"returns\" will hold the returns in chronological order, from t=0 to t=n_steps\n",
    "        ## thanks to the appendleft() function which allows to append to the position 0 in constant time O(1)\n",
    "        ## a normal python list would instead require O(N) to do this.\n",
    "        for t in range(n_steps)[::-1]:\n",
    "            disc_return_t = (returns[0] if len(returns)>0 else 0)\n",
    "            returns.appendleft( gamma*disc_return_t + rewards[t]   )    \n",
    "            \n",
    "        ## standardization of the returns is employed to make training more stable\n",
    "        eps = np.finfo(np.float32).eps.item()\n",
    "        ## eps is the smallest representable float, which is \n",
    "        # added to the standard deviation of the returns to avoid numerical instabilities        \n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "        \n",
    "        # Line 7:\n",
    "        policy_loss = []\n",
    "        for log_prob, disc_return in zip(saved_log_probs, returns):\n",
    "            policy_loss.append(-log_prob * disc_return)\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "        \n",
    "        # Line 8: PyTorch prefers gradient descent \n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i_episode % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RIWhQyJjfpEt"
   },
   "source": [
    "## Train it\n",
    "- Wir sind nun bereit, unseren Agenten zu trainieren.\n",
    "- Aber zuerst definieren wir eine Variable, die alle Trainingshyperparameter enth√§lt.\n",
    "- Sie k√∂nnen die Trainingsparameter √§ndern (und sollten üòâ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "utRe1NgtVBYF"
   },
   "outputs": [],
   "source": [
    "cartpole_hyperparameters = {\n",
    "    \"h_size\": 16,\n",
    "    \"n_training_episodes\": 1000,\n",
    "    \"n_evaluation_episodes\": 10,\n",
    "    \"max_t\": 1000,\n",
    "    \"gamma\": 1.0,\n",
    "    \"lr\": 1e-2,\n",
    "    \"env_id\": env_id,\n",
    "    \"state_space\": s_size,\n",
    "    \"action_space\": a_size,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D3lWyVXBVfl6"
   },
   "outputs": [],
   "source": [
    "# Create policy and place it to the device\n",
    "cartpole_policy = Policy(cartpole_hyperparameters[\"state_space\"], cartpole_hyperparameters[\"action_space\"], cartpole_hyperparameters[\"h_size\"]).to(device)\n",
    "cartpole_optimizer = optim.Adam(cartpole_policy.parameters(), lr=cartpole_hyperparameters[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uGf-hQCnfouB"
   },
   "outputs": [],
   "source": [
    "scores = reinforce(cartpole_policy,\n",
    "                   cartpole_optimizer,\n",
    "                   cartpole_hyperparameters[\"n_training_episodes\"], \n",
    "                   cartpole_hyperparameters[\"max_t\"],\n",
    "                   cartpole_hyperparameters[\"gamma\"], \n",
    "                   100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qajj2kXqhB3g"
   },
   "source": [
    "## Bewertungsmethode definieren üìù\n",
    "- Hier definieren wir die Auswertungsmethode, mit der wir unseren Reinforce-Agenten testen werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3FamHmxyhBEU"
   },
   "outputs": [],
   "source": [
    "def evaluate_agent(env, max_steps, n_eval_episodes, policy):\n",
    "  \"\"\"\n",
    "  Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n",
    "  :param env: The evaluation environment\n",
    "  :param n_eval_episodes: Number of episode to evaluate the agent\n",
    "  :param policy: The Reinforce agent\n",
    "  \"\"\"\n",
    "  episode_rewards = []\n",
    "  for episode in range(n_eval_episodes):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards_ep = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "      action, _ = policy.act(state)\n",
    "      new_state, reward, done, info = env.step(action)\n",
    "      total_rewards_ep += reward\n",
    "        \n",
    "      if done:\n",
    "        break\n",
    "      state = new_state\n",
    "    episode_rewards.append(total_rewards_ep)\n",
    "  mean_reward = np.mean(episode_rewards)\n",
    "  std_reward = np.std(episode_rewards)\n",
    "\n",
    "  return mean_reward, std_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xdH2QCrLTrlT"
   },
   "source": [
    "## Bewerten Sie unseren Agenten üìà"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ohGSXDyHh0xx"
   },
   "outputs": [],
   "source": [
    "evaluate_agent(eval_env, \n",
    "               cartpole_hyperparameters[\"max_t\"], \n",
    "               cartpole_hyperparameters[\"n_evaluation_episodes\"],\n",
    "               cartpole_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7CoeLkQ7TpO8"
   },
   "source": [
    "### Ver√∂ffentlichen Sie unser trainiertes Modell auf dem Hub üî•\n",
    "Da wir nun gesehen haben, dass wir nach dem Training gute Ergebnisse erzielt haben, k√∂nnen wir unser trainiertes Modell mit einer Zeile Code auf dem Hub ü§ó ver√∂ffentlichen.\n",
    "\n",
    "Hier ist ein Beispiel f√ºr eine Model Card:\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/modelcard.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jmhs1k-cftIq"
   },
   "source": [
    "### Push zum Hub\n",
    "#### Diesen Code nicht ver√§ndern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LIVsvlW_8tcw"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, snapshot_download\n",
    "from huggingface_hub.repocard import metadata_eval_result, metadata_save\n",
    "\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import json\n",
    "import imageio\n",
    "\n",
    "import tempfile\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lo4JH45if81z"
   },
   "outputs": [],
   "source": [
    "def record_video(env, policy, out_directory, fps=30):\n",
    "  \"\"\"\n",
    "  Generate a replay video of the agent\n",
    "  :param env\n",
    "  :param Qtable: Qtable of our agent\n",
    "  :param out_directory\n",
    "  :param fps: how many frame per seconds (with taxi-v3 and frozenlake-v1 we use 1)\n",
    "  \"\"\"\n",
    "  images = []  \n",
    "  done = False\n",
    "  state = env.reset()\n",
    "  img = env.render(mode='rgb_array')\n",
    "  images.append(img)\n",
    "  while not done:\n",
    "    # Take the action (index) that have the maximum expected future reward given that state\n",
    "    action, _ = policy.act(state)\n",
    "    state, reward, done, info = env.step(action) # We directly put next_state = state for recording logic\n",
    "    img = env.render(mode='rgb_array')\n",
    "    images.append(img)\n",
    "  imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_TPdq47D7_f_"
   },
   "outputs": [],
   "source": [
    "def push_to_hub(repo_id, \n",
    "                model,\n",
    "                hyperparameters,\n",
    "                eval_env,\n",
    "                video_fps=30\n",
    "                ):\n",
    "  \"\"\"\n",
    "  Evaluate, Generate a video and Upload a model to Hugging Face Hub.\n",
    "  This method does the complete pipeline:\n",
    "  - It evaluates the model\n",
    "  - It generates the model card\n",
    "  - It generates a replay video of the agent\n",
    "  - It pushes everything to the Hub\n",
    "\n",
    "  :param repo_id: repo_id: id of the model repository from the Hugging Face Hub\n",
    "  :param model: the pytorch model we want to save\n",
    "  :param hyperparameters: training hyperparameters\n",
    "  :param eval_env: evaluation environment\n",
    "  :param video_fps: how many frame per seconds to record our video replay \n",
    "  \"\"\"\n",
    "\n",
    "  _, repo_name = repo_id.split(\"/\")\n",
    "  api = HfApi()\n",
    "  \n",
    "  # Step 1: Create the repo\n",
    "  repo_url = api.create_repo(\n",
    "        repo_id=repo_id,\n",
    "        exist_ok=True,\n",
    "  )\n",
    "\n",
    "  with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    local_directory = Path(tmpdirname)\n",
    "  \n",
    "    # Step 2: Save the model\n",
    "    torch.save(model, local_directory / \"model.pt\")\n",
    "\n",
    "    # Step 3: Save the hyperparameters to JSON\n",
    "    with open(local_directory / \"hyperparameters.json\", \"w\") as outfile:\n",
    "      json.dump(hyperparameters, outfile)\n",
    "    \n",
    "    # Step 4: Evaluate the model and build JSON\n",
    "    mean_reward, std_reward = evaluate_agent(eval_env, \n",
    "                                            hyperparameters[\"max_t\"],\n",
    "                                            hyperparameters[\"n_evaluation_episodes\"], \n",
    "                                            model)\n",
    "    # Get datetime\n",
    "    eval_datetime = datetime.datetime.now()\n",
    "    eval_form_datetime = eval_datetime.isoformat()\n",
    "\n",
    "    evaluate_data = {\n",
    "          \"env_id\": hyperparameters[\"env_id\"], \n",
    "          \"mean_reward\": mean_reward,\n",
    "          \"n_evaluation_episodes\": hyperparameters[\"n_evaluation_episodes\"],\n",
    "          \"eval_datetime\": eval_form_datetime,\n",
    "    }\n",
    "\n",
    "    # Write a JSON file\n",
    "    with open(local_directory / \"results.json\", \"w\") as outfile:\n",
    "        json.dump(evaluate_data, outfile)\n",
    "\n",
    "    # Step 5: Create the model card\n",
    "    env_name = hyperparameters[\"env_id\"]\n",
    "    \n",
    "    metadata = {}\n",
    "    metadata[\"tags\"] = [\n",
    "          env_name,\n",
    "          \"reinforce\",\n",
    "          \"reinforcement-learning\",\n",
    "          \"custom-implementation\",\n",
    "          \"deep-rl-class\"\n",
    "      ]\n",
    "\n",
    "    # Add metrics\n",
    "    eval = metadata_eval_result(\n",
    "        model_pretty_name=repo_name,\n",
    "        task_pretty_name=\"reinforcement-learning\",\n",
    "        task_id=\"reinforcement-learning\",\n",
    "        metrics_pretty_name=\"mean_reward\",\n",
    "        metrics_id=\"mean_reward\",\n",
    "        metrics_value=f\"{mean_reward:.2f} +/- {std_reward:.2f}\",\n",
    "        dataset_pretty_name=env_name,\n",
    "        dataset_id=env_name,\n",
    "      )\n",
    "\n",
    "    # Merges both dictionaries\n",
    "    metadata = {**metadata, **eval}\n",
    "\n",
    "    model_card = f\"\"\"\n",
    "  # **Reinforce** Agent playing **{env_id}**\n",
    "  This is a trained model of a **Reinforce** agent playing **{env_id}** .\n",
    "  To learn to use this model and train yours check Unit 4 of the Deep Reinforcement Learning Course: https://huggingface.co/deep-rl-course/unit4/introduction\n",
    "  \"\"\"\n",
    "\n",
    "    readme_path = local_directory / \"README.md\"\n",
    "    readme = \"\"\n",
    "    if readme_path.exists():\n",
    "        with readme_path.open(\"r\", encoding=\"utf8\") as f:\n",
    "          readme = f.read()\n",
    "    else:\n",
    "      readme = model_card\n",
    "\n",
    "    with readme_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "      f.write(readme)\n",
    "\n",
    "    # Save our metrics to Readme metadata\n",
    "    metadata_save(readme_path, metadata)\n",
    "\n",
    "    # Step 6: Record a video\n",
    "    video_path =  local_directory / \"replay.mp4\"\n",
    "    record_video(env, model, video_path, video_fps)\n",
    "\n",
    "    # Step 7. Push everything to the Hub\n",
    "    api.upload_folder(\n",
    "          repo_id=repo_id,\n",
    "          folder_path=local_directory,\n",
    "          path_in_repo=\".\",\n",
    "    )\n",
    "\n",
    "    print(f\"Your model is pushed to the Hub. You can view your model here: {repo_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w17w8CxzoURM"
   },
   "source": [
    "### .\n",
    "\n",
    "Mit \"push_to_hub\" **werten Sie aus, zeichnen ein Replay auf, generieren eine Modellkarte Ihres Agenten und schieben sie an den Hub**.\n",
    "\n",
    "This way:\n",
    "- Sie k√∂nnen **unsere Arbeit vorf√ºhren** üî•.\n",
    "- Sie k√∂nnen **Ihren Agenten beim Spielen visualisieren** üëÄ\n",
    "- Du kannst **einen Agenten mit der Community teilen, den andere benutzen k√∂nnen** üíæ\n",
    "- Sie k√∂nnen **auf eine Bestenliste üèÜ zugreifen, um zu sehen, wie gut Ihr Agent im Vergleich zu Ihren Klassenkameraden abschneidet** üëâ https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cWnFC0iZooTw"
   },
   "source": [
    "Um Ihr Modell mit der Gemeinschaft teilen zu k√∂nnen, sind drei weitere Schritte erforderlich:\n",
    "\n",
    "1Ô∏è‚É£ (Falls noch nicht geschehen) Erstellen Sie ein Konto f√ºr HF ‚û° https://huggingface.co/join\n",
    "\n",
    "2Ô∏è‚É£ Melde dich an und speichere dann dein Authentifizierungs-Token von der Hugging Face Website.\n",
    "- Erstellen Sie ein neues Token (https://huggingface.co/settings/tokens) **mit Schreibrolle**\n",
    "\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/create-token.jpg\" alt=\"HF-Token erstellen\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QB5nIcxR8paT"
   },
   "outputs": [],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GyWc1x3-o3xG"
   },
   "source": [
    "Wenn Sie kein Google Colab oder ein Jupyter Notebook verwenden m√∂chten, m√ºssen Sie stattdessen diesen Befehl verwenden: `huggingface-cli login` (oder `login`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-D-zhbRoeOm"
   },
   "source": [
    "3Ô∏è‚É£ Wir sind jetzt bereit, unseren trainierten Agenten mit der Funktion `package_to_hub()` an den ü§ó Hub üî• zu √ºbertragen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UNwkTS65Uq3Q"
   },
   "outputs": [],
   "source": [
    "repo_id = \"\" #TODO Define your repo id {username/Reinforce-{model-id}}\n",
    "push_to_hub(repo_id,\n",
    "                cartpole_policy, # The model we want to save\n",
    "                cartpole_hyperparameters, # Hyperparameters\n",
    "                eval_env, # Evaluation environment\n",
    "                video_fps=30\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jrnuKH1gYZSz"
   },
   "source": [
    "Nachdem wir nun die Robustheit unserer Implementierung getestet haben, wollen wir eine komplexere Umgebung ausprobieren: PixelCopter üöÅ.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JNLVmKKVKA6j"
   },
   "source": [
    "## Zweiter Agent: PixelCopter üöÅ.\n",
    "\n",
    "### Studiere die PixelCopter-Umgebung üëÄ\n",
    "- [Die Umgebungsdokumentation](https://pygame-learning-environment.readthedocs.io/en/latest/user/games/pixelcopter.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JBSc8mlfyin3"
   },
   "outputs": [],
   "source": [
    "env_id = \"Pixelcopter-PLE-v0\"\n",
    "env = gym.make(env_id)\n",
    "eval_env = gym.make(env_id)\n",
    "s_size = env.observation_space.shape[0]\n",
    "a_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L5u_zAHsKBy7"
   },
   "outputs": [],
   "source": [
    "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
    "print(\"The State Space is: \", s_size)\n",
    "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D7yJM9YXKNbq"
   },
   "outputs": [],
   "source": [
    "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
    "print(\"The Action Space is: \", a_size)\n",
    "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NNWvlyvzalXr"
   },
   "source": [
    "Der Beobachtungsraum (7) üëÄ:\n",
    "- Spieler-Y-Position\n",
    "- Spieler-Geschwindigkeit\n",
    "- Abstand des Spielers zum Boden\n",
    "- Abstand des Spielers zur Decke\n",
    "- n√§chster Block x Abstand zum Spieler\n",
    "- n√§chste Bl√∂cke obere y-Position\n",
    "- n√§chste Bl√∂cke untere y-Position\n",
    "\n",
    "Der Aktionsraum(2) üéÆ:\n",
    "- Nach oben (Gaspedal dr√ºcken)\n",
    "- Nichts tun (nicht auf den Beschleuniger dr√ºcken)\n",
    "\n",
    "Die Belohnungsfunktion üí∞:\n",
    "- F√ºr jeden vertikalen Block, den er durchl√§uft, erh√§lt er eine positive Belohnung von +1. Jedes Mal, wenn ein Endzustand erreicht wird, erh√§lt er eine negative Belohnung von -1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aV1466QP8crz"
   },
   "source": [
    "### Definieren Sie die neue Richtlinie üß†\n",
    "- Wir brauchen ein tieferes neuronales Netz, da die Umgebung komplexer ist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I1eBkCiX2X_S"
   },
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, s_size, a_size, h_size):\n",
    "        super(Policy, self).__init__()\n",
    "        # Define the three layers here\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Define the forward process here\n",
    "        return F.softmax(x, dim=1)\n",
    "    \n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        probs = self.forward(state).cpu()\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47iuAFqV8Ws-"
   },
   "source": [
    "#### L√∂sung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wrNuVcHC8Xu7"
   },
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, s_size, a_size, h_size):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(s_size, h_size)\n",
    "        self.fc2 = nn.Linear(h_size, h_size*2)\n",
    "        self.fc3 = nn.Linear(h_size*2, a_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "    \n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        probs = self.forward(state).cpu()\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SM1QiGCSbBkM"
   },
   "source": [
    "### Definieren Sie die Hyperparameter ‚öôÔ∏è\n",
    "- Denn diese Umgebung ist komplexer.\n",
    "- Insbesondere f√ºr die versteckte Gr√∂√üe ben√∂tigen wir mehr Neuronen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y0uujOR_ypB6"
   },
   "outputs": [],
   "source": [
    "pixelcopter_hyperparameters = {\n",
    "    \"h_size\": 64,\n",
    "    \"n_training_episodes\": 50000,\n",
    "    \"n_evaluation_episodes\": 10,\n",
    "    \"max_t\": 10000,\n",
    "    \"gamma\": 0.99,\n",
    "    \"lr\": 1e-4,\n",
    "    \"env_id\": env_id,\n",
    "    \"state_space\": s_size,\n",
    "    \"action_space\": a_size,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wyvXTJWm9GJG"
   },
   "source": [
    "### Train it\n",
    "- Wir sind jetzt bereit, unseren Agenten zu trainieren üî•."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7mM2P_ckysFE"
   },
   "outputs": [],
   "source": [
    "# Create policy and place it to the device\n",
    "# torch.manual_seed(50)\n",
    "pixelcopter_policy = Policy(pixelcopter_hyperparameters[\"state_space\"], pixelcopter_hyperparameters[\"action_space\"], pixelcopter_hyperparameters[\"h_size\"]).to(device)\n",
    "pixelcopter_optimizer = optim.Adam(pixelcopter_policy.parameters(), lr=pixelcopter_hyperparameters[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v1HEqP-fy-Rf"
   },
   "outputs": [],
   "source": [
    "scores = reinforce(pixelcopter_policy,\n",
    "                   pixelcopter_optimizer,\n",
    "                   pixelcopter_hyperparameters[\"n_training_episodes\"], \n",
    "                   pixelcopter_hyperparameters[\"max_t\"],\n",
    "                   pixelcopter_hyperparameters[\"gamma\"], \n",
    "                   1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8kwFQ-Ip85BE"
   },
   "source": [
    "### Ver√∂ffentlichen Sie unser trainiertes Modell auf dem Hub üî•."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6PtB7LRbTKWK"
   },
   "outputs": [],
   "source": [
    "repo_id = \"\" #TODO Define your repo id {username/Reinforce-{model-id}}\n",
    "push_to_hub(repo_id,\n",
    "                pixelcopter_policy, # The model we want to save\n",
    "                pixelcopter_hyperparameters, # Hyperparameters\n",
    "                eval_env, # Evaluation environment\n",
    "                video_fps=30\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7VDcJ29FcOyb"
   },
   "source": [
    "## Einige zus√§tzliche Herausforderungen üèÜ\n",
    "Die beste Art zu lernen **ist, Dinge selbst auszuprobieren**! Wie Sie gesehen haben, ist der derzeitige Agent nicht besonders gut. Als ersten Vorschlag k√∂nnen Sie f√ºr mehr Schritte trainieren. Versuchen Sie aber auch, bessere Parameter zu finden.\n",
    "\n",
    "In der [Rangliste] (https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) finden Sie Ihre Agenten. K√∂nnen Sie sich an die Spitze setzen?\n",
    "\n",
    "Hier sind einige Ideen, um dies zu erreichen:\n",
    "* Trainiere mehr Schritte\n",
    "* Probiere verschiedene Hyperparameter aus, indem du dir ansiehst, was deine Klassenkameraden gemacht haben üëâ https://huggingface.co/models?other=reinforce\n",
    "* **Pushen Sie Ihr neu trainiertes Modell** auf dem Hub üî•\n",
    "* **Verbesserung der Implementierung f√ºr komplexere Umgebungen** (wie w√§re es z.B., das Netzwerk in ein Convolutional Neural Network zu √§ndern, um\n",
    "Frames als Beobachtung zu behandeln)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x62pP0PHdA-y"
   },
   "source": [
    "________________________________________________________________________\n",
    "\n",
    "**Gl√ºckwunsch zum Abschluss dieser Einheit**! Es gab eine Menge Informationen.\n",
    "Und herzlichen Gl√ºckwunsch zum Abschluss des Tutorials. Du hast gerade deinen ersten Deep Reinforcement Learning-Agenten von Grund auf mit PyTorch programmiert und ihn im Hub geteilt ü•≥.\n",
    "\n",
    "Z√∂gern Sie nicht, diese Einheit zu iterieren **durch Verbesserung der Implementierung f√ºr komplexere Umgebungen** (wie w√§re es zum Beispiel, das Netzwerk in ein Convolutional Neural Network zu √§ndern, um\n",
    "Frames als Beobachtung zu behandeln)?\n",
    "\n",
    "In der n√§chsten Einheit **werden wir mehr √ºber Unity MLAgents** lernen, indem wir Agenten in Unity-Umgebungen trainieren. Auf diese Weise werden Sie bereit sein, an den **AI vs. AI Herausforderungen teilzunehmen, bei denen Sie Ihre Agenten trainieren werden\n",
    "gegen andere Agenten in einer Schneeballschlacht und einem Fu√üballspiel antreten**.\n",
    "\n",
    "Klingt lustig? Bis zum n√§chsten Mal!\n",
    "\n",
    "Zum Schluss w√ºrden wir gerne **h√∂ren, was Sie √ºber den Kurs denken und wie wir ihn verbessern k√∂nnen**. Wenn Sie also ein Feedback haben, bitte üëâ [f√ºllen Sie dieses Formular aus] (https://forms.gle/BzKXWzLAGZESGNaE9)\n",
    "\n",
    "Wir sehen uns in Referat 5! üî•\n",
    "\n",
    "### Keep Learning, stay awesome ü§ó\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "BPLwsPajb1f8",
    "L_WSo0VUV99t",
    "mjY-eq3eWh9O",
    "JoTC9o2SczNn",
    "gfGJNZBUP7Vn",
    "YB0Cxrw1StrP",
    "47iuAFqV8Ws-",
    "x62pP0PHdA-y"
   ],
   "include_colab_link": true,
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
