{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/huggingface/deep-rl-class/blob/GymnasiumUpdate%2FUnit4/notebooks/unit4/unit4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"In Colab √∂ffnen\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CjRWziAVU2lZ"
   },
   "source": [
    "# Unit 4: Programmieren Sie Ihren ersten Deep Reinforcement Learning Algorithmus mit PyTorch: Reinforce. Und teste seine Robustheit üí™.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/thumbnail.png\" alt=\"thumbnail\"/>\n",
    "\n",
    "\n",
    "In diesem Notizbuch werden Sie Ihren ersten Deep Reinforcement Learning-Algorithmus von Grund auf programmieren: Reinforce (auch Monte Carlo Policy Gradient genannt).\n",
    "\n",
    "Reinforce ist eine *Policy-basierte Methode*: ein Deep Reinforcement Learning-Algorithmus, der versucht, **die Policy direkt zu optimieren, ohne eine Action-Value-Funktion** zu verwenden.\n",
    "\n",
    "Genauer gesagt ist Reinforce eine *Policy-Gradient-Methode*, eine Unterklasse von *Policy-basierten Methoden*, die darauf abzielt, **die Policy direkt zu optimieren, indem sie die Gewichte der optimalen Policy mithilfe des Gradientenanstiegs sch√§tzt**.\n",
    "\n",
    "Um ihre Robustheit zu testen, werden wir sie in 2 verschiedenen einfachen Umgebungen trainieren:\n",
    "- Cartpole-v1\n",
    "- Pixelcopter-Umgebung\n",
    "\n",
    "‚¨áÔ∏è Hier ist ein Beispiel f√ºr das, was **am Ende dieses Notizbuchs erreicht wird** ‚¨áÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4rBom2sbo7S"
   },
   "source": [
    "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/envs.gif\" alt=\"Umgebungen\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BPLwsPajb1f8"
   },
   "source": [
    "### üéÆ Umgebungen:\n",
    "\n",
    "- [CartPole-v1](https://www.gymlibrary.dev/environments/classic_control/cart_pole/)\n",
    "- [PixelCopter](https://pygame-learning-environment.readthedocs.io/en/latest/user/games/pixelcopter.html)\n",
    "\n",
    "### üìö RL-Library:\n",
    "\n",
    "- Python\n",
    "- PyTorch\n",
    "\n",
    "\n",
    "Wir versuchen st√§ndig, unsere Tutorials zu verbessern. **Wenn Sie also Fehler in diesem Notizbuch** finden, √∂ffnen Sie bitte [einen Fehler im GitHub Repo](https://github.com/huggingface/deep-rl-class/issues)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_WSo0VUV99t"
   },
   "source": [
    "## Ziele dieses Notizbuchs üèÜ\n",
    "Am Ende des Notizbuchs werden Sie:\n",
    "- In der Lage sein, **einen Reinforce-Algorithmus mit PyTorch von Grund auf zu programmieren.**\n",
    "- In der Lage sein, **die Robustheit deines Agenten unter Verwendung einfacher Umgebungen zu testen.**\n",
    "- In der Lage sein, **Ihren trainierten Agenten auf den Hub** zu pushen, mit einer sch√∂nen Videowiedergabe und einem Bewertungsergebnis üî•."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lEPrZg2eWa4R"
   },
   "source": [
    "## Dieses Notizbuch stammt aus dem Deep Reinforcement Learning Kurs.\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/deep-rl-course-illustration.jpg\" alt=\"Deep RL Course illustration\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6p5HnEefISCB"
   },
   "source": [
    "In diesem kostenlosen Kurs lernen Sie:\n",
    "\n",
    "- üìñ Deep Reinforcement Learning in **Theorie und Praxis** studieren.\n",
    "- üßë‚Äçüíª Lernen Sie, **ber√ºhmte Deep RL-Bibliotheken** wie Stable Baselines3, RL Baselines3 Zoo, CleanRL und Sample Factory 2.0 zu verwenden.\n",
    "- ü§ñ Trainieren Sie **Agenten in einzigartigen Umgebungen**.\n",
    "\n",
    "Und mehr, siehe üìö den Lehrplan üëâ https://simoninithomas.github.io/deep-rl-course\n",
    "\n",
    "Vergessen Sie nicht, sich **<a href=\"http://eepurl.com/ic5ZUD\">f√ºr den Kurs anzumelden</a>** (wir sammeln Ihre E-Mail, um Ihnen **die Links zu senden, wenn die einzelnen Einheiten ver√∂ffentlicht werden, und Sie √ºber die Herausforderungen und Aktualisierungen zu informieren).**\n",
    "\n",
    "\n",
    "Der beste Weg, um in Kontakt zu bleiben, ist, unserem Discord-Server beizutreten, um sich mit der Community und mit uns auszutauschen üëâüèª https://discord.gg/ydHrjt3WP5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mjY-eq3eWh9O"
   },
   "source": [
    "## Voraussetzungen üèóÔ∏è\n",
    "Bevor Sie sich mit dem Notebook besch√§ftigen, m√ºssen Sie:\n",
    "\n",
    "üî≤ üìö [Studieren Sie Policy Gradients durch Lesen von Unit 4](https://huggingface.co/deep-rl-course/unit4/introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bsh4ZAamchSl"
   },
   "source": [
    "# Wir programmieren den Reinforce-Algorithmus von Grund auf neu üî•.\n",
    "\n",
    "\n",
    "Um diese praktische √úbung f√ºr den Zertifizierungsprozess zu validieren, m√ºssen Sie Ihre trainierten Modelle an den Hub √ºbertragen.\n",
    "\n",
    "- Erhalten Sie ein Ergebnis von >= 350 f√ºr \"Cartpole-v1\".\n",
    "- Erhalte ein Ergebnis von >= 5 f√ºr \"PixelCopter\".\n",
    "\n",
    "Um dein Ergebnis zu finden, gehe zum Leaderboard und suche dein Modell, **das Ergebnis = mean_reward - std of reward**. **Wenn du dein Modell nicht auf der Bestenliste siehst, gehe unten auf der Bestenlistenseite und klicke auf die Schaltfl√§che \"Aktualisieren \"**.\n",
    "\n",
    "Weitere Informationen √ºber den Zertifizierungsprozess finden Sie in diesem Abschnitt üëâ https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JoTC9o2SczNn"
   },
   "source": [
    "## Ein Ratschlag üí°\n",
    "Es ist besser, dieses Colab in einer Kopie auf Ihrem Google Drive auszuf√ºhren, so dass Sie **bei Zeit√ºberschreitungen** immer noch das gespeicherte Notizbuch auf Ihrem Google Drive haben und nicht alles von Grund auf neu ausf√ºllen m√ºssen.\n",
    "\n",
    "Dazu kannst du entweder \"Strg + S\" oder \"Datei > Kopie in Google Drive speichern\" verwenden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PU4FVzaoM6fC"
   },
   "source": [
    "## Die GPU einstellen üí™.\n",
    "- Um **das Training des Agenten zu beschleunigen, werden wir einen Grafikprozessor** verwenden. Gehen Sie dazu auf \"Runtime > Change Runtime type\".\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step1.jpg\" alt=\"GPU Schritt 1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KV0NyFdQM9ZG"
   },
   "source": [
    "- Hardware-Beschleuniger > GPU\".\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step2.jpg\" alt=\"GPU Schritt 2\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bTpYcVZVMzUI"
   },
   "source": [
    "## Erstellen einer virtuellen Anzeige üñ•\n",
    "\n",
    "W√§hrend der Arbeit mit dem Notebook m√ºssen wir ein Wiederholungsvideo erstellen. Dazu ben√∂tigen wir mit colab **einen virtuellen Bildschirm, um die Umgebung zu rendern** (und somit die Bilder aufzunehmen).\n",
    "\n",
    "Daher wird die folgende Zelle die Librairies installieren und einen virtuellen Bildschirm erstellen und starten üñ•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jV6wjQ7Be7p5"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!apt install python-opengl\n",
    "!apt install ffmpeg\n",
    "!apt install xvfb\n",
    "!pip install pyvirtualdisplay\n",
    "!pip install pyglet==1.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sr-Nuyb1dBm0"
   },
   "outputs": [],
   "source": [
    "# Virtual display\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "virtual_display = Display(visible=0, size=(1400, 900))\n",
    "virtual_display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tjrLfPFIW8XK"
   },
   "source": [
    "## Installieren Sie die Abh√§ngigkeiten üîΩ.\n",
    "Der erste Schritt besteht darin, die Abh√§ngigkeiten zu installieren. Wir werden mehrere davon installieren:\n",
    "\n",
    "- `gym`\n",
    "- `gym-games`: Zus√§tzliche Gym-Umgebungen, die mit PyGame erstellt wurden.\n",
    "- `huggingface_hub`: ü§ó dient als zentraler Ort, an dem jeder Modelle und Datens√§tze teilen und erforschen kann. Es bietet Versionierung, Metriken, Visualisierungen und andere Funktionen, die eine einfache Zusammenarbeit mit anderen erm√∂glichen.\n",
    "\n",
    "Sie fragen sich vielleicht, warum wir gym und nicht gymnasium, eine neuere Version von gym, installieren? **Weil die Gym-Spiele, die wir verwenden, noch nicht mit Gym aktualisiert sind**.\n",
    "\n",
    "Die Unterschiede, die Sie hier finden werden:\n",
    "- In `gym` haben wir kein `terminated` und `truncated` sondern nur `done`.\n",
    "- In `gym` liefert die Verwendung von `env.step()` die Ergebnisse `state, reward, done, info`.\n",
    "\n",
    "Mehr √ºber die Unterschiede zwischen Gym und Gymnasium kannst du hier erfahren üëâ https://gymnasium.farama.org/content/migration-guide/\n",
    "\n",
    "\n",
    "Hier k√∂nnen Sie alle verf√ºgbaren Reinforce-Modelle sehen üëâ https://huggingface.co/models?other=reinforce\n",
    "\n",
    "Und hier finden Sie alle Deep Reinforcement Learning-Modelle üëâ https://huggingface.co/models?pipeline_tag=reinforcement-learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e8ZVi-uydpgL"
   },
   "outputs": [],
   "source": [
    "!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AAHAq6RZW3rn"
   },
   "source": [
    "## Importieren der Pakete üì¶\n",
    "Zus√§tzlich zum Import der installierten Bibliotheken, importieren wir auch:\n",
    "\n",
    "- `imageio`: Eine Bibliothek, die uns helfen wird, ein Wiedergabevideo zu erzeugen\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V8oadoJSWp7C"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# Gym\n",
    "import gym\n",
    "import gym_pygame\n",
    "\n",
    "# Hugging Face Hub\n",
    "from huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RfxJYdMeeVgv"
   },
   "source": [
    "## Pr√ºfen, ob wir eine GPU haben\n",
    "\n",
    "- Pr√ºfen wir, ob wir eine GPU haben\n",
    "- Wenn dies der Fall ist, sollten Sie `Ger√§t:cuda0` sehen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kaJu5FeZxXGY"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U5TNYa14aRav"
   },
   "outputs": [],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBPecCtBL_pZ"
   },
   "source": [
    "Wir sind nun bereit, unseren Reinforce-Algorithmus zu implementieren üî•."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8KEyKYo2ZSC-"
   },
   "source": [
    "# Erster Agent: Playing CartPole-v1 ü§ñ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "haLArKURMyuF"
   },
   "source": [
    "## Erstellen Sie die CartPole-Umgebung und verstehen Sie, wie sie funktioniert.\n",
    "### [Die Umgebung üéÆ](https://www.gymlibrary.dev/environments/classic_control/cart_pole/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AH_TaLKFXo_8"
   },
   "source": [
    "### Warum verwenden wir eine einfache Umgebung wie CartPole-v1?\n",
    "Wie in [Tipps und Tricks zum Verst√§rkungslernen] (https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html) erl√§utert, m√ºssen Sie bei der Neuimplementierung Ihres Agenten **sich vergewissern, dass er korrekt funktioniert, und mit einfachen Umgebungen Fehler finden, bevor Sie tiefer gehen**. Denn die Fehlersuche ist in einfachen Umgebungen viel einfacher.\n",
    "\n",
    "\n",
    "> Versuchen Sie, bei Spielzeugproblemen ein \"Lebenszeichen\" zu geben.\n",
    "\n",
    "\n",
    "> Validieren Sie die Implementierung, indem Sie sie auf immer schwierigeren Umgebungen laufen lassen (Sie k√∂nnen die Ergebnisse mit dem RL-Zoo vergleichen). F√ºr diesen Schritt m√ºssen Sie normalerweise eine Hyperparameter-Optimierung durchf√ºhren.\n",
    "___\n",
    "### Die CartPole-v1-Umgebung\n",
    "\n",
    "> Eine Stange ist √ºber ein unbet√§tigtes Gelenk an einem Wagen befestigt, der sich auf einer reibungsfreien Bahn bewegt. Das Pendel wird aufrecht auf dem Wagen platziert und das Ziel ist es, die Stange auszubalancieren, indem Kr√§fte in die linke und rechte Richtung auf den Wagen ausge√ºbt werden.\n",
    "\n",
    "\n",
    "\n",
    "Wir beginnen also mit CartPole-v1. Das Ziel ist es, den Wagen nach links oder rechts zu schieben, **so dass der Pol im Gleichgewicht bleibt.**\n",
    "\n",
    "Die Episode endet, wenn:\n",
    "- der Winkel der Stange gr√∂√üer als ¬±12¬∞ ist\n",
    "- Die Position des Wagens ist gr√∂√üer als ¬±2,4\n",
    "- Die Episodenl√§nge ist gr√∂√üer als 500\n",
    "\n",
    "Wir erhalten eine Belohnung üí∞ von +1 f√ºr jeden Zeitschritt, den der Pol im Gleichgewicht bleibt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "POOOk15_K6KA"
   },
   "outputs": [],
   "source": [
    "env_id = \"CartPole-v1\"\n",
    "# Create the env\n",
    "env = gym.make(env_id)\n",
    "\n",
    "# Create the evaluation env\n",
    "eval_env = gym.make(env_id)\n",
    "\n",
    "# Get the state space and action space\n",
    "s_size = env.observation_space.shape[0]\n",
    "a_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FMLFrjiBNLYJ"
   },
   "outputs": [],
   "source": [
    "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
    "print(\"The State Space is: \", s_size)\n",
    "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lu6t4sRNNWkN"
   },
   "outputs": [],
   "source": [
    "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
    "print(\"The Action Space is: \", a_size)\n",
    "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7SJMJj3WaFOz"
   },
   "source": [
    "## Bauen wir die Reinforce-Architektur auf\n",
    "Diese Implementierung basiert auf zwei Implementierungen:\n",
    "- [PyTorch official Reinforcement Learning example](https://github.com/pytorch/examples/blob/main/reinforcement_learning/reinforce.py)\n",
    "- [Udacity Reinforce](https://github.com/udacity/deep-reinforcement-learning/blob/master/reinforce/REINFORCE.ipynb)\n",
    "- [Verbesserung der Integration durch Chris1nexus](https://github.com/huggingface/deep-rl-class/pull/95)\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/reinforce.png\" alt=\"Reinforce\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "49kogtxBODX8"
   },
   "source": [
    "Wir wollen also:\n",
    "- Zwei vollst√§ndig verbundene Schichten (fc1 und fc2).\n",
    "- Verwendung von ReLU als Aktivierungsfunktion von fc1\n",
    "- Verwendung von Softmax zur Ausgabe einer Wahrscheinlichkeitsverteilung √ºber Aktionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w2LHcHhVZvPZ"
   },
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, s_size, a_size, h_size):\n",
    "        super(Policy, self).__init__()\n",
    "        # Create two fully connected layers\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Define the forward pass\n",
    "        # state goes to fc1 then we apply ReLU activation function\n",
    "\n",
    "        # fc1 outputs goes to fc2\n",
    "\n",
    "        # We output the softmax\n",
    "    \n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "        Given a state, take action\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        probs = self.forward(state).cpu()\n",
    "        m = Categorical(probs)\n",
    "        action = np.argmax(m)\n",
    "        return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rOMrdwSYOWSC"
   },
   "source": [
    "### L√∂sung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jGdhRSVrOV4K"
   },
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, s_size, a_size, h_size):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(s_size, h_size)\n",
    "        self.fc2 = nn.Linear(h_size, a_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "    \n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        probs = self.forward(state).cpu()\n",
    "        m = Categorical(probs)\n",
    "        action = np.argmax(m)\n",
    "        return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZTGWL4g2eM5B"
   },
   "source": [
    "Ich habe einen Fehler gemacht, k√∂nnen Sie erraten, wo?\n",
    "\n",
    "- Um das herauszufinden, machen wir einen Vorw√§rtspass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lwnqGBCNePor"
   },
   "outputs": [],
   "source": [
    "debug_policy = Policy(s_size, a_size, 64).to(device)\n",
    "debug_policy.act(env.reset())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14UYkoxCPaor"
   },
   "source": [
    "- Hier sehen wir, dass die Fehlermeldung `ValueError: Das Wertargument von log_prob muss ein Tensor sein\".\n",
    "\n",
    "- Das bedeutet, dass `action` in `m.log_prob(action)` ein Tensor sein muss **aber das ist er nicht.**\n",
    "\n",
    "- Wissen Sie, warum? Pr√ºfen Sie die Funktion act und versuchen Sie herauszufinden, warum sie nicht funktioniert.\n",
    "\n",
    "Hinweis üí°: Irgendetwas ist in dieser Implementierung falsch. Erinnern Sie sich daran, dass wir mit der Funktion act **eine Aktion aus der Wahrscheinlichkeitsverteilung √ºber Aktionen** ziehen wollen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gfGJNZBUP7Vn"
   },
   "source": [
    "### (Real) L√∂sung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ho_UHf49N9i4"
   },
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, s_size, a_size, h_size):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(s_size, h_size)\n",
    "        self.fc2 = nn.Linear(h_size, a_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "    \n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        probs = self.forward(state).cpu()\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgJWQFU_eUYw"
   },
   "source": [
    "Durch die Verwendung von CartPole war es einfacher zu debuggen, da **wir wissen, dass der Fehler von unserer Integration kommt und nicht von unserer einfachen Umgebung**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-20i7Pk0l1T"
   },
   "source": [
    "- Da **wir eine Aktion aus der Wahrscheinlichkeitsverteilung √ºber Aktionen** ausw√§hlen wollen, k√∂nnen wir nicht `Aktion = np.argmax(m)` verwenden, da dies immer die Aktion mit der h√∂chsten Wahrscheinlichkeit ausgibt.\n",
    "\n",
    "- Wir m√ºssen es durch `action = m.sample()` ersetzen, das eine Aktion aus der Wahrscheinlichkeitsverteilung P(.|s) ausw√§hlt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4MXoqetzfIoW"
   },
   "source": [
    "### Bauen wir den Reinforce-Trainingsalgorithmus auf\n",
    "Dies ist der Pseudocode des Reinforce-Algorithmus:\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/pg_pseudocode.png\" alt=\"Policy gradient pseudocode\"/>\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QmcXG-9i2Qu2"
   },
   "source": [
    "- Bei der Berechnung der Rendite Gt (Zeile 6) sehen wir, dass wir die Summe der diskontierten Renditen **ausgehend vom Zeitschritt t** berechnen.\n",
    "\n",
    "- Warum? Weil unsere Policy nur **Handlungen auf der Grundlage der Konsequenzen verst√§rken sollte**: Belohnungen, die vor einer Handlung erzielt wurden, sind also nutzlos (da sie nicht auf die Handlung zur√ºckzuf√ºhren sind), **nur die Belohnungen, die nach der Handlung kommen, z√§hlen**.\n",
    "\n",
    "- Bevor Sie dies kodieren, sollten Sie diesen Abschnitt lesen [lassen Sie sich nicht von der Vergangenheit ablenken] (https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#don-t-let-the-past-distract-you), in dem erkl√§rt wird, warum wir den Reward-to-Go-Gradienten verwenden.\n",
    "\n",
    "Wir verwenden eine interessante Technik, die von [Chris1nexus](https://github.com/Chris1nexus) kodiert wurde, um **die Rendite in jedem Zeitschritt effizient zu berechnen**. In den Kommentaren wird das Verfahren erl√§utert. Z√∂gern Sie nicht, auch [die PR-Erkl√§rung zu lesen](https://github.com/huggingface/deep-rl-class/pull/95)\n",
    "Aber im Gro√üen und Ganzen geht es darum, **die Rendite in jedem Zeitschritt effizient zu berechnen**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O554nUGPpcoq"
   },
   "source": [
    "Die zweite Frage, die Sie sich stellen k√∂nnen, ist **Warum minimieren wir den Verlust**? Sie sprachen von Gradientenaufstieg und nicht von Gradientenabstieg?\n",
    "\n",
    "- Wir wollen unsere Nutzenfunktion $J(\\theta)$ maximieren, aber in PyTorch wie in Tensorflow ist es besser, eine Zielfunktion zu **minimieren**.\n",
    "    - Nehmen wir also an, wir wollen Aktion 3 in einem bestimmten Zeitschritt verst√§rken. Vor dem Training dieser Aktion ist P 0.25.\n",
    "    - Wir wollen also $\\theta$ so ver√§ndern, dass $\\pi_\\theta(a_3|s; \\theta) > 0.25$\n",
    "    - Da sich alle P zu 1 summieren m√ºssen, wird max $\\pi_\\theta(a_3|s; \\theta)$ die **Minimierung der anderen Aktionswahrscheinlichkeit$ bewirken.\n",
    "    - Wir sollten PyTorch also anweisen, $1 - \\pi_\\theta(a_3|s; \\theta)$ zu minimieren.\n",
    "    - Diese Verlustfunktion n√§hert sich 0, wenn $\\pi_\\theta(a_3|s; \\theta)$ sich 1 n√§hert.\n",
    "    - Wir ermutigen also den Gradienten zu maximal $\\pi_\\theta(a_3|s; \\theta)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iOdv8Q9NfLK7"
   },
   "outputs": [],
   "source": [
    "def reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):\n",
    "    # Help us to calculate the score during the training\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    # Line 3 of pseudocode\n",
    "    for i_episode in range(1, n_training_episodes+1):\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state = # TODO: reset the environment\n",
    "        # Line 4 of pseudocode\n",
    "        for t in range(max_t):\n",
    "            action, log_prob = # TODO get the action\n",
    "            saved_log_probs.append(log_prob)\n",
    "            state, reward, done, _ = # TODO: take an env step\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break \n",
    "        scores_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "        \n",
    "        # Line 6 of pseudocode: calculate the return\n",
    "        returns = deque(maxlen=max_t) \n",
    "        n_steps = len(rewards) \n",
    "        # Compute the discounted returns at each timestep,\n",
    "        # as the sum of the gamma-discounted return at time t (G_t) + the reward at time t\n",
    "        \n",
    "        # In O(N) time, where N is the number of time steps\n",
    "        # (this definition of the discounted return G_t follows the definition of this quantity \n",
    "        # shown at page 44 of Sutton&Barto 2017 2nd draft)\n",
    "        # G_t = r_(t+1) + r_(t+2) + ...\n",
    "        \n",
    "        # Given this formulation, the returns at each timestep t can be computed \n",
    "        # by re-using the computed future returns G_(t+1) to compute the current return G_t\n",
    "        # G_t = r_(t+1) + gamma*G_(t+1)\n",
    "        # G_(t-1) = r_t + gamma* G_t\n",
    "        # (this follows a dynamic programming approach, with which we memorize solutions in order \n",
    "        # to avoid computing them multiple times)\n",
    "        \n",
    "        # This is correct since the above is equivalent to (see also page 46 of Sutton&Barto 2017 2nd draft)\n",
    "        # G_(t-1) = r_t + gamma*r_(t+1) + gamma*gamma*r_(t+2) + ...\n",
    "        \n",
    "        \n",
    "        ## Given the above, we calculate the returns at timestep t as: \n",
    "        #               gamma[t] * return[t] + reward[t]\n",
    "        #\n",
    "        ## We compute this starting from the last timestep to the first, in order\n",
    "        ## to employ the formula presented above and avoid redundant computations that would be needed \n",
    "        ## if we were to do it from first to last.\n",
    "        \n",
    "        ## Hence, the queue \"returns\" will hold the returns in chronological order, from t=0 to t=n_steps\n",
    "        ## thanks to the appendleft() function which allows to append to the position 0 in constant time O(1)\n",
    "        ## a normal python list would instead require O(N) to do this.\n",
    "        for t in range(n_steps)[::-1]:\n",
    "            disc_return_t = (returns[0] if len(returns)>0 else 0)\n",
    "            returns.appendleft(    ) # TODO: complete here        \n",
    "       \n",
    "        ## standardization of the returns is employed to make training more stable\n",
    "        eps = np.finfo(np.float32).eps.item()\n",
    "        \n",
    "        ## eps is the smallest representable float, which is \n",
    "        # added to the standard deviation of the returns to avoid numerical instabilities\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "        \n",
    "        # Line 7:\n",
    "        policy_loss = []\n",
    "        for log_prob, disc_return in zip(saved_log_probs, returns):\n",
    "            policy_loss.append(-log_prob * disc_return)\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "        \n",
    "        # Line 8: PyTorch prefers gradient descent \n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i_episode % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YB0Cxrw1StrP"
   },
   "source": [
    "#### L√∂sung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NCNvyElRStWG"
   },
   "outputs": [],
   "source": [
    "def reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):\n",
    "    # Help us to calculate the score during the training\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    # Line 3 of pseudocode\n",
    "    for i_episode in range(1, n_training_episodes+1):\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state = env.reset()\n",
    "        # Line 4 of pseudocode\n",
    "        for t in range(max_t):\n",
    "            action, log_prob = policy.act(state)\n",
    "            saved_log_probs.append(log_prob)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break \n",
    "        scores_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "        \n",
    "        # Line 6 of pseudocode: calculate the return\n",
    "        returns = deque(maxlen=max_t) \n",
    "        n_steps = len(rewards) \n",
    "        # Compute the discounted returns at each timestep,\n",
    "        # as \n",
    "        #      the sum of the gamma-discounted return at time t (G_t) + the reward at time t\n",
    "        #\n",
    "        # In O(N) time, where N is the number of time steps\n",
    "        # (this definition of the discounted return G_t follows the definition of this quantity \n",
    "        # shown at page 44 of Sutton&Barto 2017 2nd draft)\n",
    "        # G_t = r_(t+1) + r_(t+2) + ...\n",
    "        \n",
    "        # Given this formulation, the returns at each timestep t can be computed \n",
    "        # by re-using the computed future returns G_(t+1) to compute the current return G_t\n",
    "        # G_t = r_(t+1) + gamma*G_(t+1)\n",
    "        # G_(t-1) = r_t + gamma* G_t\n",
    "        # (this follows a dynamic programming approach, with which we memorize solutions in order \n",
    "        # to avoid computing them multiple times)\n",
    "        \n",
    "        # This is correct since the above is equivalent to (see also page 46 of Sutton&Barto 2017 2nd draft)\n",
    "        # G_(t-1) = r_t + gamma*r_(t+1) + gamma*gamma*r_(t+2) + ...\n",
    "        \n",
    "        \n",
    "        ## Given the above, we calculate the returns at timestep t as: \n",
    "        #               gamma[t] * return[t] + reward[t]\n",
    "        #\n",
    "        ## We compute this starting from the last timestep to the first, in order\n",
    "        ## to employ the formula presented above and avoid redundant computations that would be needed \n",
    "        ## if we were to do it from first to last.\n",
    "        \n",
    "        ## Hence, the queue \"returns\" will hold the returns in chronological order, from t=0 to t=n_steps\n",
    "        ## thanks to the appendleft() function which allows to append to the position 0 in constant time O(1)\n",
    "        ## a normal python list would instead require O(N) to do this.\n",
    "        for t in range(n_steps)[::-1]:\n",
    "            disc_return_t = (returns[0] if len(returns)>0 else 0)\n",
    "            returns.appendleft( gamma*disc_return_t + rewards[t]   )    \n",
    "            \n",
    "        ## standardization of the returns is employed to make training more stable\n",
    "        eps = np.finfo(np.float32).eps.item()\n",
    "        ## eps is the smallest representable float, which is \n",
    "        # added to the standard deviation of the returns to avoid numerical instabilities        \n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "        \n",
    "        # Line 7:\n",
    "        policy_loss = []\n",
    "        for log_prob, disc_return in zip(saved_log_probs, returns):\n",
    "            policy_loss.append(-log_prob * disc_return)\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "        \n",
    "        # Line 8: PyTorch prefers gradient descent \n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i_episode % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RIWhQyJjfpEt"
   },
   "source": [
    "## Train it\n",
    "- Wir sind nun bereit, unseren Agenten zu trainieren.\n",
    "- Aber zuerst definieren wir eine Variable, die alle Trainingshyperparameter enth√§lt.\n",
    "- Sie k√∂nnen die Trainingsparameter √§ndern (und sollten üòâ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "utRe1NgtVBYF"
   },
   "outputs": [],
   "source": [
    "cartpole_hyperparameters = {\n",
    "    \"h_size\": 16,\n",
    "    \"n_training_episodes\": 1000,\n",
    "    \"n_evaluation_episodes\": 10,\n",
    "    \"max_t\": 1000,\n",
    "    \"gamma\": 1.0,\n",
    "    \"lr\": 1e-2,\n",
    "    \"env_id\": env_id,\n",
    "    \"state_space\": s_size,\n",
    "    \"action_space\": a_size,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D3lWyVXBVfl6"
   },
   "outputs": [],
   "source": [
    "# Create policy and place it to the device\n",
    "cartpole_policy = Policy(cartpole_hyperparameters[\"state_space\"], cartpole_hyperparameters[\"action_space\"], cartpole_hyperparameters[\"h_size\"]).to(device)\n",
    "cartpole_optimizer = optim.Adam(cartpole_policy.parameters(), lr=cartpole_hyperparameters[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uGf-hQCnfouB"
   },
   "outputs": [],
   "source": [
    "scores = reinforce(cartpole_policy,\n",
    "                   cartpole_optimizer,\n",
    "                   cartpole_hyperparameters[\"n_training_episodes\"], \n",
    "                   cartpole_hyperparameters[\"max_t\"],\n",
    "                   cartpole_hyperparameters[\"gamma\"], \n",
    "                   100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qajj2kXqhB3g"
   },
   "source": [
    "## Bewertungsmethode definieren üìù\n",
    "- Hier definieren wir die Auswertungsmethode, mit der wir unseren Reinforce-Agenten testen werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3FamHmxyhBEU"
   },
   "outputs": [],
   "source": [
    "def evaluate_agent(env, max_steps, n_eval_episodes, policy):\n",
    "  \"\"\"\n",
    "  Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n",
    "  :param env: The evaluation environment\n",
    "  :param n_eval_episodes: Number of episode to evaluate the agent\n",
    "  :param policy: The Reinforce agent\n",
    "  \"\"\"\n",
    "  episode_rewards = []\n",
    "  for episode in range(n_eval_episodes):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards_ep = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "      action, _ = policy.act(state)\n",
    "      new_state, reward, done, info = env.step(action)\n",
    "      total_rewards_ep += reward\n",
    "        \n",
    "      if done:\n",
    "        break\n",
    "      state = new_state\n",
    "    episode_rewards.append(total_rewards_ep)\n",
    "  mean_reward = np.mean(episode_rewards)\n",
    "  std_reward = np.std(episode_rewards)\n",
    "\n",
    "  return mean_reward, std_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xdH2QCrLTrlT"
   },
   "source": [
    "## Bewerten Sie unseren Agenten üìà"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ohGSXDyHh0xx"
   },
   "outputs": [],
   "source": [
    "evaluate_agent(eval_env, \n",
    "               cartpole_hyperparameters[\"max_t\"], \n",
    "               cartpole_hyperparameters[\"n_evaluation_episodes\"],\n",
    "               cartpole_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7CoeLkQ7TpO8"
   },
   "source": [
    "### Ver√∂ffentlichen Sie unser trainiertes Modell auf dem Hub üî•\n",
    "Da wir nun gesehen haben, dass wir nach dem Training gute Ergebnisse erzielt haben, k√∂nnen wir unser trainiertes Modell mit einer Zeile Code auf dem Hub ü§ó ver√∂ffentlichen.\n",
    "\n",
    "Hier ist ein Beispiel f√ºr eine Model Card:\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/modelcard.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jmhs1k-cftIq"
   },
   "source": [
    "### Push zum Hub\n",
    "#### Diesen Code nicht ver√§ndern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LIVsvlW_8tcw"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, snapshot_download\n",
    "from huggingface_hub.repocard import metadata_eval_result, metadata_save\n",
    "\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import json\n",
    "import imageio\n",
    "\n",
    "import tempfile\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lo4JH45if81z"
   },
   "outputs": [],
   "source": [
    "def record_video(env, policy, out_directory, fps=30):\n",
    "  \"\"\"\n",
    "  Generate a replay video of the agent\n",
    "  :param env\n",
    "  :param Qtable: Qtable of our agent\n",
    "  :param out_directory\n",
    "  :param fps: how many frame per seconds (with taxi-v3 and frozenlake-v1 we use 1)\n",
    "  \"\"\"\n",
    "  images = []  \n",
    "  done = False\n",
    "  state = env.reset()\n",
    "  img = env.render(mode='rgb_array')\n",
    "  images.append(img)\n",
    "  while not done:\n",
    "    # Take the action (index) that have the maximum expected future reward given that state\n",
    "    action, _ = policy.act(state)\n",
    "    state, reward, done, info = env.step(action) # We directly put next_state = state for recording logic\n",
    "    img = env.render(mode='rgb_array')\n",
    "    images.append(img)\n",
    "  imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_TPdq47D7_f_"
   },
   "outputs": [],
   "source": [
    "def push_to_hub(repo_id, \n",
    "                model,\n",
    "                hyperparameters,\n",
    "                eval_env,\n",
    "                video_fps=30\n",
    "                ):\n",
    "  \"\"\"\n",
    "  Evaluate, Generate a video and Upload a model to Hugging Face Hub.\n",
    "  This method does the complete pipeline:\n",
    "  - It evaluates the model\n",
    "  - It generates the model card\n",
    "  - It generates a replay video of the agent\n",
    "  - It pushes everything to the Hub\n",
    "\n",
    "  :param repo_id: repo_id: id of the model repository from the Hugging Face Hub\n",
    "  :param model: the pytorch model we want to save\n",
    "  :param hyperparameters: training hyperparameters\n",
    "  :param eval_env: evaluation environment\n",
    "  :param video_fps: how many frame per seconds to record our video replay \n",
    "  \"\"\"\n",
    "\n",
    "  _, repo_name = repo_id.split(\"/\")\n",
    "  api = HfApi()\n",
    "  \n",
    "  # Step 1: Create the repo\n",
    "  repo_url = api.create_repo(\n",
    "        repo_id=repo_id,\n",
    "        exist_ok=True,\n",
    "  )\n",
    "\n",
    "  with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    local_directory = Path(tmpdirname)\n",
    "  \n",
    "    # Step 2: Save the model\n",
    "    torch.save(model, local_directory / \"model.pt\")\n",
    "\n",
    "    # Step 3: Save the hyperparameters to JSON\n",
    "    with open(local_directory / \"hyperparameters.json\", \"w\") as outfile:\n",
    "      json.dump(hyperparameters, outfile)\n",
    "    \n",
    "    # Step 4: Evaluate the model and build JSON\n",
    "    mean_reward, std_reward = evaluate_agent(eval_env, \n",
    "                                            hyperparameters[\"max_t\"],\n",
    "                                            hyperparameters[\"n_evaluation_episodes\"], \n",
    "                                            model)\n",
    "    # Get datetime\n",
    "    eval_datetime = datetime.datetime.now()\n",
    "    eval_form_datetime = eval_datetime.isoformat()\n",
    "\n",
    "    evaluate_data = {\n",
    "          \"env_id\": hyperparameters[\"env_id\"], \n",
    "          \"mean_reward\": mean_reward,\n",
    "          \"n_evaluation_episodes\": hyperparameters[\"n_evaluation_episodes\"],\n",
    "          \"eval_datetime\": eval_form_datetime,\n",
    "    }\n",
    "\n",
    "    # Write a JSON file\n",
    "    with open(local_directory / \"results.json\", \"w\") as outfile:\n",
    "        json.dump(evaluate_data, outfile)\n",
    "\n",
    "    # Step 5: Create the model card\n",
    "    env_name = hyperparameters[\"env_id\"]\n",
    "    \n",
    "    metadata = {}\n",
    "    metadata[\"tags\"] = [\n",
    "          env_name,\n",
    "          \"reinforce\",\n",
    "          \"reinforcement-learning\",\n",
    "          \"custom-implementation\",\n",
    "          \"deep-rl-class\"\n",
    "      ]\n",
    "\n",
    "    # Add metrics\n",
    "    eval = metadata_eval_result(\n",
    "        model_pretty_name=repo_name,\n",
    "        task_pretty_name=\"reinforcement-learning\",\n",
    "        task_id=\"reinforcement-learning\",\n",
    "        metrics_pretty_name=\"mean_reward\",\n",
    "        metrics_id=\"mean_reward\",\n",
    "        metrics_value=f\"{mean_reward:.2f} +/- {std_reward:.2f}\",\n",
    "        dataset_pretty_name=env_name,\n",
    "        dataset_id=env_name,\n",
    "      )\n",
    "\n",
    "    # Merges both dictionaries\n",
    "    metadata = {**metadata, **eval}\n",
    "\n",
    "    model_card = f\"\"\"\n",
    "  # **Reinforce** Agent playing **{env_id}**\n",
    "  This is a trained model of a **Reinforce** agent playing **{env_id}** .\n",
    "  To learn to use this model and train yours check Unit 4 of the Deep Reinforcement Learning Course: https://huggingface.co/deep-rl-course/unit4/introduction\n",
    "  \"\"\"\n",
    "\n",
    "    readme_path = local_directory / \"README.md\"\n",
    "    readme = \"\"\n",
    "    if readme_path.exists():\n",
    "        with readme_path.open(\"r\", encoding=\"utf8\") as f:\n",
    "          readme = f.read()\n",
    "    else:\n",
    "      readme = model_card\n",
    "\n",
    "    with readme_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "      f.write(readme)\n",
    "\n",
    "    # Save our metrics to Readme metadata\n",
    "    metadata_save(readme_path, metadata)\n",
    "\n",
    "    # Step 6: Record a video\n",
    "    video_path =  local_directory / \"replay.mp4\"\n",
    "    record_video(env, model, video_path, video_fps)\n",
    "\n",
    "    # Step 7. Push everything to the Hub\n",
    "    api.upload_folder(\n",
    "          repo_id=repo_id,\n",
    "          folder_path=local_directory,\n",
    "          path_in_repo=\".\",\n",
    "    )\n",
    "\n",
    "    print(f\"Your model is pushed to the Hub. You can view your model here: {repo_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w17w8CxzoURM"
   },
   "source": [
    "### .\n",
    "\n",
    "Mit \"push_to_hub\" **werten Sie aus, zeichnen ein Replay auf, generieren eine Modellkarte Ihres Agenten und schieben sie an den Hub**.\n",
    "\n",
    "This way:\n",
    "- Sie k√∂nnen **unsere Arbeit vorf√ºhren** üî•.\n",
    "- Sie k√∂nnen **Ihren Agenten beim Spielen visualisieren** üëÄ\n",
    "- Du kannst **einen Agenten mit der Community teilen, den andere benutzen k√∂nnen** üíæ\n",
    "- Sie k√∂nnen **auf eine Bestenliste üèÜ zugreifen, um zu sehen, wie gut Ihr Agent im Vergleich zu Ihren Klassenkameraden abschneidet** üëâ https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cWnFC0iZooTw"
   },
   "source": [
    "Um Ihr Modell mit der Gemeinschaft teilen zu k√∂nnen, sind drei weitere Schritte erforderlich:\n",
    "\n",
    "1Ô∏è‚É£ (Falls noch nicht geschehen) Erstellen Sie ein Konto f√ºr HF ‚û° https://huggingface.co/join\n",
    "\n",
    "2Ô∏è‚É£ Melde dich an und speichere dann dein Authentifizierungs-Token von der Hugging Face Website.\n",
    "- Erstellen Sie ein neues Token (https://huggingface.co/settings/tokens) **mit Schreibrolle**\n",
    "\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/create-token.jpg\" alt=\"HF-Token erstellen\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QB5nIcxR8paT"
   },
   "outputs": [],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GyWc1x3-o3xG"
   },
   "source": [
    "Wenn Sie kein Google Colab oder ein Jupyter Notebook verwenden m√∂chten, m√ºssen Sie stattdessen diesen Befehl verwenden: `huggingface-cli login` (oder `login`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-D-zhbRoeOm"
   },
   "source": [
    "3Ô∏è‚É£ Wir sind jetzt bereit, unseren trainierten Agenten mit der Funktion `package_to_hub()` an den ü§ó Hub üî• zu √ºbertragen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UNwkTS65Uq3Q"
   },
   "outputs": [],
   "source": [
    "repo_id = \"\" #TODO Define your repo id {username/Reinforce-{model-id}}\n",
    "push_to_hub(repo_id,\n",
    "                cartpole_policy, # The model we want to save\n",
    "                cartpole_hyperparameters, # Hyperparameters\n",
    "                eval_env, # Evaluation environment\n",
    "                video_fps=30\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jrnuKH1gYZSz"
   },
   "source": [
    "Nachdem wir nun die Robustheit unserer Implementierung getestet haben, wollen wir eine komplexere Umgebung ausprobieren: PixelCopter üöÅ.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JNLVmKKVKA6j"
   },
   "source": [
    "## Zweiter Agent: PixelCopter üöÅ.\n",
    "\n",
    "### Studiere die PixelCopter-Umgebung üëÄ\n",
    "- [Die Umgebungsdokumentation](https://pygame-learning-environment.readthedocs.io/en/latest/user/games/pixelcopter.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JBSc8mlfyin3"
   },
   "outputs": [],
   "source": [
    "env_id = \"Pixelcopter-PLE-v0\"\n",
    "env = gym.make(env_id)\n",
    "eval_env = gym.make(env_id)\n",
    "s_size = env.observation_space.shape[0]\n",
    "a_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L5u_zAHsKBy7"
   },
   "outputs": [],
   "source": [
    "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
    "print(\"The State Space is: \", s_size)\n",
    "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D7yJM9YXKNbq"
   },
   "outputs": [],
   "source": [
    "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
    "print(\"The Action Space is: \", a_size)\n",
    "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NNWvlyvzalXr"
   },
   "source": [
    "Der Beobachtungsraum (7) üëÄ:\n",
    "- Spieler-Y-Position\n",
    "- Spieler-Geschwindigkeit\n",
    "- Abstand des Spielers zum Boden\n",
    "- Abstand des Spielers zur Decke\n",
    "- n√§chster Block x Abstand zum Spieler\n",
    "- n√§chste Bl√∂cke obere y-Position\n",
    "- n√§chste Bl√∂cke untere y-Position\n",
    "\n",
    "Der Aktionsraum(2) üéÆ:\n",
    "- Nach oben (Gaspedal dr√ºcken)\n",
    "- Nichts tun (nicht auf den Beschleuniger dr√ºcken)\n",
    "\n",
    "Die Belohnungsfunktion üí∞:\n",
    "- F√ºr jeden vertikalen Block, den er durchl√§uft, erh√§lt er eine positive Belohnung von +1. Jedes Mal, wenn ein Endzustand erreicht wird, erh√§lt er eine negative Belohnung von -1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aV1466QP8crz"
   },
   "source": [
    "### Definieren Sie die neue Richtlinie üß†\n",
    "- Wir brauchen ein tieferes neuronales Netz, da die Umgebung komplexer ist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I1eBkCiX2X_S"
   },
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, s_size, a_size, h_size):\n",
    "        super(Policy, self).__init__()\n",
    "        # Define the three layers here\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Define the forward process here\n",
    "        return F.softmax(x, dim=1)\n",
    "    \n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        probs = self.forward(state).cpu()\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47iuAFqV8Ws-"
   },
   "source": [
    "#### L√∂sung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wrNuVcHC8Xu7"
   },
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, s_size, a_size, h_size):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(s_size, h_size)\n",
    "        self.fc2 = nn.Linear(h_size, h_size*2)\n",
    "        self.fc3 = nn.Linear(h_size*2, a_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "    \n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        probs = self.forward(state).cpu()\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SM1QiGCSbBkM"
   },
   "source": [
    "### Definieren Sie die Hyperparameter ‚öôÔ∏è\n",
    "- Denn diese Umgebung ist komplexer.\n",
    "- Insbesondere f√ºr die versteckte Gr√∂√üe ben√∂tigen wir mehr Neuronen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y0uujOR_ypB6"
   },
   "outputs": [],
   "source": [
    "pixelcopter_hyperparameters = {\n",
    "    \"h_size\": 64,\n",
    "    \"n_training_episodes\": 50000,\n",
    "    \"n_evaluation_episodes\": 10,\n",
    "    \"max_t\": 10000,\n",
    "    \"gamma\": 0.99,\n",
    "    \"lr\": 1e-4,\n",
    "    \"env_id\": env_id,\n",
    "    \"state_space\": s_size,\n",
    "    \"action_space\": a_size,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wyvXTJWm9GJG"
   },
   "source": [
    "### Train it\n",
    "- Wir sind jetzt bereit, unseren Agenten zu trainieren üî•."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7mM2P_ckysFE"
   },
   "outputs": [],
   "source": [
    "# Create policy and place it to the device\n",
    "# torch.manual_seed(50)\n",
    "pixelcopter_policy = Policy(pixelcopter_hyperparameters[\"state_space\"], pixelcopter_hyperparameters[\"action_space\"], pixelcopter_hyperparameters[\"h_size\"]).to(device)\n",
    "pixelcopter_optimizer = optim.Adam(pixelcopter_policy.parameters(), lr=pixelcopter_hyperparameters[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v1HEqP-fy-Rf"
   },
   "outputs": [],
   "source": [
    "scores = reinforce(pixelcopter_policy,\n",
    "                   pixelcopter_optimizer,\n",
    "                   pixelcopter_hyperparameters[\"n_training_episodes\"], \n",
    "                   pixelcopter_hyperparameters[\"max_t\"],\n",
    "                   pixelcopter_hyperparameters[\"gamma\"], \n",
    "                   1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8kwFQ-Ip85BE"
   },
   "source": [
    "### Ver√∂ffentlichen Sie unser trainiertes Modell auf dem Hub üî•."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6PtB7LRbTKWK"
   },
   "outputs": [],
   "source": [
    "repo_id = \"\" #TODO Define your repo id {username/Reinforce-{model-id}}\n",
    "push_to_hub(repo_id,\n",
    "                pixelcopter_policy, # The model we want to save\n",
    "                pixelcopter_hyperparameters, # Hyperparameters\n",
    "                eval_env, # Evaluation environment\n",
    "                video_fps=30\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7VDcJ29FcOyb"
   },
   "source": [
    "## Einige zus√§tzliche Herausforderungen üèÜ\n",
    "Die beste Art zu lernen **ist, Dinge selbst auszuprobieren**! Wie Sie gesehen haben, ist der derzeitige Agent nicht besonders gut. Als ersten Vorschlag k√∂nnen Sie f√ºr mehr Schritte trainieren. Versuchen Sie aber auch, bessere Parameter zu finden.\n",
    "\n",
    "In der [Rangliste] (https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) finden Sie Ihre Agenten. K√∂nnen Sie sich an die Spitze setzen?\n",
    "\n",
    "Hier sind einige Ideen, um dies zu erreichen:\n",
    "* Trainiere mehr Schritte\n",
    "* Probiere verschiedene Hyperparameter aus, indem du dir ansiehst, was deine Klassenkameraden gemacht haben üëâ https://huggingface.co/models?other=reinforce\n",
    "* **Pushen Sie Ihr neu trainiertes Modell** auf dem Hub üî•\n",
    "* **Verbesserung der Implementierung f√ºr komplexere Umgebungen** (wie w√§re es z.B., das Netzwerk in ein Convolutional Neural Network zu √§ndern, um\n",
    "Frames als Beobachtung zu behandeln)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x62pP0PHdA-y"
   },
   "source": [
    "________________________________________________________________________\n",
    "\n",
    "**Gl√ºckwunsch zum Abschluss dieser Einheit**! Es gab eine Menge Informationen.\n",
    "Und herzlichen Gl√ºckwunsch zum Abschluss des Tutorials. Du hast gerade deinen ersten Deep Reinforcement Learning-Agenten von Grund auf mit PyTorch programmiert und ihn im Hub geteilt ü•≥.\n",
    "\n",
    "Z√∂gern Sie nicht, diese Einheit zu iterieren **durch Verbesserung der Implementierung f√ºr komplexere Umgebungen** (wie w√§re es zum Beispiel, das Netzwerk in ein Convolutional Neural Network zu √§ndern, um\n",
    "Frames als Beobachtung zu behandeln)?\n",
    "\n",
    "In der n√§chsten Einheit **werden wir mehr √ºber Unity MLAgents** lernen, indem wir Agenten in Unity-Umgebungen trainieren. Auf diese Weise werden Sie bereit sein, an den **AI vs. AI Herausforderungen teilzunehmen, bei denen Sie Ihre Agenten trainieren werden\n",
    "gegen andere Agenten in einer Schneeballschlacht und einem Fu√üballspiel antreten**.\n",
    "\n",
    "Klingt lustig? Bis zum n√§chsten Mal!\n",
    "\n",
    "Zum Schluss w√ºrden wir gerne **h√∂ren, was Sie √ºber den Kurs denken und wie wir ihn verbessern k√∂nnen**. Wenn Sie also ein Feedback haben, bitte üëâ [f√ºllen Sie dieses Formular aus] (https://forms.gle/BzKXWzLAGZESGNaE9)\n",
    "\n",
    "Wir sehen uns in Referat 5! üî•\n",
    "\n",
    "### Keep Learning, stay awesome ü§ó\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "BPLwsPajb1f8",
    "L_WSo0VUV99t",
    "mjY-eq3eWh9O",
    "JoTC9o2SczNn",
    "gfGJNZBUP7Vn",
    "YB0Cxrw1StrP",
    "47iuAFqV8Ws-",
    "x62pP0PHdA-y"
   ],
   "include_colab_link": true,
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
