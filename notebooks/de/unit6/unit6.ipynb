{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/notebooks/unit6/unit6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"In Colab √∂ffnen\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-PTReiOw-RAN"
   },
   "source": [
    "# Einheit 6: Advantage Actor Critic (A2C) unter Verwendung von Robotik-Simulationen mit Panda-Gym ü§ñ\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit8/thumbnail.png\" alt=\"Thumbnail\"/>\n",
    "\n",
    "In diesem Notizbuch lernst du, A2C mit [Panda-Gym](https://github.com/qgallouedec/panda-gym) zu verwenden. Sie werden **einen Roboterarm** (Franka Emika Panda Roboter) trainieren, um eine Aufgabe auszuf√ºhren:\n",
    "\n",
    "- Reichweite\": Der Roboter muss seinen Endeffektor an einer Zielposition platzieren.\n",
    "\n",
    "Danach k√∂nnen Sie **andere Robotik-Aufgaben** trainieren.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QInFitfWno1Q"
   },
   "source": [
    "### üéÆ Umgebungen:\n",
    "\n",
    "- [Panda-Gym](https://github.com/qgallouedec/panda-gym)\n",
    "\n",
    "###üìö RL-Library:\n",
    "\n",
    "- [Stable-Baselines3](https://stable-baselines3.readthedocs.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2CcdX4g3oFlp"
   },
   "source": [
    "Wir versuchen st√§ndig, unsere Tutorials zu verbessern, also **wenn Sie Probleme in diesem Notizbuch** finden, √∂ffnen Sie bitte [einen Fehler im GitHub Repo](https://github.com/huggingface/deep-rl-class/issues)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MoubJX20oKaQ"
   },
   "source": [
    "## Ziele dieses Notizbuchs üèÜ\n",
    "\n",
    "Am Ende des Notizbuchs werden Sie:\n",
    "\n",
    "- In der Lage sein, **Panda-Gym**, die Umgebungsbibliothek, zu benutzen.\n",
    "- In der Lage sein, **Roboter mit A2C** zu trainieren.\n",
    "- Verstehen, warum **wir die Eingabe normalisieren m√ºssen**.\n",
    "- In der Lage sein, **Ihren trainierten Agenten und den Code auf den Hub zu pushen**, mit einer sch√∂nen Videowiedergabe und einer Bewertungsnote üî•.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DoUNkTExoUED"
   },
   "source": [
    "## Dieses Notizbuch stammt aus dem Deep Reinforcement Learning Kurs.\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/deep-rl-course-illustration.jpg\" alt=\"Deep RL Course illustration\"/>\n",
    "\n",
    "In diesem kostenlosen Kurs werden Sie:\n",
    "\n",
    "- üìñ Deep Reinforcement Learning in **Theorie und Praxis** studieren.\n",
    "- üßë‚Äçüíª Lernen Sie, **ber√ºhmte Deep RL-Bibliotheken** wie Stable Baselines3, RL Baselines3 Zoo, CleanRL und Sample Factory 2.0 zu verwenden.\n",
    "- ü§ñ Trainieren Sie **Agenten in einzigartigen Umgebungen**.\n",
    "\n",
    "Und mehr, siehe üìö den Lehrplan üëâ https://simoninithomas.github.io/deep-rl-course\n",
    "\n",
    "Vergessen Sie nicht, sich **<a href=\"http://eepurl.com/ic5ZUD\">f√ºr den Kurs anzumelden</a>** (wir sammeln Ihre E-Mail, um Ihnen **die Links zu senden, wenn die einzelnen Einheiten ver√∂ffentlicht werden, und Sie √ºber die Herausforderungen und Aktualisierungen zu informieren).**\n",
    "\n",
    "\n",
    "Der beste Weg, um in Kontakt zu bleiben, ist, unserem Discord-Server beizutreten, um sich mit der Community und mit uns auszutauschen üëâüèª https://discord.gg/ydHrjt3WP5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BTuQAUAPoa5E"
   },
   "source": [
    "## Voraussetzungen üèóÔ∏è\n",
    "Bevor Sie sich mit dem Notebook besch√§ftigen, m√ºssen Sie:\n",
    "\n",
    "üî≤ üìö Studium der [akteurskritischen Methoden durch Lekt√ºre von Einheit 6](https://huggingface.co/deep-rl-course/unit6/introduction) ü§ó"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iajHvVDWoo01"
   },
   "source": [
    "# Lasst uns unsere ersten Roboter trainieren ü§ñ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zbOENTE2os_D"
   },
   "source": [
    "Um dieses Hands-on f√ºr den [Zertifizierungsprozess] (https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process) zu validieren, m√ºssen Sie Ihr trainiertes Modell an den Hub senden und die folgenden Ergebnisse erhalten:\n",
    "\n",
    "- PandaReachDense-v3\" erh√§lt ein Ergebnis von >= -3,5.\n",
    "\n",
    "Um Ihr Ergebnis zu finden, gehen Sie zur [Bestenliste] (https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) und suchen Sie Ihr Modell, **das Ergebnis = mean_reward - std of reward**\n",
    "\n",
    "Weitere Informationen √ºber den Zertifizierungsprozess finden Sie in diesem Abschnitt üëâ https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PU4FVzaoM6fC"
   },
   "source": [
    "## Die GPU einstellen üí™.\n",
    "- Um **das Training des Agenten zu beschleunigen, werden wir einen Grafikprozessor** verwenden. Gehen Sie dazu auf \"Runtime > Change Runtime type\".\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step1.jpg\" alt=\"GPU Schritt 1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KV0NyFdQM9ZG"
   },
   "source": [
    "- Hardware-Beschleuniger > GPU\".\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step2.jpg\" alt=\"GPU Schritt 2\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bTpYcVZVMzUI"
   },
   "source": [
    "## Erstellen einer virtuellen Anzeige üîΩ.\n",
    "\n",
    "W√§hrend der Arbeit mit dem Notebook m√ºssen wir ein Wiederholungsvideo erstellen. Dazu ben√∂tigen wir mit colab **einen virtuellen Bildschirm, um die Umgebung zu rendern** (und somit die Bilder aufzunehmen).\n",
    "\n",
    "Daher wird die folgende Zelle die Librairies installieren und einen virtuellen Bildschirm erstellen und starten üñ•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jV6wjQ7Be7p5"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!apt install python-opengl\n",
    "!apt install ffmpeg\n",
    "!apt install xvfb\n",
    "!pip3 install pyvirtualdisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ww5PQH1gNLI4"
   },
   "outputs": [],
   "source": [
    "# Virtual display\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "virtual_display = Display(visible=0, size=(1400, 900))\n",
    "virtual_display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e1obkbdJ_KnG"
   },
   "source": [
    "### Abh√§ngigkeiten installieren üîΩ.\n",
    "\n",
    "Der erste Schritt ist die Installation der Abh√§ngigkeiten, wir werden mehrere davon installieren:\n",
    "- `gymnasium`\n",
    "- panda-gym\": Enth√§lt die Umgebungen f√ºr die Roboterarme.\n",
    "- stable-baselines3\": Die SB3-Bibliothek f√ºr Deep Reinforcement Learning.\n",
    "- Umarmungsfl√§che_sb3`: Zus√§tzlicher Code f√ºr Stable-baselines3 zum Laden und Hochladen von Modellen aus dem Hugging Face ü§ó Hub.\n",
    "- huggingface_hub`: Bibliothek, die es jedem erlaubt, mit den Hub-Repositories zu arbeiten.\n",
    "\n",
    "‚è≤ Die Installation kann **10 Minuten** dauern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TgZUkjKYSgvn"
   },
   "outputs": [],
   "source": [
    "!pip install stable-baselines3[extra]\n",
    "!pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ABneW6tOSpyU"
   },
   "outputs": [],
   "source": [
    "!pip install huggingface_sb3\n",
    "!pip install huggingface_hub\n",
    "!pip install panda_gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QTep3PQQABLr"
   },
   "source": [
    "## Importieren Sie die Pakete üì¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HpiB8VdnQ7Bk"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import gymnasium as gym\n",
    "import panda_gym\n",
    "\n",
    "from huggingface_sb3 import load_from_hub, package_to_hub\n",
    "\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lfBwIS_oAVXI"
   },
   "source": [
    "## PandaReachDense-v3 ü¶æ\n",
    "\n",
    "Der Agent, den wir trainieren wollen, ist ein Roboterarm, der Steuerungen durchf√ºhren muss (den Arm bewegen und den Endeffektor benutzen).\n",
    "\n",
    "In der Robotik ist der *Endeffektor* das Ger√§t am Ende eines Roboterarms, das f√ºr die Interaktion mit der Umgebung bestimmt ist.\n",
    "\n",
    "In `PandaReach` muss der Roboter seinen Endeffektor an einer Zielposition (gr√ºner Ball) platzieren.\n",
    "\n",
    "Wir werden die dichte Version dieser Umgebung verwenden. Das bedeutet, dass wir eine *dichte Belohnungsfunktion* erhalten, die **bei jedem Zeitschritt** eine Belohnung bereitstellt (je n√§her der Agent an der Erf√ºllung der Aufgabe ist, desto h√∂her die Belohnung). Im Gegensatz zu einer *sparsamen Belohnungsfunktion*, bei der die Umgebung **eine Belohnung zur√ºckgibt, wenn und nur wenn die Aufgabe erf√ºllt ist**.\n",
    "\n",
    "Au√üerdem verwenden wir die *End-Effektor-Verschiebungssteuerung*, d.h. die **Aktion entspricht der Verschiebung des Endeffektors**. Wir steuern nicht die individuelle Bewegung jedes Gelenks (Gelenksteuerung).\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit8/robotics.jpg\" alt=\"Robotik\"/>\n",
    "\n",
    "\n",
    "Auf diese Weise wird **das Training einfacher**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "frVXOrnlBerQ"
   },
   "source": [
    "### Erstellen Sie die Umgebung\n",
    "\n",
    "#### Die Umgebung üéÆ\n",
    "\n",
    "In `PandaReachDense-v3` muss der Roboterarm seinen Endeffektor an einer Zielposition (gr√ºne Kugel) platzieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zXzAu3HYF1WD"
   },
   "outputs": [],
   "source": [
    "env_id = \"PandaReachDense-v3\"\n",
    "\n",
    "# Create the env\n",
    "env = gym.make(env_id)\n",
    "\n",
    "# Get the state space and action space\n",
    "s_size = env.observation_space.shape\n",
    "a_size = env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E-U9dexcF-FB"
   },
   "outputs": [],
   "source": [
    "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
    "print(\"The State Space is: \", s_size)\n",
    "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_JClfElGFnF"
   },
   "source": [
    "Der Beobachtungsraum **ist ein W√∂rterbuch mit 3 verschiedenen Elementen**:\n",
    "- `erreichtes_Ziel`: (x,y,z) Position des Ziels.\n",
    "- angestrebtes_Ziel`: (x,y,z) Abstand zwischen der Zielposition und der aktuellen Objektposition.\n",
    "- Beobachtung\": Position (x,y,z) und Geschwindigkeit des Endeffektors (vx, vy, vz).\n",
    "\n",
    "Da es sich bei der Beobachtung um ein W√∂rterbuch handelt, **m√ºssen wir eine MultiInputPolicy-Richtlinie anstelle von MlpPolicy** verwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ib1Kxy4AF-FC"
   },
   "outputs": [],
   "source": [
    "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
    "print(\"The Action Space is: \", a_size)\n",
    "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5MHTHEHZS4yp"
   },
   "source": [
    "Der Aktionsraum ist ein Vektor mit 3 Werten:\n",
    "- Steuerung x, y, z Bewegung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S5sXcg469ysB"
   },
   "source": [
    "### Beobachtung und Belohnung normalisieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ZyX6qf3Zva9"
   },
   "source": [
    "Eine gute Praxis beim Reinforcement Learning ist die [Normalisierung der Input-Features] (https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html).\n",
    "\n",
    "Zu diesem Zweck gibt es einen Wrapper, der einen laufenden Durchschnitt und eine Standardabweichung der Eingangsmerkmale berechnet.\n",
    "\n",
    "Wir normalisieren auch die Belohnungen mit demselben Wrapper, indem wir `norm_reward = True` hinzuf√ºgen\n",
    "\n",
    "[Sie sollten die Dokumentation lesen, um diese Zelle zu f√ºllen](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecnormalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1RsDtHHAQ9Ie"
   },
   "outputs": [],
   "source": [
    "env = make_vec_env(env_id, n_envs=4)\n",
    "\n",
    "# Adding this wrapper to normalize the observation and the reward\n",
    "env = # TODO: Add the wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tF42HvI7-gs5"
   },
   "source": [
    "#### L√∂sung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2O67mqgC-hol"
   },
   "outputs": [],
   "source": [
    "env = make_vec_env(env_id, n_envs=4)\n",
    "\n",
    "env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JmEVU6z1ZA-"
   },
   "source": [
    "### Erstellen des A2C-Modells ü§ñ\n",
    "\n",
    "Weitere Informationen zur A2C-Implementierung mit StableBaselines3 finden Sie unter: https://stable-baselines3.readthedocs.io/en/master/modules/a2c.html#notes\n",
    "\n",
    "Um die besten Parameter zu finden, habe ich die [offiziellen geschulten Agenten des Stable-Baselines3-Teams] (https://huggingface.co/sb3) gepr√ºft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vR3T4qFt164I"
   },
   "outputs": [],
   "source": [
    "model = # Create the A2C model and try to find the best parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nWAuOOLh-oQf"
   },
   "source": [
    "#### L√∂sung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FKFLY54T-pU1"
   },
   "outputs": [],
   "source": [
    "model = A2C(policy = \"MultiInputPolicy\",\n",
    "            env = env,\n",
    "            verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "opyK3mpJ1-m9"
   },
   "source": [
    "### Trainieren Sie den A2C-Agenten üèÉ\n",
    "- Lassen Sie uns unseren Agenten f√ºr 1.000.000 Zeitschritte trainieren, vergessen Sie nicht, die GPU auf Colab zu verwenden. Es wird ungef√§hr ~25-40min dauern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4TuGHZD7RF1G"
   },
   "outputs": [],
   "source": [
    "model.learn(1_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MfYtjj19cKFr"
   },
   "outputs": [],
   "source": [
    "# Save the model and  VecNormalize statistics when saving the agent\n",
    "model.save(\"a2c-PandaReachDense-v3\")\n",
    "env.save(\"vec_normalize.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "01M9GCd32Ig-"
   },
   "source": [
    "### Bewerten Sie den Agenten üìà\n",
    "- Nun, da unser Agent trainiert ist, m√ºssen wir seine Leistung **pr√ºfen**.\n",
    "- Stable-Baselines3 bietet daf√ºr eine Methode: `evaluate_policy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "liirTVoDkHq3"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "\n",
    "# Load the saved statistics\n",
    "eval_env = DummyVecEnv([lambda: gym.make(\"PandaReachDense-v3\")])\n",
    "eval_env = VecNormalize.load(\"vec_normalize.pkl\", eval_env)\n",
    "\n",
    "# We need to override the render_mode\n",
    "eval_env.render_mode = \"rgb_array\"\n",
    "\n",
    "#  do not update them at test time\n",
    "eval_env.training = False\n",
    "# reward normalization is not needed at test time\n",
    "eval_env.norm_reward = False\n",
    "\n",
    "# Load the agent\n",
    "model = A2C.load(\"a2c-PandaReachDense-v3\")\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env)\n",
    "\n",
    "print(f\"Mean reward = {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "44L9LVQaavR8"
   },
   "source": [
    "### Ver√∂ffentlichen Sie Ihr trainiertes Modell auf dem Hub üî•\n",
    "Da wir nun gesehen haben, dass wir nach dem Training gute Ergebnisse erzielt haben, k√∂nnen wir unser trainiertes Modell mit einer Zeile Code auf dem Hub ver√∂ffentlichen.\n",
    "\n",
    "üìö Die Dokumentation der Bibliotheken üëâ https://github.com/huggingface/huggingface_sb3/tree/main#hugging-face--x-stable-baselines3-v20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MkMk99m8bgaQ"
   },
   "source": [
    "Durch die Verwendung von `package_to_hub`, wie wir bereits in den fr√ºheren Einheiten erw√§hnt haben, **werten Sie aus, zeichnen eine Wiedergabe auf, erzeugen eine Modellkarte Ihres Agenten und schieben sie zum Hub**.\n",
    "\n",
    "This way:\n",
    "- Du kannst **unsere Arbeit vorf√ºhren** üî•.\n",
    "- Sie k√∂nnen **Ihren Agenten beim Spielen visualisieren** üëÄ\n",
    "- Du kannst **einen Agenten mit der Community teilen, den andere benutzen k√∂nnen** üíæ\n",
    "- Sie k√∂nnen **auf eine Bestenliste üèÜ zugreifen, um zu sehen, wie gut Ihr Agent im Vergleich zu Ihren Klassenkameraden abschneidet** üëâ https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JquRrWytA6eo"
   },
   "source": [
    "Um Ihr Modell mit der Gemeinschaft teilen zu k√∂nnen, sind drei weitere Schritte erforderlich:\n",
    "\n",
    "1Ô∏è‚É£ (Falls noch nicht geschehen) Erstellen Sie ein Konto f√ºr HF ‚û° https://huggingface.co/join\n",
    "\n",
    "2Ô∏è‚É£ Melde dich an und speichere dann dein Authentifizierungs-Token von der Hugging Face Website.\n",
    "- Erstellen Sie ein neues Token (https://huggingface.co/settings/tokens) **mit Schreibrolle**\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/create-token.jpg\" alt=\"HF-Token erstellen\">\n",
    "\n",
    "- Kopieren Sie das Token\n",
    "- F√ºhren Sie die Zelle unten aus und f√ºgen Sie das Token ein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GZiFBBlzxzxY"
   },
   "outputs": [],
   "source": [
    "notebook_login()\n",
    "!git config --global credential.helper store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_tsf2uv0g_4p"
   },
   "source": [
    "Wenn Sie kein Google Colab oder ein Jupyter Notebook verwenden m√∂chten, m√ºssen Sie stattdessen diesen Befehl verwenden: `huggingface-cli login`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FGNh9VsZok0i"
   },
   "source": [
    "3Ô∏è‚É£ Wir sind jetzt bereit, unseren trainierten Agenten mit der Funktion `package_to_hub()` an den ü§ó Hub üî• zu √ºbertragen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "juxItTNf1W74"
   },
   "source": [
    "In dieser Umgebung kann die **Ausf√ºhrung dieser Zelle etwa 10 Minuten dauern**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V1N8r8QVwcCE"
   },
   "outputs": [],
   "source": [
    "from huggingface_sb3 import package_to_hub\n",
    "\n",
    "package_to_hub(\n",
    "    model=model,\n",
    "    model_name=f\"a2c-{env_id}\",\n",
    "    model_architecture=\"A2C\",\n",
    "    env_id=env_id,\n",
    "    eval_env=eval_env,\n",
    "    repo_id=f\"ThomasSimonini/a2c-{env_id}\", # Change the username\n",
    "    commit_message=\"Initial commit\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G3xy3Nf3c2O1"
   },
   "source": [
    "## Einige zus√§tzliche Herausforderungen üèÜ\n",
    "Der beste Weg zu lernen **ist, Dinge selbst auszuprobieren**! Warum nicht `PandaPickAndPlace-v3` ausprobieren?\n",
    "\n",
    "Wenn du fortgeschrittenere Aufgaben f√ºr panda-gym ausprobieren willst, musst du pr√ºfen, was mit **TQC oder SAC** (einem f√ºr Robotikaufgaben geeigneten, stichprobenartigeren Algorithmus) gemacht wurde. In der realen Robotik werden Sie aus einem einfachen Grund einen Algorithmus verwenden, der effizienter ist: Im Gegensatz zu einer Simulation **wenn Sie Ihren Roboterarm zu stark bewegen, besteht die Gefahr, dass er kaputt geht**.\n",
    "\n",
    "PandaPickAndPlace-v1 (dieses Modell verwendet die v1-Version der Umgebung): https://huggingface.co/sb3/tqc-PandaPickAndPlace-v1\n",
    "\n",
    "Und z√∂gern Sie nicht, die Panda-Gym-Dokumentation hier zu lesen: https://panda-gym.readthedocs.io/en/latest/usage/train_with_sb3.html\n",
    "\n",
    "Wir stellen Ihnen die Schritte zum Trainieren eines weiteren Agenten zur Verf√ºgung (optional):\n",
    "\n",
    "1. Definieren Sie eine Umgebung namens \"PandaPickAndPlace-v3\".\n",
    "2. Erstellen Sie eine vektorisierte Umgebung\n",
    "3. F√ºge einen Wrapper hinzu, um die Beobachtungen und Belohnungen zu normalisieren. [Pr√ºfen Sie die Dokumentation](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecnormalize)\n",
    "4. Erstellen Sie das A2C-Modell (vergessen Sie nicht verbose=1, um die Trainingsprotokolle zu drucken).\n",
    "5. Trainieren Sie es f√ºr 1M Timesteps\n",
    "6. Speichern Sie das Modell und VecNormalize-Statistiken beim Speichern des Agenten\n",
    "7. Beurteilen Sie Ihren Agenten\n",
    "8. Ver√∂ffentliche dein trainiertes Modell auf dem Hub üî• mit `package_to_hub`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKGbFXZq9ikN"
   },
   "source": [
    "### L√∂sung (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J-cC-Feg9iMm"
   },
   "outputs": [],
   "source": [
    "# 1 - 2\n",
    "env_id = \"PandaPickAndPlace-v3\"\n",
    "env = make_vec_env(env_id, n_envs=4)\n",
    "\n",
    "# 3\n",
    "env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.)\n",
    "\n",
    "# 4\n",
    "model = A2C(policy = \"MultiInputPolicy\",\n",
    "            env = env,\n",
    "            verbose=1)\n",
    "# 5\n",
    "model.learn(1_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-UnlKLmpg80p"
   },
   "outputs": [],
   "source": [
    "# 6\n",
    "model_name = \"a2c-PandaPickAndPlace-v3\";\n",
    "model.save(model_name)\n",
    "env.save(\"vec_normalize.pkl\")\n",
    "\n",
    "# 7\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "\n",
    "# Load the saved statistics\n",
    "eval_env = DummyVecEnv([lambda: gym.make(\"PandaPickAndPlace-v3\")])\n",
    "eval_env = VecNormalize.load(\"vec_normalize.pkl\", eval_env)\n",
    "\n",
    "#  do not update them at test time\n",
    "eval_env.training = False\n",
    "# reward normalization is not needed at test time\n",
    "eval_env.norm_reward = False\n",
    "\n",
    "# Load the agent\n",
    "model = A2C.load(model_name)\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env)\n",
    "\n",
    "print(f\"Mean reward = {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "# 8\n",
    "package_to_hub(\n",
    "    model=model,\n",
    "    model_name=f\"a2c-{env_id}\",\n",
    "    model_architecture=\"A2C\",\n",
    "    env_id=env_id,\n",
    "    eval_env=eval_env,\n",
    "    repo_id=f\"ThomasSimonini/a2c-{env_id}\", # TODO: Change the username\n",
    "    commit_message=\"Initial commit\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "usatLaZ8dM4P"
   },
   "source": [
    "Wir sehen uns in Referat 7! üî•\n",
    "## Lernt weiter, bleibt fantastisch ü§ó"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "tF42HvI7-gs5"
   ],
   "include_colab_link": true,
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
