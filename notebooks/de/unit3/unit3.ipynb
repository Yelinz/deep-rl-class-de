{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7xBVPzoXxOg"
   },
   "source": [
    "# Einheit 3: Tiefes Q-Learning mit Atari-Spielen üëæ mit RL Baselines3 Zoo\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/thumbnail.jpg\" alt=\"Unit 3 Thumbnail\">\n",
    "\n",
    "In diesem Notizbuch **trainieren Sie einen Deep Q-Learning-Agenten**, der Space Invaders spielt. Dazu verwenden Sie [RL Baselines3 Zoo] (https://github.com/DLR-RM/rl-baselines3-zoo), ein auf [Stable-Baselines3] (https://stable-baselines3.readthedocs.io/en/master/) basierendes Trainingsframework, das Skripte f√ºr das Training, die Auswertung von Agenten, die Abstimmung von Hyperparametern, die Darstellung von Ergebnissen und die Aufnahme von Videos bereitstellt.\n",
    "\n",
    "Wir verwenden die [RL-Baselines-3 Zoo Integration, eine Vanilla-Version von Deep Q-Learning](https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html) ohne Erweiterungen wie Double-DQN, Dueling-DQN und Prioritized Experience Replay.\n",
    "\n",
    "‚¨áÔ∏è Hier ein Beispiel daf√ºr, was **Sie erreichen werden** ‚¨áÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J9S713biXntc"
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<video controls autoplay><source src=\"https://huggingface.co/ThomasSimonini/ppo-SpaceInvadersNoFrameskip-v4/resolve/main/replay.mp4\" type=\"video/mp4\"></video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ykJiGevCMVc5"
   },
   "source": [
    "### üéÆ Umgebungen:\n",
    "\n",
    "- [SpacesInvadersNoFrameskip-v4](https://gymnasium.farama.org/environments/atari/space_invaders/)\n",
    "\n",
    "Sie k√∂nnen den Unterschied zwischen den Space Invaders Versionen hier sehen üëâ https://gymnasium.farama.org/environments/atari/space_invaders/#variants\n",
    "\n",
    "### üìö RL-Library:\n",
    "\n",
    "- [RL-Baselines3-Zoo](https://github.com/DLR-RM/rl-baselines3-zoo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wciHGjrFYz9m"
   },
   "source": [
    "## Ziele dieses Notizbuchs üèÜ\n",
    "Am Ende des Heftes wirst du:\n",
    "- In der Lage sein, tiefer zu verstehen **wie RL Baselines3 Zoo funktioniert**.\n",
    "- In der Lage sein, **Ihren trainierten Agenten und den Code in den Hub** mit einer sch√∂nen Videowiedergabe und einem Bewertungsergebnis zu pushen üî•.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TsnP0rjxMn1e"
   },
   "source": [
    "## Dieses Notebook stammt aus dem Deep Reinforcement Learning Kurs.\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/deep-rl-course-illustration.jpg\" alt=\"Deep RL Course illustration\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nw6fJHIAZd-J"
   },
   "source": [
    "In diesem kostenlosen Kurs lernen Sie:\n",
    "\n",
    "- üìñ Deep Reinforcement Learning in **Theorie und Praxis** studieren.\n",
    "- üßë‚Äçüíª Lernen Sie, **ber√ºhmte Deep RL-Bibliotheken** wie Stable Baselines3, RL Baselines3 Zoo, CleanRL und Sample Factory 2.0 zu verwenden.\n",
    "- ü§ñ Trainieren Sie **Agenten in einzigartigen Umgebungen**.\n",
    "\n",
    "Und mehr, siehe üìö den Lehrplan üëâ https://simoninithomas.github.io/deep-rl-course\n",
    "\n",
    "Vergessen Sie nicht, sich **<a href=\"http://eepurl.com/ic5ZUD\">f√ºr den Kurs anzumelden</a>** (wir sammeln Ihre E-Mail, um Ihnen **die Links zu senden, wenn die einzelnen Einheiten ver√∂ffentlicht werden, und Sie √ºber die Herausforderungen und Aktualisierungen zu informieren).**\n",
    "\n",
    "\n",
    "Der beste Weg, um in Kontakt zu bleiben, ist, unserem Discord-Server beizutreten, um sich mit der Community und mit uns auszutauschen üëâüèª https://discord.gg/ydHrjt3WP5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wS_cVefO-aYg"
   },
   "source": [
    "# RL-Baselines3 Zoo und seine Abh√§ngigkeiten installieren üìö.\n",
    "\n",
    "Wenn Sie \"ERROR: pip's dependency resolver does not currently take into account all the packages that are installed\" sehen, **das ist normal und kein kritischer Fehler**, gibt es einen Versionskonflikt. Aber die Pakete, die wir brauchen, sind installiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hLTwHqIWdnPb"
   },
   "outputs": [],
   "source": [
    "# For now we install this update of RL-Baselines3 Zoo\n",
    "!pip install git+https://github.com/DLR-RM/rl-baselines3-zoo@update/hf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p0xe2sJHdtHy"
   },
   "source": [
    "WENN UND NUR WENN DIE OBIGE VERSION NICHT MEHR EXISTIERT. ENTKOMMENTIEREN UND INSTALLIEREN SIE DIE UNTEN STEHENDE VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N0d6wy-F-f39"
   },
   "outputs": [],
   "source": [
    "#!pip install rl_zoo3==2.0.0a9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8_MllY6Om1eI"
   },
   "outputs": [],
   "source": [
    "!apt-get install swig cmake ffmpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4S9mJiKg6SqC"
   },
   "source": [
    "Um Atari-Spiele in Gymnasium verwenden zu k√∂nnen, m√ºssen wir das Atari-Paket installieren. Und accept-rom-license, um die Rom-Dateien (Spiele-Dateien) herunterzuladen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NsRP-lX1_2fC"
   },
   "outputs": [],
   "source": [
    "!pip install gymnasium[atari]\n",
    "!pip install gymnasium[accept-rom-license]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bTpYcVZVMzUI"
   },
   "source": [
    "## Erstellen einer virtuellen Anzeige üîΩ.\n",
    "\n",
    "W√§hrend der Arbeit mit dem Notebook m√ºssen wir ein Wiederholungsvideo erstellen. Dazu ben√∂tigen wir mit colab **einen virtuellen Bildschirm, um die Umgebung zu rendern** (und somit die Bilder aufzunehmen).\n",
    "\n",
    "Daher wird die folgende Zelle die Librairies installieren und einen virtuellen Bildschirm erstellen und starten üñ•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jV6wjQ7Be7p5"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!apt install python-opengl\n",
    "!apt install xvfb\n",
    "!pip3 install pyvirtualdisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BE5JWP5rQIKf"
   },
   "outputs": [],
   "source": [
    "# Virtual display\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "virtual_display = Display(visible=0, size=(1400, 900))\n",
    "virtual_display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Von Q-Learning zu Deep Q-Learning\n",
    "\n",
    "Wir haben gelernt, dass **Q-Learning ein Algorithmus ist, mit dem wir unsere Q-Funktion** trainieren, eine **Aktionswertfunktion**, die den Wert eines bestimmten Zustands und einer bestimmten Aktion in diesem Zustand bestimmt.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-function.jpg\" alt=\"Q-function\"/>\n",
    "</figure>\n",
    "\n",
    "Das **Q kommt von \"der Qualit√§t\" dieser Handlung in diesem Zustand.**\n",
    "\n",
    "Intern wird unsere Q-Funktion durch eine **Q-Tabelle kodiert, eine Tabelle, in der jede Zelle einem Wert eines Zustands-Aktionspaares entspricht.** Man kann sich diese Q-Tabelle als **den Speicher oder Spickzettel unserer Q-Funktion vorstellen.**\n",
    "\n",
    "Das Problem ist, dass Q-Learning eine *tabellarische Methode* ist. Dies wird zu einem Problem, wenn die Zust√§nde und Aktionsr√§ume **nicht klein genug sind, um effizient durch Arrays und Tabellen dargestellt zu werden**. Mit anderen Worten: es ist **nicht skalierbar**.\n",
    "Q-Learning funktionierte gut mit kleinen Zustandsr√§umen wie:\n",
    "\n",
    "- FrozenLake, wir hatten 16 Zust√§nde.\n",
    "- Taxi-v3, wir hatten 500 Zust√§nde.\n",
    "\n",
    "Aber stellen Sie sich vor, was wir heute tun werden: Wir werden einen Agenten trainieren, damit er lernt, Space Invaders zu spielen, ein komplexeres Spiel, bei dem die Frames als Input dienen.\n",
    "\n",
    "Wie **[Nikita Melkozerov] (https://twitter.com/meln1k) erw√§hnte, haben Atari-Umgebungen** einen Beobachtungsraum mit der Form (210, 160, 3)*, der Werte von 0 bis 255 enth√§lt, so dass wir \\\\(256^{210 \\times 160 \\times 3} = 256^{100800}\\\\) m√∂gliche Beobachtungen haben (zum Vergleich: wir haben ungef√§hr \\\\\\(10^{80}\\) Atome im beobachtbaren Universum).\n",
    "\n",
    "* Ein Einzelbild in Atari besteht aus einem Bild von 210x160 Pixeln. Da die Bilder in Farbe (RGB) sind, gibt es 3 Kan√§le. Aus diesem Grund ist die Form (210, 160, 3). F√ºr jedes Pixel kann der Wert von 0 bis 255 gehen.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/atari.jpg\" alt=\"Atari State Space\"/>\n",
    "\n",
    "Der Zustandsraum ist also gigantisch; aus diesem Grund w√§re das Erstellen und Aktualisieren einer Q-Tabelle f√ºr diese Umgebung nicht effizient. In diesem Fall ist die beste Idee, die Q-Werte mit Hilfe einer parametrisierten Q-Funktion \\\\(Q_{\\theta}(s,a)\\\\) zu approximieren.\n",
    "\n",
    "Dieses neuronale Netz approximiert f√ºr einen Zustand die verschiedenen Q-Werte f√ºr jede m√∂gliche Aktion in diesem Zustand. Und das ist genau das, was Deep Q-Learning tut.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/deep.jpg\" alt=\"Deep Q-Learning\"/>\n",
    "\n",
    "\n",
    "Nachdem wir nun Deep Q-Learning verstanden haben, wollen wir nun tiefer in das Deep Q-Network eintauchen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Das Deep Q-Network (DQN)\n",
    "Dies ist die Architektur unseres Deep Q-Learning-Netzwerks:\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/deep-q-network.jpg\" alt=\"Deep Q Network\"/>\n",
    "\n",
    "Als Eingabe nehmen wir einen **Stapel von 4 Einzelbildern**, der als Zustand durch das Netz l√§uft, und geben einen **Vektor von Q-Werten f√ºr jede m√∂gliche Aktion in diesem Zustand** aus. Dann m√ºssen wir, wie beim Q-Learning, nur noch unsere Epsilon-Greedy-Strategie verwenden, um die zu ergreifende Aktion auszuw√§hlen.\n",
    "\n",
    "Bei der Initialisierung des neuronalen Netzes ist **die Sch√§tzung der Q-Werte miserabel**. Aber w√§hrend des Trainings wird unser Deep Q-Network-Agent eine Situation mit der passenden Aktion assoziieren und **erlernen, das Spiel gut zu spielen**.\n",
    "\n",
    "## Vorverarbeitung der Eingabe und zeitliche Begrenzung\n",
    "\n",
    "Wir m√ºssen die Eingabe **vorverarbeiten**. Dies ist ein wesentlicher Schritt, da wir die Komplexit√§t unseres Zustands **reduzieren wollen, um die f√ºr das Training ben√∂tigte Rechenzeit zu verringern**.\n",
    "\n",
    "Um dies zu erreichen, **reduzieren wir den Zustandsraum auf 84x84 und graustufen ihn**. Das k√∂nnen wir tun, da die Farben in Atari-Umgebungen keine wichtigen Informationen hinzuf√ºgen.\n",
    "Dies ist eine gro√üe Verbesserung, da wir **unsere drei Farbkan√§le (RGB) auf 1 reduzieren**.\n",
    "\n",
    "In einigen Spielen k√∂nnen wir auch **einen Teil des Bildschirms abschneiden**, wenn er keine wichtigen Informationen enth√§lt.\n",
    "Dann stapeln wir vier Bilder zusammen.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/preprocessing.jpg\" alt=\"Preprocessing\"/>\n",
    "\n",
    "**Warum stapeln wir vier Frames zusammen?**\n",
    "Wir stapeln Bilder zusammen, weil es uns hilft, **das Problem der zeitlichen Begrenzung** zu l√∂sen. Nehmen wir ein Beispiel mit dem Spiel Pong. Wenn Sie dieses Bild sehen:\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/temporal-limitation.jpg\" alt=\"Zeitliche Begrenzung\"/>\n",
    "\n",
    "K√∂nnen Sie mir sagen, wohin der Ball fliegt?\n",
    "Nein, denn ein Bild reicht nicht aus, um ein Gef√ºhl f√ºr die Bewegung zu bekommen! Aber was ist, wenn ich drei weitere Bilder hinzuf√ºge? **Hier kannst du sehen, dass der Ball nach rechts geht**.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/temporal-limitation-2.jpg\" alt=\"Zeitliche Begrenzung\"/>\n",
    "Um die zeitlichen Informationen zu erfassen, werden daher vier Bilder √ºbereinander gelegt.\n",
    "\n",
    "Dann werden die gestapelten Bilder von drei Faltungsschichten verarbeitet. Diese Schichten **erm√∂glichen es uns, r√§umliche Beziehungen in Bildern zu erfassen und zu nutzen**. Da die Bilder √ºbereinander gestapelt sind, k√∂nnen wir **aber auch einige zeitliche Eigenschaften √ºber diese Bilder hinweg ausnutzen**.\n",
    "\n",
    "Wenn Sie nicht wissen, was Faltungsschichten sind, machen Sie sich keine Sorgen. Sie k√∂nnen sich [Lektion 4 dieses kostenlosen Deep Learning-Kurses von Udacity] ansehen (https://www.udacity.com/course/deep-learning-pytorch--ud188)\n",
    "\n",
    "Schlie√ülich haben wir ein paar vollst√§ndig verbundene Schichten, die einen Q-Wert f√ºr jede m√∂gliche Aktion in diesem Zustand ausgeben.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/deep-q-network.jpg\" alt=\"Deep Q Network\"/>\n",
    "\n",
    "Wir sehen also, dass Deep Q-Learning ein neuronales Netz verwendet, um f√ºr einen Zustand die verschiedenen Q-Werte f√ºr jede m√∂gliche Aktion in diesem Zustand zu approximieren. Schauen wir uns nun den Deep Q-Learning-Algorithmus an.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Der tiefe Q-Learning-Algorithmus\n",
    "\n",
    "Wir haben gelernt, dass Deep Q-Learning **ein tiefes neuronales Netz verwendet, um die verschiedenen Q-Werte f√ºr jede m√∂gliche Aktion in einem Zustand** zu approximieren (Wert-Funktions-Sch√§tzung).\n",
    "\n",
    "Der Unterschied besteht darin, dass w√§hrend der Trainingsphase der Q-Wert eines Zustands-Aktionspaares nicht direkt aktualisiert wird, wie wir es beim Q-Learning getan haben:\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/q-ex-5.jpg\" alt=\"Q Loss\"/>\n",
    "\n",
    "Beim Deep Q-Learning erstellen wir eine **Verlustfunktion, die unsere Q-Wert-Vorhersage mit dem Q-Ziel vergleicht und den Gradientenabstieg verwendet, um die Gewichte unseres Deep Q-Netzes zu aktualisieren, um unsere Q-Werte besser anzun√§hern**.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/Q-target.jpg\" alt=\"Q-Ziel\"/>\n",
    "\n",
    "Der Deep Q-Learning-Trainingsalgorithmus hat *zwei Phasen*:\n",
    "\n",
    "- **Sampling**: Wir f√ºhren Aktionen durch und **speichern die beobachteten Erfahrungstupel in einem Wiedergabespeicher**.\n",
    "- **Training**: Wir w√§hlen einen **kleinen Stapel von Tupeln nach dem Zufallsprinzip aus und lernen aus diesem Stapel mit Hilfe eines Aktualisierungsschritts nach dem Gradientenabstieg**.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/sampling-training.jpg\" alt=\"Sampling Training\"/>\n",
    "\n",
    "Dies ist nicht der einzige Unterschied zum Q-Learning. Das tiefe Q-Learning-Training **kann unter Instabilit√§t leiden**, vor allem wegen der Kombination einer nichtlinearen Q-Wert-Funktion (Neuronales Netz) und Bootstrapping (wenn wir Ziele mit vorhandenen Sch√§tzungen und nicht mit einer tats√§chlichen vollst√§ndigen R√ºckkehr aktualisieren).\n",
    "\n",
    "Um das Training zu stabilisieren, implementieren wir drei verschiedene L√∂sungen:\n",
    "1. *Experience Replay* f√ºr eine **effizientere Nutzung von Erfahrungen**.\n",
    "2. *Fixed Q-Target* **zur Stabilisierung des Trainings**.\n",
    "3. *Double Deep Q-Learning*, um **das Problem der √úbersch√§tzung von Q-Werten** zu l√∂sen.\n",
    "\n",
    "Gehen wir sie durch!\n",
    "\n",
    "\n",
    "\n",
    "## Erfahrungswiederholung f√ºr eine effizientere Nutzung von Erfahrungen\n",
    "\n",
    "Warum erstellen wir einen Wiedergabespeicher?\n",
    "\n",
    "Die Erfahrungswiederholung beim Deep Q-Learning hat zwei Funktionen:\n",
    "\n",
    "1. **Eine effizientere Nutzung der Erfahrungen w√§hrend des Trainings**.\n",
    "Normalerweise interagiert der Agent beim Online-Verst√§rkungslernen mit der Umgebung, sammelt Erfahrungen (Zustand, Aktion, Belohnung und n√§chster Zustand), lernt aus ihnen (aktualisiert das neuronale Netz) und verwirft sie. Dies ist nicht effizient.\n",
    "\n",
    "Die Erfahrungswiederholung hilft, indem sie **die Erfahrungen aus dem Training effizienter nutzt**. Wir verwenden einen Wiederholungspuffer, der Erfahrungsproben speichert, **die wir w√§hrend des Trainings wiederverwenden k√∂nnen**.\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/experience-replay.jpg\" alt=\"Erfahrungswiederholung\"/>\n",
    "\n",
    "‚áí Dies erm√∂glicht es dem Agenten, **mehrmals aus denselben Erfahrungen zu lernen**.\n",
    "\n",
    "2. **Vermeiden Sie das Vergessen fr√ºherer Erfahrungen (auch bekannt als katastrophale Interferenz oder katastrophales Vergessen) und reduzieren Sie die Korrelation zwischen Erfahrungen**.\n",
    "- **[katastrophales Vergessen](https://en.wikipedia.org/wiki/Catastrophic_interference)**: Das Problem, das sich ergibt, wenn wir unserem neuronalen Netz aufeinanderfolgende Muster von Erfahrungen geben, ist, dass es dazu neigt, **die vorherigen Erfahrungen zu vergessen, wenn es neue Erfahrungen macht**. Wenn der Agent beispielsweise in der ersten Ebene ist und dann in der zweiten, die anders ist, kann er vergessen, wie er sich in der ersten Ebene verhalten und spielen soll.\n",
    "\n",
    "Die L√∂sung besteht darin, einen Wiederholungspuffer zu erstellen, der Erfahrungstupel speichert, w√§hrend er mit der Umgebung interagiert, und dann eine kleine Menge von Tupeln abzufragen. Dadurch wird verhindert, dass **das Netzwerk nur √ºber das lernt, was es unmittelbar zuvor getan hat.**\n",
    "\n",
    "Die Wiederholung von Erfahrungen hat noch weitere Vorteile. Indem wir die Erfahrungen nach dem Zufallsprinzip abfragen, beseitigen wir Korrelationen in den Beobachtungssequenzen und verhindern, dass **Aktionswerte oszillieren oder in katastrophaler Weise divergieren.**\n",
    "\n",
    "Im Pseudocode von Deep Q-Learning **initialisieren wir einen Wiedergabespeicher D mit der Kapazit√§t N** (N ist ein Hyperparameter, den Sie definieren k√∂nnen). Wir speichern dann Erfahrungen im Speicher und nehmen eine Reihe von Erfahrungen auf, um das Deep Q-Netz w√§hrend der Trainingsphase zu f√ºttern.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/experience-replay-pseudocode.jpg\" alt=\"Experience Replay Pseudocode\"/>\n",
    "\n",
    "## Festes Q-Ziel zur Stabilisierung des Trainings \n",
    "\n",
    "Wenn wir den TD-Fehler (auch bekannt als Verlust) berechnen wollen, berechnen wir die **Differenz zwischen dem TD-Ziel (Q-Ziel) und dem aktuellen Q-Wert (Sch√§tzung von Q)**.\n",
    "\n",
    "Aber wir haben **keine Ahnung vom tats√§chlichen TD-Ziel**. Wir m√ºssen es sch√§tzen. Anhand der Bellman-Gleichung haben wir gesehen, dass das TD-Ziel nur die Belohnung f√ºr die Durchf√ºhrung dieser Aktion in diesem Zustand plus den abgezinsten h√∂chsten Q-Wert f√ºr den n√§chsten Zustand ist.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/Q-target.jpg\" alt=\"Q-Ziel\"/>\n",
    "\n",
    "Das Problem ist jedoch, dass wir dieselben Parameter (Gewichte) f√ºr die Sch√§tzung des TD-Ziels **und** des Q-Werts verwenden. Folglich gibt es eine signifikante Korrelation zwischen dem TD-Ziel und den Parametern, die wir √§ndern.\n",
    "\n",
    "Daher verschieben sich bei jedem Trainingsschritt **sowohl unsere Q-Werte als auch die Zielwerte.** Wir kommen unserem Ziel n√§her, aber das Ziel bewegt sich auch. Es ist, als w√ºrden wir einem beweglichen Ziel hinterherjagen! Dies kann zu erheblichen Schwankungen im Training f√ºhren.\n",
    "\n",
    "Es ist so, als w√§ren Sie ein Cowboy (die Q-Sch√§tzung) und wollten eine Kuh fangen (das Q-Ziel). Ihr Ziel ist es, n√§her heranzukommen (den Fehler zu verringern).\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/qtarget-1.jpg\" alt=\"Q-Ziel\"/>\n",
    "\n",
    "In jedem Zeitschritt versuchst du, dich der Kuh zu n√§hern, die sich ebenfalls in jedem Zeitschritt bewegt (weil du die gleichen Parameter verwendest).\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/qtarget-2.jpg\" alt=\"Q-Ziel\"/>\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/qtarget-3.jpg\" alt=\"Q-Ziel\"/>\n",
    "Dies f√ºhrt zu einem bizarren Pfad der Verfolgung (eine signifikante Oszillation im Training).\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/qtarget-4.jpg\" alt=\"Q-Ziel\"/>\n",
    "\n",
    "Stattdessen sehen wir im Pseudocode, dass wir:\n",
    "- Verwendung eines **separaten Netzwerks mit festen Parametern** zur Sch√§tzung des TD-Targets\n",
    "- **Kopieren Sie die Parameter aus unserem tiefen Q-Netz alle C-Schritte**, um das Zielnetz zu aktualisieren.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/fixed-q-target-pseudocode.jpg\" alt=\"Fixed Q-target Pseudocode\"/>\n",
    "\n",
    "\n",
    "\n",
    "## Doppeltes DQN \n",
    "Double DQNs, oder Double Deep Q-Learning neuronale Netze, wurden [von Hado van Hasselt](https://papers.nips.cc/paper/3964-double-q-learning) eingef√ºhrt. Diese Methode **behandelt das Problem der √úbersch√§tzung der Q-Werte**.\n",
    "\n",
    "Um dieses Problem zu verstehen, erinnern wir uns daran, wie wir das TD Target berechnen:\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/TD-1.jpg\" alt=\"TD target\"/>\n",
    "\n",
    "Bei der Berechnung des TD-Ziels stehen wir vor einem einfachen Problem: Wie k√∂nnen wir sicher sein, dass **die beste Aktion f√ºr den n√§chsten Zustand die Aktion mit dem h√∂chsten Q-Wert ist?**\n",
    "\n",
    "Wir wissen, dass die Genauigkeit der Q-Werte davon abh√§ngt, welche Aktion wir ausprobiert haben **und** welche benachbarten Zust√§nde wir erkundet haben.\n",
    "\n",
    "Folglich haben wir zu Beginn des Trainings nicht gen√ºgend Informationen √ºber die beste Aktion, die wir durchf√ºhren sollten. Daher kann die Annahme des maximalen Q-Werts (der verrauscht ist) als beste Aktion zu falsch positiven Ergebnissen f√ºhren. Wenn nicht-optimalen Aktionen regelm√§√üig **ein h√∂herer Q-Wert als der optimalen besten Aktion zugewiesen wird, wird das Lernen kompliziert**.\n",
    "\n",
    "Die L√∂sung lautet: Bei der Berechnung des Q-Ziels verwenden wir zwei Netze, um die Aktionsauswahl von der Erzeugung des Q-Zielwerts zu entkoppeln. Wir:\n",
    "- Wir verwenden unser **DQN-Netz**, um die beste Aktion f√ºr den n√§chsten Zustand zu w√§hlen (die Aktion mit dem h√∂chsten Q-Wert).\n",
    "- Wir verwenden unser **Zielnetz**, um den Ziel-Q-Wert f√ºr die Durchf√ºhrung dieser Aktion im n√§chsten Zustand zu berechnen.\n",
    "\n",
    "Daher hilft uns Double DQN dabei, die √úbersch√§tzung der Q-Werte zu reduzieren und folglich schneller und stabiler zu lernen.\n",
    "\n",
    "Seit diesen drei Verbesserungen beim Deep Q-Learning sind viele weitere hinzugekommen, wie z. B. Prioritized Experience Replay und Dueling Deep Q-Learning. Sie liegen au√üerhalb des Rahmens dieses Kurses, aber wenn Sie daran interessiert sind, lesen Sie die Links in der Leseliste.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0vgANIBBZg1p"
   },
   "source": [
    "## Voraussetzungen üèóÔ∏è\n",
    "Bevor Sie sich mit dem Notebook besch√§ftigen, m√ºssen Sie:\n",
    "\n",
    "üî≤ üìö **[Deep Q-Learning durch Lesen von Einheit 3](https://huggingface.co/deep-rl-course/unit3/introduction)** ü§ó"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7kszpGFaRVhq"
   },
   "source": [
    "Wir versuchen st√§ndig, unsere Anleitungen zu verbessern. **Wenn Sie also Probleme in diesem Notizbuch** finden, √∂ffnen Sie bitte [ein Problem im Github Repo](https://github.com/huggingface/deep-rl-class/issues)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QR0jZtYreSI5"
   },
   "source": [
    "# Trainieren wir einen Deep-Q-Learning-Agenten, der Atari' Space Invaders üëæ spielt, und laden wir ihn in den Hub hoch.\n",
    "\n",
    "Wir empfehlen den Sch√ºlerinnen und Sch√ºlern dringend, **Google Colab f√ºr die praktischen √úbungen zu verwenden, anstatt sie auf ihren Computern auszuf√ºhren**.\n",
    "\n",
    "Durch die Verwendung von Google Colab k√∂nnen **Sie sich auf das Lernen und Experimentieren konzentrieren, ohne sich um die technischen Aspekte der Einrichtung Ihrer Umgebungen zu k√ºmmern**.\n",
    "\n",
    "Um diese praktische √úbung f√ºr den Zertifizierungsprozess zu validieren, m√ºssen Sie Ihr trainiertes Modell an den Hub senden und **ein Ergebnis von >= 200** erhalten.\n",
    "\n",
    "Um Ihr Ergebnis zu ermitteln, gehen Sie zum Leaderboard und suchen Sie Ihr Modell, **das Ergebnis = mean_reward - std of reward**\n",
    "\n",
    "Weitere Informationen √ºber den Zertifizierungsprozess finden Sie in diesem Abschnitt üëâ https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5iPgzluo9z-u"
   },
   "source": [
    "# Trainiere unseren Deep Q-Learning Agent, um Space Invaders zu spielen üëæ\n",
    "\n",
    "Um einen Agenten mit RL-Baselines3-Zoo zu trainieren, m√ºssen wir nur zwei Dinge tun:\n",
    "\n",
    "1. Erstellen Sie eine Hyperparameter-Konfigurationsdatei, die unsere Trainings-Hyperparameter mit dem Namen \"dqn.yml\" enthalten wird.\n",
    "\n",
    "Dies ist ein Beispiel f√ºr eine Vorlage:\n",
    "\n",
    "```\n",
    "SpaceInvadersNoFrameskip-v4:\n",
    "  env_wrapper:\n",
    "    - stable_baselines3.common.atari_wrappers.AtariWrapper\n",
    "  frame_stack: 4\n",
    "  policy: 'CnnPolicy'\n",
    "  n_timesteps: !!float 1e7\n",
    "  buffer_size: 100000\n",
    "  learning_rate: !!float 1e-4\n",
    "  batch_size: 32\n",
    "  learning_starts: 100000\n",
    "  target_update_interval: 1000\n",
    "  train_freq: 4\n",
    "  gradient_steps: 1\n",
    "  exploration_fraction: 0.1\n",
    "  exploration_final_eps: 0.01\n",
    "  # If True, you need to deactivate handle_timeout_termination\n",
    "  # in the replay_buffer_kwargs\n",
    "  optimize_memory_usage: False\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VjblFSVDQOj"
   },
   "source": [
    "Hier sehen wir das:\n",
    "- Wir verwenden den \"Atari Wrapper\", der die Eingabe vorverarbeitet (Frame-Reduktion, Graustufen, 4 Frames stapeln)\n",
    "- Wir verwenden `CnnPolicy`, da wir Faltungsschichten zur Verarbeitung der Frames verwenden\n",
    "- Wir trainieren es f√ºr 10 Millionen `n_Zeitschritte`\n",
    "- Die Gr√∂√üe des Speichers (Experience Replay) betr√§gt 100000, d.h. die Anzahl der Erfahrungsschritte, die Sie gespeichert haben, um Ihren Agenten erneut zu trainieren.\n",
    "\n",
    "üí° Mein Rat ist, **die Trainingszeitschritte auf 1M zu reduzieren,** was auf einem P100 etwa 90 Minuten dauern wird. !nvidia-smi\" wird Ihnen sagen, welche GPU Sie verwenden. Bei 10 Millionen Schritten wird dies etwa 9 Stunden dauern, was wahrscheinlich zu einer Zeit√ºberschreitung von Colab f√ºhren k√∂nnte. Ich empfehle, dies auf Ihrem lokalen Computer (oder irgendwo anders) auszuf√ºhren. Klicken Sie einfach auf: Datei>Download\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5qTkbWrkECOJ"
   },
   "source": [
    "Was die Optimierung der Hyperparameter angeht, so rate ich, sich auf diese 3 Hyperparameter zu konzentrieren:\n",
    "- `Lernrate`\n",
    "- Puffergr√∂√üe (Erfahrungsspeichergr√∂√üe)`\n",
    "- `Stapelgr√∂√üe`\n",
    "\n",
    "Als gute Praxis sollten Sie **die Dokumentation lesen, um zu verstehen, was jeder Hyperparameter bewirkt**: https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html#parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hn8bRTHvERRL"
   },
   "source": [
    "2. Wir starten das Training und speichern die Modelle im Ordner \"Logs\" üìÅ.\n",
    "\n",
    "- Wir definieren den Algorithmus nach `--algo`, speichern das Modell nach `-f` und die Konfiguration der Hyperparameter nach `-c`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xr1TVW4xfbz3"
   },
   "outputs": [],
   "source": [
    "!python -m rl_zoo3.train --algo ________ --env SpaceInvadersNoFrameskip-v4  -f _________  -c _________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SeChoX-3SZfP"
   },
   "source": [
    "#### L√∂sung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PuocgdokSab9"
   },
   "outputs": [],
   "source": [
    "!python -m rl_zoo3.train --algo dqn  --env SpaceInvadersNoFrameskip-v4 -f logs/ -c dqn.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_dLomIiMKQaf"
   },
   "source": [
    "## Lass uns unseren Agenten auswerten üëÄ\n",
    "- RL-Baselines3-Zoo bietet `enjoy.py`, ein Python-Skript zur Auswertung unseres Agenten. In den meisten RL-Bibliotheken nennen wir das Auswertungsskript `enjoy.py`.\n",
    "- Lassen wir es f√ºr 5000 Zeitschritte auswerten üî•."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "co5um_KeKbBJ"
   },
   "outputs": [],
   "source": [
    "!python -m rl_zoo3.enjoy  --algo dqn  --env SpaceInvadersNoFrameskip-v4  --no-render  --n-timesteps _________  --folder logs/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q24K1tyWSj7t"
   },
   "source": [
    "#### L√∂sung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P_uSmwGRSk0z"
   },
   "outputs": [],
   "source": [
    "!python -m rl_zoo3.enjoy  --algo dqn  --env SpaceInvadersNoFrameskip-v4  --no-render  --n-timesteps 5000  --folder logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "liBeTltiHJtr"
   },
   "source": [
    "## Ver√∂ffentliche unser trainiertes Modell auf dem Hub üöÄ\n",
    "Da wir nun gesehen haben, dass wir nach dem Training gute Ergebnisse erzielt haben, k√∂nnen wir unser trainiertes Modell mit einer Zeile Code auf dem Hub ü§ó ver√∂ffentlichen.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit3/space-invaders-model.gif\" alt=\"Space Invaders model\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezbHS1q3HYVV"
   },
   "source": [
    "Mit `rl_zoo3.push_to_hub` **werten Sie aus, zeichnen ein Replay auf, generieren eine Modellkarte Ihres Agenten und schieben sie zum Hub**.\n",
    "\n",
    "Auf diese Weise:\n",
    "- Sie k√∂nnen **unsere Arbeit vorf√ºhren** üî•.\n",
    "- Sie k√∂nnen **Ihren Agenten beim Spielen visualisieren** üëÄ\n",
    "- Du kannst **einen Agenten mit der Community teilen, den andere benutzen k√∂nnen** üíæ\n",
    "- Sie k√∂nnen **auf eine Bestenliste üèÜ zugreifen, um zu sehen, wie gut Ihr Agent im Vergleich zu Ihren Klassenkameraden abschneidet** üëâ https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMSeZRBiHk6X"
   },
   "source": [
    "Um Ihr Modell mit der Gemeinschaft teilen zu k√∂nnen, sind drei weitere Schritte erforderlich:\n",
    "\n",
    "1Ô∏è‚É£ (Falls noch nicht geschehen) Erstellen Sie ein Konto f√ºr HF ‚û° https://huggingface.co/join\n",
    "\n",
    "2Ô∏è‚É£ Melde dich an und speichere dann dein Authentifizierungs-Token von der Hugging Face Website.\n",
    "- Erstellen Sie ein neues Token (https://huggingface.co/settings/tokens) **mit Schreibrolle**\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/create-token.jpg\" alt=\"HF-Token erstellen\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9O6FI0F8HnzE"
   },
   "source": [
    "- Kopieren Sie das Token\n",
    "- F√ºhren Sie die Zelle unten aus und f√ºgen Sie das Token ein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ppu9yePwHrZX"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.\n",
    "notebook_login()\n",
    "!git config --global credential.helper store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2RVEdunPHs8B"
   },
   "source": [
    "Wenn Sie kein Google Colab oder ein Jupyter Notebook verwenden m√∂chten, m√ºssen Sie stattdessen diesen Befehl verwenden: `huggingface-cli login`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dSLwdmvhHvjw"
   },
   "source": [
    "3Ô∏è‚É£ Wir sind jetzt bereit, unseren geschulten Agenten zum ü§ó Hub üî• zu bringen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PW436XnhHw1H"
   },
   "source": [
    "F√ºhren wir die Datei push_to_hub.py aus, um unseren trainierten Agenten in den Hub hochzuladen.\n",
    "\n",
    "`-Repo-Name`: Der Name des Repo\n",
    "\n",
    "`-orga`: Ihr Umarmungsgesicht-Benutzername\n",
    "\n",
    "`-f`: Wo sich der Ordner des trainierten Modells befindet (in unserem Fall `logs`)\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit3/select-id.png\" alt=\"Id ausw√§hlen\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ygk2sEktTDEw"
   },
   "outputs": [],
   "source": [
    "!python -m rl_zoo3.push_to_hub  --algo dqn  --env SpaceInvadersNoFrameskip-v4  --repo-name _____________________ -orga _____________________ -f logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "otgpa0rhS9wR"
   },
   "source": [
    "#### L√∂sung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_HQNlAXuEhci"
   },
   "outputs": [],
   "source": [
    "!python -m rl_zoo3.push_to_hub  --algo dqn  --env SpaceInvadersNoFrameskip-v4  --repo-name dqn-SpaceInvadersNoFrameskip-v4  -orga ThomasSimonini  -f logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0D4F5zsTTJ-L"
   },
   "source": [
    "###."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ff89kd2HL1_s"
   },
   "source": [
    "Herzlichen Gl√ºckwunsch ü•≥ Sie haben gerade Ihren ersten Deep Q-Learning-Agenten mit RL-Baselines-3 Zoo trainiert und hochgeladen. Das obige Skript sollte einen Link zu einem Modell-Repository wie https://huggingface.co/ThomasSimonini/dqn-SpaceInvadersNoFrameskip-v4 angezeigt haben. Wenn Sie zu diesem Link gehen, k√∂nnen Sie:\n",
    "\n",
    "- Eine **Video-Vorschau Ihres Agenten** auf der rechten Seite sehen.\n",
    "- Klicken Sie auf \"Dateien und Versionen\", um alle Dateien im Repository zu sehen.\n",
    "- Klicken Sie auf \"Use in stable-baselines3\", um ein Codeschnipsel zu erhalten, das zeigt, wie man das Modell l√§dt.\n",
    "- Eine Modellkarte (Datei `README.md`), die eine Beschreibung des Modells und der verwendeten Hyperparameter enth√§lt.\n",
    "\n",
    "Unter der Haube verwendet der Hub git-basierte Repositories (keine Sorge, wenn Sie nicht wissen, was git ist), was bedeutet, dass Sie das Modell mit neuen Versionen aktualisieren k√∂nnen, wenn Sie experimentieren und Ihren Agenten verbessern.\n",
    "\n",
    "**Vergleiche die Ergebnisse deiner Agenten mit denen deiner Klassenkameraden** mit Hilfe des [leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) üèÜ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fyRKcCYY-dIo"
   },
   "source": [
    "## Laden Sie ein leistungsstarkes trainiertes Modell üî•.\n",
    "- Das Stable-Baselines3-Team hat **mehr als 150 trainierte Deep Reinforcement Learning-Agenten auf den Hub** hochgeladen.\n",
    "\n",
    "Sie k√∂nnen sie hier finden: üëâ https://huggingface.co/sb3\n",
    "\n",
    "Einige Beispiele:\n",
    "- Asteroiden: https://huggingface.co/sb3/dqn-AsteroidsNoFrameskip-v4\n",
    "- Beam Rider: https://huggingface.co/sb3/dqn-BeamRiderNoFrameskip-v4\n",
    "- Breakout: https://huggingface.co/sb3/dqn-BreakoutNoFrameskip-v4\n",
    "- Road Runner: https://huggingface.co/sb3/dqn-RoadRunnerNoFrameskip-v4\n",
    "\n",
    "Laden wir einen Agenten, der Beam Rider spielt: https://huggingface.co/sb3/dqn-BeamRiderNoFrameskip-v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B-9QVFIROI5Y"
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<video controls autoplay><source src=\"https://huggingface.co/sb3/dqn-BeamRiderNoFrameskip-v4/resolve/main/replay.mp4\" type=\"video/mp4\"></video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ZQNY_r6NJtC"
   },
   "source": [
    "1. Wir laden das Modell mit `rl_zoo3.load_from_hub` herunter und legen es in einem neuen Ordner ab, den wir `rl_trained` nennen k√∂nnen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OdBNZHy0NGTR"
   },
   "outputs": [],
   "source": [
    "# Download model and save it into the logs/ folder\n",
    "!python -m rl_zoo3.load_from_hub --algo dqn --env BeamRiderNoFrameskip-v4 -orga sb3 -f rl_trained/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFt6hmWsNdBo"
   },
   "source": [
    "2. Lassen Sie uns f√ºr 5000 Zeitschritte auswerten, ob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aOxs0rNuN0uS"
   },
   "outputs": [],
   "source": [
    "!python -m rl_zoo3.enjoy --algo dqn --env BeamRiderNoFrameskip-v4 -n 5000  -f rl_trained/ --no-render"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxMDuDfPON57"
   },
   "source": [
    "Warum nicht versuchen, Ihre eigenen **Deep Q-Learning Agent spielen BeamRiderNoFrameskip-v4 zu trainieren? üèÜ.**\n",
    "\n",
    "Wenn Sie versuchen wollen, √ºberpr√ºfen Sie https://huggingface.co/sb3/dqn-BeamRiderNoFrameskip-v4#hyperparameters **in der Modellkarte, haben Sie die Hyperparameter des trainierten Agenten.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xL_ZtUgpOuY6"
   },
   "source": [
    "Aber die Suche nach Hyperparametern kann eine schwierige Aufgabe sein. Gl√ºcklicherweise werden wir in der n√§chsten Einheit sehen, wie wir **Optuna f√ºr die Optimierung der Hyperparameter üî• verwenden k√∂nnen.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-pqaco8W-huW"
   },
   "source": [
    "## Einige zus√§tzliche Herausforderungen üèÜ\n",
    "Die beste Art zu lernen **ist, Dinge selbst auszuprobieren**!\n",
    "\n",
    "In der [Rangliste] (https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) findest du deine Agenten. Kannst du es an die Spitze schaffen?\n",
    "\n",
    "Hier ist eine Liste von Umgebungen, mit denen du deinen Agenten trainieren kannst:\n",
    "- BeamRiderNoFrameskip-v4\n",
    "- BreakoutKeinFrameskip-v4\n",
    "- EnduroKeinFrameskip-v4\n",
    "- PongNoFrameskip-v4\n",
    "\n",
    "Au√üerdem **wenn Sie lernen wollen, Deep Q-Learning selbst zu implementieren**, sollten Sie sich unbedingt die CleanRL-Implementierung ansehen: https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/dqn_atari.py\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/atari-envs.gif\" alt=\"Umgebungen\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "paS-XKo4-kmu"
   },
   "source": [
    "________________________________________________________________________\n",
    "Herzlichen Gl√ºckwunsch zum Abschluss dieses Kapitels!\n",
    "\n",
    "Wenn du dich immer noch verwirrt f√ºhlst mit all diesen Elementen...das ist v√∂llig normal! **So ging es mir und allen anderen, die RL studiert haben.\n",
    "\n",
    "Nimm dir Zeit, um den Stoff wirklich zu **verstehen, bevor du weitermachst und die zus√§tzlichen Herausforderungen ausprobierst**. Es ist wichtig, diese Elemente zu beherrschen und eine solide Grundlage zu haben.\n",
    "\n",
    "In der n√§chsten Einheit **werden wir etwas √ºber [Optuna](https://optuna.org/)** lernen. Eine der wichtigsten Aufgaben beim Deep Reinforcement Learning ist es, einen guten Satz von Trainingshyperparametern zu finden. Und Optuna ist eine Bibliothek, die Ihnen hilft, die Suche zu automatisieren.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WRx7tO7-mvC"
   },
   "source": [
    "\n",
    "\n",
    "### Dies ist ein Kurs, der mit Ihnen aufgebaut wurde üë∑üèø‚Äç‚ôÄÔ∏è\n",
    "\n",
    "Schlie√ülich wollen wir den Kurs mit Hilfe Ihres Feedbacks verbessern und aktualisieren. Wenn Sie welche haben, f√ºllen Sie bitte dieses Formular aus üëâ https://forms.gle/3HgA7bEHwAmmLfwh9\n",
    "\n",
    "Wir versuchen st√§ndig, unsere Tutorials zu verbessern. **Wenn Sie also Probleme in diesem Notizbuch finden**, √∂ffnen Sie bitte [einen Fehler im Github Repo] (https://github.com/huggingface/deep-rl-class/issues)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kc3udPT-RcXc"
   },
   "source": [
    "Wir sehen uns bei Bonuseinheit 2! üî•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fS3Xerx0fIMV"
   },
   "source": [
    "### Keep Learning, Stay Awesome ü§ó"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
