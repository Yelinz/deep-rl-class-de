{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/notebooks/unit3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"In Colab √∂ffnen\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7xBVPzoXxOg"
   },
   "source": [
    "# Einheit 3: Tiefes Q-Learning mit Atari-Spielen üëæ mit RL Baselines3 Zoo\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/thumbnail.jpg\" alt=\"Unit 3 Thumbnail\">\n",
    "\n",
    "In diesem Notizbuch **trainieren Sie einen Deep Q-Learning-Agenten**, der Space Invaders spielt. Dazu verwenden Sie [RL Baselines3 Zoo] (https://github.com/DLR-RM/rl-baselines3-zoo), ein auf [Stable-Baselines3] (https://stable-baselines3.readthedocs.io/en/master/) basierendes Trainingsframework, das Skripte f√ºr das Training, die Auswertung von Agenten, die Abstimmung von Hyperparametern, die Darstellung von Ergebnissen und die Aufnahme von Videos bereitstellt.\n",
    "\n",
    "Wir verwenden die [RL-Baselines-3 Zoo Integration, eine Vanilla-Version von Deep Q-Learning](https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html) ohne Erweiterungen wie Double-DQN, Dueling-DQN und Prioritized Experience Replay.\n",
    "\n",
    "‚¨áÔ∏è Hier ein Beispiel daf√ºr, was **Sie erreichen werden** ‚¨áÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J9S713biXntc"
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<video controls autoplay><source src=\"https://huggingface.co/ThomasSimonini/ppo-SpaceInvadersNoFrameskip-v4/resolve/main/replay.mp4\" type=\"video/mp4\"></video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ykJiGevCMVc5"
   },
   "source": [
    "### üéÆ Umgebungen:\n",
    "\n",
    "- [SpacesInvadersNoFrameskip-v4](https://gymnasium.farama.org/environments/atari/space_invaders/)\n",
    "\n",
    "Sie k√∂nnen den Unterschied zwischen den Space Invaders Versionen hier sehen üëâ https://gymnasium.farama.org/environments/atari/space_invaders/#variants\n",
    "\n",
    "### üìö RL-Library:\n",
    "\n",
    "- [RL-Baselines3-Zoo](https://github.com/DLR-RM/rl-baselines3-zoo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wciHGjrFYz9m"
   },
   "source": [
    "## Ziele dieses Notizbuchs üèÜ\n",
    "Am Ende des Heftes wirst du:\n",
    "- In der Lage sein, tiefer zu verstehen **wie RL Baselines3 Zoo funktioniert**.\n",
    "- In der Lage sein, **Ihren trainierten Agenten und den Code in den Hub** mit einer sch√∂nen Videowiedergabe und einem Bewertungsergebnis zu pushen üî•.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TsnP0rjxMn1e"
   },
   "source": [
    "## Dieses Notebook stammt aus dem Deep Reinforcement Learning Kurs.\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/deep-rl-course-illustration.jpg\" alt=\"Deep RL Course illustration\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nw6fJHIAZd-J"
   },
   "source": [
    "In diesem kostenlosen Kurs lernen Sie:\n",
    "\n",
    "- üìñ Deep Reinforcement Learning in **Theorie und Praxis** studieren.\n",
    "- üßë‚Äçüíª Lernen Sie, **ber√ºhmte Deep RL-Bibliotheken** wie Stable Baselines3, RL Baselines3 Zoo, CleanRL und Sample Factory 2.0 zu verwenden.\n",
    "- ü§ñ Trainieren Sie **Agenten in einzigartigen Umgebungen**.\n",
    "\n",
    "Und mehr, siehe üìö den Lehrplan üëâ https://simoninithomas.github.io/deep-rl-course\n",
    "\n",
    "Vergessen Sie nicht, sich **<a href=\"http://eepurl.com/ic5ZUD\">f√ºr den Kurs anzumelden</a>** (wir sammeln Ihre E-Mail, um Ihnen **die Links zu senden, wenn die einzelnen Einheiten ver√∂ffentlicht werden, und Sie √ºber die Herausforderungen und Aktualisierungen zu informieren).**\n",
    "\n",
    "\n",
    "Der beste Weg, um in Kontakt zu bleiben, ist, unserem Discord-Server beizutreten, um sich mit der Community und mit uns auszutauschen üëâüèª https://discord.gg/ydHrjt3WP5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0vgANIBBZg1p"
   },
   "source": [
    "## Voraussetzungen üèóÔ∏è\n",
    "Bevor Sie sich mit dem Notebook besch√§ftigen, m√ºssen Sie:\n",
    "\n",
    "üî≤ üìö **[Deep Q-Learning durch Lesen von Einheit 3](https://huggingface.co/deep-rl-course/unit3/introduction)** ü§ó"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7kszpGFaRVhq"
   },
   "source": [
    "Wir versuchen st√§ndig, unsere Anleitungen zu verbessern. **Wenn Sie also Probleme in diesem Notizbuch** finden, √∂ffnen Sie bitte [ein Problem im Github Repo](https://github.com/huggingface/deep-rl-class/issues)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QR0jZtYreSI5"
   },
   "source": [
    "# Trainieren wir einen Deep-Q-Learning-Agenten, der Atari' Space Invaders üëæ spielt, und laden wir ihn in den Hub hoch.\n",
    "\n",
    "Wir empfehlen den Sch√ºlerinnen und Sch√ºlern dringend, **Google Colab f√ºr die praktischen √úbungen zu verwenden, anstatt sie auf ihren Computern auszuf√ºhren**.\n",
    "\n",
    "Durch die Verwendung von Google Colab k√∂nnen **Sie sich auf das Lernen und Experimentieren konzentrieren, ohne sich um die technischen Aspekte der Einrichtung Ihrer Umgebungen zu k√ºmmern**.\n",
    "\n",
    "Um diese praktische √úbung f√ºr den Zertifizierungsprozess zu validieren, m√ºssen Sie Ihr trainiertes Modell an den Hub senden und **ein Ergebnis von >= 200** erhalten.\n",
    "\n",
    "Um Ihr Ergebnis zu ermitteln, gehen Sie zum Leaderboard und suchen Sie Ihr Modell, **das Ergebnis = mean_reward - std of reward**\n",
    "\n",
    "Weitere Informationen √ºber den Zertifizierungsprozess finden Sie in diesem Abschnitt üëâ https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nc8BnyVEc3Ys"
   },
   "source": [
    "## Ein Ratschlag üí°\n",
    "Es ist besser, dieses Colab in einer Kopie auf Ihrem Google Drive auszuf√ºhren, so dass Sie **bei Zeit√ºberschreitungen** immer noch das gespeicherte Notizbuch auf Ihrem Google Drive haben und nicht alles von Grund auf neu ausf√ºllen m√ºssen.\n",
    "\n",
    "Dazu kannst du entweder \"Strg + S\" oder \"Datei > Kopie in Google Drive speichern\" verwenden.\n",
    "\n",
    "Au√üerdem werden wir es **90 Minuten lang mit 1M Zeitschritten** trainieren. Die Eingabe von `!nvidia-smi` verr√§t Ihnen, welche GPU Sie verwenden.\n",
    "\n",
    "Wenn Sie mehr als 10 Millionen Schritte trainieren wollen, wird dies etwa 9 Stunden dauern, was m√∂glicherweise zu einer Zeit√ºberschreitung von Colab f√ºhrt. In diesem Fall empfehle ich, das Programm auf Ihrem lokalen Computer (oder irgendwo anders) auszuf√ºhren. Klicken Sie einfach auf: Datei>Download\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PU4FVzaoM6fC"
   },
   "source": [
    "## Die GPU einstellen üí™.\n",
    "- Um **das Training des Agenten zu beschleunigen, werden wir einen Grafikprozessor** verwenden. Gehen Sie dazu auf \"Runtime > Change Runtime type\".\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step1.jpg\" alt=\"GPU Schritt 1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KV0NyFdQM9ZG"
   },
   "source": [
    "- Hardware-Beschleuniger > GPU\".\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step2.jpg\" alt=\"GPU Schritt 2\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wS_cVefO-aYg"
   },
   "source": [
    "# RL-Baselines3 Zoo und seine Abh√§ngigkeiten installieren üìö.\n",
    "\n",
    "Wenn Sie \"ERROR: pip's dependency resolver does not currently take into account all the packages that are installed\" sehen, **das ist normal und kein kritischer Fehler**, gibt es einen Versionskonflikt. Aber die Pakete, die wir brauchen, sind installiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hLTwHqIWdnPb"
   },
   "outputs": [],
   "source": [
    "# For now we install this update of RL-Baselines3 Zoo\n",
    "!pip install git+https://github.com/DLR-RM/rl-baselines3-zoo@update/hf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p0xe2sJHdtHy"
   },
   "source": [
    "WENN UND NUR WENN DIE OBIGE VERSION NICHT MEHR EXISTIERT. ENTKOMMENTIEREN UND INSTALLIEREN SIE DIE UNTEN STEHENDE VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N0d6wy-F-f39"
   },
   "outputs": [],
   "source": [
    "#!pip install rl_zoo3==2.0.0a9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8_MllY6Om1eI"
   },
   "outputs": [],
   "source": [
    "!apt-get install swig cmake ffmpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4S9mJiKg6SqC"
   },
   "source": [
    "Um Atari-Spiele in Gymnasium verwenden zu k√∂nnen, m√ºssen wir das Atari-Paket installieren. Und accept-rom-license, um die Rom-Dateien (Spiele-Dateien) herunterzuladen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NsRP-lX1_2fC"
   },
   "outputs": [],
   "source": [
    "!pip install gymnasium[atari]\n",
    "!pip install gymnasium[accept-rom-license]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bTpYcVZVMzUI"
   },
   "source": [
    "## Erstellen einer virtuellen Anzeige üîΩ.\n",
    "\n",
    "W√§hrend der Arbeit mit dem Notebook m√ºssen wir ein Wiederholungsvideo erstellen. Dazu ben√∂tigen wir mit colab **einen virtuellen Bildschirm, um die Umgebung zu rendern** (und somit die Bilder aufzunehmen).\n",
    "\n",
    "Daher wird die folgende Zelle die Librairies installieren und einen virtuellen Bildschirm erstellen und starten üñ•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jV6wjQ7Be7p5"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!apt install python-opengl\n",
    "!apt install xvfb\n",
    "!pip3 install pyvirtualdisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BE5JWP5rQIKf"
   },
   "outputs": [],
   "source": [
    "# Virtual display\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "virtual_display = Display(visible=0, size=(1400, 900))\n",
    "virtual_display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5iPgzluo9z-u"
   },
   "source": [
    "## Trainiere unseren Deep Q-Learning Agent, um Space Invaders zu spielen üëæ\n",
    "\n",
    "Um einen Agenten mit RL-Baselines3-Zoo zu trainieren, m√ºssen wir nur zwei Dinge tun:\n",
    "\n",
    "1. Erstellen Sie eine Hyperparameter-Konfigurationsdatei, die unsere Trainings-Hyperparameter mit dem Namen \"dqn.yml\" enthalten wird.\n",
    "\n",
    "Dies ist ein Beispiel f√ºr eine Vorlage:\n",
    "\n",
    "```\n",
    "SpaceInvadersNoFrameskip-v4:\n",
    "  env_wrapper:\n",
    "    - stable_baselines3.common.atari_wrappers.AtariWrapper\n",
    "  frame_stack: 4\n",
    "  Richtlinie: 'CnnPolicy'\n",
    "  n_timesteps: !!float 1e6\n",
    "  puffer_gr√∂√üe: 100000\n",
    "  lern_rate: !!float 1e-4\n",
    "  batch_size: 32\n",
    "  lern_starts: 100000\n",
    "  Ziel_aktualisieren_Intervall: 1000\n",
    "  train_freq: 4\n",
    "  gradient_steps: 1\n",
    "  Erkundung_Anteil: 0.1\n",
    "  exploration_final_eps: 0.01\n",
    "  # Wenn True, m√ºssen Sie handle_timeout_termination deaktivieren.\n",
    "  # in den replay_buffer_kwargs\n",
    "  optimize_memory_usage: False\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VjblFSVDQOj"
   },
   "source": [
    "Hier sehen wir das:\n",
    "- Wir verwenden den \"Atari Wrapper\", der die Eingabe vorverarbeitet (Frame-Reduktion, Graustufen, 4 Frames stapeln)\n",
    "- Wir verwenden `CnnPolicy`, da wir Faltungsschichten zur Verarbeitung der Frames verwenden\n",
    "- Wir trainieren es f√ºr 10 Millionen `n_Zeitschritte`\n",
    "- Die Gr√∂√üe des Speichers (Experience Replay) betr√§gt 100000, d.h. die Anzahl der Erfahrungsschritte, die Sie gespeichert haben, um Ihren Agenten erneut zu trainieren.\n",
    "\n",
    "üí° Mein Rat ist, **die Trainingszeitschritte auf 1M zu reduzieren,** was auf einem P100 etwa 90 Minuten dauern wird. !nvidia-smi\" wird Ihnen sagen, welche GPU Sie verwenden. Bei 10 Millionen Schritten wird dies etwa 9 Stunden dauern, was wahrscheinlich zu einer Zeit√ºberschreitung von Colab f√ºhren k√∂nnte. Ich empfehle, dies auf Ihrem lokalen Computer (oder irgendwo anders) auszuf√ºhren. Klicken Sie einfach auf: Datei>Download\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5qTkbWrkECOJ"
   },
   "source": [
    "Was die Optimierung der Hyperparameter angeht, so rate ich, sich auf diese 3 Hyperparameter zu konzentrieren:\n",
    "- `Lernrate`\n",
    "- Puffergr√∂√üe (Erfahrungsspeichergr√∂√üe)`\n",
    "- `Stapelgr√∂√üe`\n",
    "\n",
    "Als gute Praxis sollten Sie **die Dokumentation lesen, um zu verstehen, was jeder Hyperparameter bewirkt**: https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html#parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hn8bRTHvERRL"
   },
   "source": [
    "2. Wir starten das Training und speichern die Modelle im Ordner \"Logs\" üìÅ.\n",
    "\n",
    "- Wir definieren den Algorithmus nach `--algo`, speichern das Modell nach `-f` und die Konfiguration der Hyperparameter nach `-c`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xr1TVW4xfbz3"
   },
   "outputs": [],
   "source": [
    "!python -m rl_zoo3.train --algo ________ --env SpaceInvadersNoFrameskip-v4  -f _________  -c _________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SeChoX-3SZfP"
   },
   "source": [
    "#### L√∂sung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PuocgdokSab9"
   },
   "outputs": [],
   "source": [
    "!python -m rl_zoo3.train --algo dqn  --env SpaceInvadersNoFrameskip-v4 -f logs/ -c dqn.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_dLomIiMKQaf"
   },
   "source": [
    "## Lass uns unseren Agenten auswerten üëÄ\n",
    "- RL-Baselines3-Zoo bietet `enjoy.py`, ein Python-Skript zur Auswertung unseres Agenten. In den meisten RL-Bibliotheken nennen wir das Auswertungsskript `enjoy.py`.\n",
    "- Lassen wir es f√ºr 5000 Zeitschritte auswerten üî•."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "co5um_KeKbBJ"
   },
   "outputs": [],
   "source": [
    "!python -m rl_zoo3.enjoy  --algo dqn  --env SpaceInvadersNoFrameskip-v4  --no-render  --n-timesteps _________  --folder logs/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q24K1tyWSj7t"
   },
   "source": [
    "#### L√∂sung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P_uSmwGRSk0z"
   },
   "outputs": [],
   "source": [
    "!python -m rl_zoo3.enjoy  --algo dqn  --env SpaceInvadersNoFrameskip-v4  --no-render  --n-timesteps 5000  --folder logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "liBeTltiHJtr"
   },
   "source": [
    "## Ver√∂ffentliche unser trainiertes Modell auf dem Hub üöÄ\n",
    "Da wir nun gesehen haben, dass wir nach dem Training gute Ergebnisse erzielt haben, k√∂nnen wir unser trainiertes Modell mit einer Zeile Code auf dem Hub ü§ó ver√∂ffentlichen.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit3/space-invaders-model.gif\" alt=\"Space Invaders model\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezbHS1q3HYVV"
   },
   "source": [
    "Mit `rl_zoo3.push_to_hub` **werten Sie aus, zeichnen ein Replay auf, generieren eine Modellkarte Ihres Agenten und schieben sie zum Hub**.\n",
    "\n",
    "Auf diese Weise:\n",
    "- Sie k√∂nnen **unsere Arbeit vorf√ºhren** üî•.\n",
    "- Sie k√∂nnen **Ihren Agenten beim Spielen visualisieren** üëÄ\n",
    "- Du kannst **einen Agenten mit der Community teilen, den andere benutzen k√∂nnen** üíæ\n",
    "- Sie k√∂nnen **auf eine Bestenliste üèÜ zugreifen, um zu sehen, wie gut Ihr Agent im Vergleich zu Ihren Klassenkameraden abschneidet** üëâ https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMSeZRBiHk6X"
   },
   "source": [
    "Um Ihr Modell mit der Gemeinschaft teilen zu k√∂nnen, sind drei weitere Schritte erforderlich:\n",
    "\n",
    "1Ô∏è‚É£ (Falls noch nicht geschehen) Erstellen Sie ein Konto f√ºr HF ‚û° https://huggingface.co/join\n",
    "\n",
    "2Ô∏è‚É£ Melde dich an und speichere dann dein Authentifizierungs-Token von der Hugging Face Website.\n",
    "- Erstellen Sie ein neues Token (https://huggingface.co/settings/tokens) **mit Schreibrolle**\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/create-token.jpg\" alt=\"HF-Token erstellen\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9O6FI0F8HnzE"
   },
   "source": [
    "- Kopieren Sie das Token\n",
    "- F√ºhren Sie die Zelle unten aus und f√ºgen Sie das Token ein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ppu9yePwHrZX"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.\n",
    "notebook_login()\n",
    "!git config --global credential.helper store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2RVEdunPHs8B"
   },
   "source": [
    "Wenn Sie kein Google Colab oder ein Jupyter Notebook verwenden m√∂chten, m√ºssen Sie stattdessen diesen Befehl verwenden: `huggingface-cli login`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dSLwdmvhHvjw"
   },
   "source": [
    "3Ô∏è‚É£ Wir sind jetzt bereit, unseren geschulten Agenten zum ü§ó Hub üî• zu bringen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PW436XnhHw1H"
   },
   "source": [
    "F√ºhren wir die Datei push_to_hub.py aus, um unseren trainierten Agenten in den Hub hochzuladen.\n",
    "\n",
    "`-Repo-Name`: Der Name des Repo\n",
    "\n",
    "`-orga`: Ihr Umarmungsgesicht-Benutzername\n",
    "\n",
    "`-f`: Wo sich der Ordner des trainierten Modells befindet (in unserem Fall `logs`)\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit3/select-id.png\" alt=\"Id ausw√§hlen\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ygk2sEktTDEw"
   },
   "outputs": [],
   "source": [
    "!python -m rl_zoo3.push_to_hub  --algo dqn  --env SpaceInvadersNoFrameskip-v4  --repo-name _____________________ -orga _____________________ -f logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "otgpa0rhS9wR"
   },
   "source": [
    "#### L√∂sung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_HQNlAXuEhci"
   },
   "outputs": [],
   "source": [
    "!python -m rl_zoo3.push_to_hub  --algo dqn  --env SpaceInvadersNoFrameskip-v4  --repo-name dqn-SpaceInvadersNoFrameskip-v4  -orga ThomasSimonini  -f logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0D4F5zsTTJ-L"
   },
   "source": [
    "###."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ff89kd2HL1_s"
   },
   "source": [
    "Herzlichen Gl√ºckwunsch ü•≥ Sie haben gerade Ihren ersten Deep Q-Learning-Agenten mit RL-Baselines-3 Zoo trainiert und hochgeladen. Das obige Skript sollte einen Link zu einem Modell-Repository wie https://huggingface.co/ThomasSimonini/dqn-SpaceInvadersNoFrameskip-v4 angezeigt haben. Wenn Sie zu diesem Link gehen, k√∂nnen Sie:\n",
    "\n",
    "- Eine **Video-Vorschau Ihres Agenten** auf der rechten Seite sehen.\n",
    "- Klicken Sie auf \"Dateien und Versionen\", um alle Dateien im Repository zu sehen.\n",
    "- Klicken Sie auf \"Use in stable-baselines3\", um ein Codeschnipsel zu erhalten, das zeigt, wie man das Modell l√§dt.\n",
    "- Eine Modellkarte (Datei `README.md`), die eine Beschreibung des Modells und der verwendeten Hyperparameter enth√§lt.\n",
    "\n",
    "Unter der Haube verwendet der Hub git-basierte Repositories (keine Sorge, wenn Sie nicht wissen, was git ist), was bedeutet, dass Sie das Modell mit neuen Versionen aktualisieren k√∂nnen, wenn Sie experimentieren und Ihren Agenten verbessern.\n",
    "\n",
    "**Vergleiche die Ergebnisse deiner Agenten mit denen deiner Klassenkameraden** mit Hilfe des [leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) üèÜ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fyRKcCYY-dIo"
   },
   "source": [
    "## Laden Sie ein leistungsstarkes trainiertes Modell üî•.\n",
    "- Das Stable-Baselines3-Team hat **mehr als 150 trainierte Deep Reinforcement Learning-Agenten auf den Hub** hochgeladen.\n",
    "\n",
    "Sie k√∂nnen sie hier finden: üëâ https://huggingface.co/sb3\n",
    "\n",
    "Einige Beispiele:\n",
    "- Asteroiden: https://huggingface.co/sb3/dqn-AsteroidsNoFrameskip-v4\n",
    "- Beam Rider: https://huggingface.co/sb3/dqn-BeamRiderNoFrameskip-v4\n",
    "- Breakout: https://huggingface.co/sb3/dqn-BreakoutNoFrameskip-v4\n",
    "- Road Runner: https://huggingface.co/sb3/dqn-RoadRunnerNoFrameskip-v4\n",
    "\n",
    "Laden wir einen Agenten, der Beam Rider spielt: https://huggingface.co/sb3/dqn-BeamRiderNoFrameskip-v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B-9QVFIROI5Y"
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<video controls autoplay><source src=\"https://huggingface.co/sb3/dqn-BeamRiderNoFrameskip-v4/resolve/main/replay.mp4\" type=\"video/mp4\"></video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ZQNY_r6NJtC"
   },
   "source": [
    "1. Wir laden das Modell mit `rl_zoo3.load_from_hub` herunter und legen es in einem neuen Ordner ab, den wir `rl_trained` nennen k√∂nnen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OdBNZHy0NGTR"
   },
   "outputs": [],
   "source": [
    "# Download model and save it into the logs/ folder\n",
    "!python -m rl_zoo3.load_from_hub --algo dqn --env BeamRiderNoFrameskip-v4 -orga sb3 -f rl_trained/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFt6hmWsNdBo"
   },
   "source": [
    "2. Lassen Sie uns f√ºr 5000 Zeitschritte auswerten, ob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aOxs0rNuN0uS"
   },
   "outputs": [],
   "source": [
    "!python -m rl_zoo3.enjoy --algo dqn --env BeamRiderNoFrameskip-v4 -n 5000  -f rl_trained/ --no-render"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxMDuDfPON57"
   },
   "source": [
    "Warum nicht versuchen, Ihre eigenen **Deep Q-Learning Agent spielen BeamRiderNoFrameskip-v4 zu trainieren? üèÜ.**\n",
    "\n",
    "Wenn Sie versuchen wollen, √ºberpr√ºfen Sie https://huggingface.co/sb3/dqn-BeamRiderNoFrameskip-v4#hyperparameters **in der Modellkarte, haben Sie die Hyperparameter des trainierten Agenten.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xL_ZtUgpOuY6"
   },
   "source": [
    "Aber die Suche nach Hyperparametern kann eine schwierige Aufgabe sein. Gl√ºcklicherweise werden wir in der n√§chsten Einheit sehen, wie wir **Optuna f√ºr die Optimierung der Hyperparameter üî• verwenden k√∂nnen.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-pqaco8W-huW"
   },
   "source": [
    "## Einige zus√§tzliche Herausforderungen üèÜ\n",
    "Die beste Art zu lernen **ist, Dinge selbst auszuprobieren**!\n",
    "\n",
    "In der [Rangliste] (https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) findest du deine Agenten. Kannst du es an die Spitze schaffen?\n",
    "\n",
    "Hier ist eine Liste von Umgebungen, mit denen du deinen Agenten trainieren kannst:\n",
    "- BeamRiderNoFrameskip-v4\n",
    "- BreakoutKeinFrameskip-v4\n",
    "- EnduroKeinFrameskip-v4\n",
    "- PongNoFrameskip-v4\n",
    "\n",
    "Au√üerdem **wenn Sie lernen wollen, Deep Q-Learning selbst zu implementieren**, sollten Sie sich unbedingt die CleanRL-Implementierung ansehen: https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/dqn_atari.py\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/atari-envs.gif\" alt=\"Umgebungen\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "paS-XKo4-kmu"
   },
   "source": [
    "________________________________________________________________________\n",
    "Herzlichen Gl√ºckwunsch zum Abschluss dieses Kapitels!\n",
    "\n",
    "Wenn du dich immer noch verwirrt f√ºhlst mit all diesen Elementen...das ist v√∂llig normal! **So ging es mir und allen anderen, die RL studiert haben.\n",
    "\n",
    "Nimm dir Zeit, um den Stoff wirklich zu **verstehen, bevor du weitermachst und die zus√§tzlichen Herausforderungen ausprobierst**. Es ist wichtig, diese Elemente zu beherrschen und eine solide Grundlage zu haben.\n",
    "\n",
    "In der n√§chsten Einheit **werden wir etwas √ºber [Optuna](https://optuna.org/)** lernen. Eine der wichtigsten Aufgaben beim Deep Reinforcement Learning ist es, einen guten Satz von Trainingshyperparametern zu finden. Und Optuna ist eine Bibliothek, die Ihnen hilft, die Suche zu automatisieren.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WRx7tO7-mvC"
   },
   "source": [
    "\n",
    "\n",
    "### Dies ist ein Kurs, der mit Ihnen aufgebaut wurde üë∑üèø‚Äç‚ôÄÔ∏è\n",
    "\n",
    "Schlie√ülich wollen wir den Kurs mit Hilfe Ihres Feedbacks verbessern und aktualisieren. Wenn Sie welche haben, f√ºllen Sie bitte dieses Formular aus üëâ https://forms.gle/3HgA7bEHwAmmLfwh9\n",
    "\n",
    "Wir versuchen st√§ndig, unsere Tutorials zu verbessern. **Wenn Sie also Probleme in diesem Notizbuch finden**, √∂ffnen Sie bitte [einen Fehler im Github Repo] (https://github.com/huggingface/deep-rl-class/issues)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kc3udPT-RcXc"
   },
   "source": [
    "Wir sehen uns bei Bonuseinheit 2! üî•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fS3Xerx0fIMV"
   },
   "source": [
    "### Keep Learning, Stay Awesome ü§ó"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
