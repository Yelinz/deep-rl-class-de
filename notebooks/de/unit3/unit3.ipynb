{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/notebooks/unit3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"In Colab öffnen\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7xBVPzoXxOg"
   },
   "source": [
    "# Einheit 3: Tiefes Q-Learning mit Atari-Spielen 👾 mit RL Baselines3 Zoo\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/thumbnail.jpg\" alt=\"Unit 3 Thumbnail\">\n",
    "\n",
    "In diesem Notizbuch **trainieren Sie einen Deep Q-Learning-Agenten**, der Space Invaders spielt. Dazu verwenden Sie [RL Baselines3 Zoo] (https://github.com/DLR-RM/rl-baselines3-zoo), ein auf [Stable-Baselines3] (https://stable-baselines3.readthedocs.io/en/master/) basierendes Trainingsframework, das Skripte für das Training, die Auswertung von Agenten, die Abstimmung von Hyperparametern, die Darstellung von Ergebnissen und die Aufnahme von Videos bereitstellt.\n",
    "\n",
    "Wir verwenden die [RL-Baselines-3 Zoo Integration, eine Vanilla-Version von Deep Q-Learning](https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html) ohne Erweiterungen wie Double-DQN, Dueling-DQN und Prioritized Experience Replay.\n",
    "\n",
    "⬇️ Hier ein Beispiel dafür, was **Sie erreichen werden** ⬇️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J9S713biXntc"
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<video controls autoplay><source src=\"https://huggingface.co/ThomasSimonini/ppo-SpaceInvadersNoFrameskip-v4/resolve/main/replay.mp4\" type=\"video/mp4\"></video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ykJiGevCMVc5"
   },
   "source": [
    "### 🎮 Umgebungen:\n",
    "\n",
    "- [SpacesInvadersNoFrameskip-v4](https://gymnasium.farama.org/environments/atari/space_invaders/)\n",
    "\n",
    "Sie können den Unterschied zwischen den Space Invaders Versionen hier sehen 👉 https://gymnasium.farama.org/environments/atari/space_invaders/#variants\n",
    "\n",
    "### 📚 RL-Library:\n",
    "\n",
    "- [RL-Baselines3-Zoo](https://github.com/DLR-RM/rl-baselines3-zoo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wciHGjrFYz9m"
   },
   "source": [
    "## Ziele dieses Notizbuchs 🏆\n",
    "Am Ende des Heftes wirst du:\n",
    "- In der Lage sein, tiefer zu verstehen **wie RL Baselines3 Zoo funktioniert**.\n",
    "- In der Lage sein, **Ihren trainierten Agenten und den Code in den Hub** mit einer schönen Videowiedergabe und einem Bewertungsergebnis zu pushen 🔥.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TsnP0rjxMn1e"
   },
   "source": [
    "## Dieses Notebook stammt aus dem Deep Reinforcement Learning Kurs.\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/deep-rl-course-illustration.jpg\" alt=\"Deep RL Course illustration\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nw6fJHIAZd-J"
   },
   "source": [
    "In diesem kostenlosen Kurs lernen Sie:\n",
    "\n",
    "- 📖 Deep Reinforcement Learning in **Theorie und Praxis** studieren.\n",
    "- 🧑‍💻 Lernen Sie, **berühmte Deep RL-Bibliotheken** wie Stable Baselines3, RL Baselines3 Zoo, CleanRL und Sample Factory 2.0 zu verwenden.\n",
    "- 🤖 Trainieren Sie **Agenten in einzigartigen Umgebungen**.\n",
    "\n",
    "Und mehr, siehe 📚 den Lehrplan 👉 https://simoninithomas.github.io/deep-rl-course\n",
    "\n",
    "Vergessen Sie nicht, sich **<a href=\"http://eepurl.com/ic5ZUD\">für den Kurs anzumelden</a>** (wir sammeln Ihre E-Mail, um Ihnen **die Links zu senden, wenn die einzelnen Einheiten veröffentlicht werden, und Sie über die Herausforderungen und Aktualisierungen zu informieren).**\n",
    "\n",
    "\n",
    "Der beste Weg, um in Kontakt zu bleiben, ist, unserem Discord-Server beizutreten, um sich mit der Community und mit uns auszutauschen 👉🏻 https://discord.gg/ydHrjt3WP5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0vgANIBBZg1p"
   },
   "source": [
    "## Voraussetzungen 🏗️\n",
    "Bevor Sie sich mit dem Notebook beschäftigen, müssen Sie:\n",
    "\n",
    "🔲 📚 **[Deep Q-Learning durch Lesen von Einheit 3](https://huggingface.co/deep-rl-course/unit3/introduction)** 🤗"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7kszpGFaRVhq"
   },
   "source": [
    "Wir versuchen ständig, unsere Anleitungen zu verbessern. **Wenn Sie also Probleme in diesem Notizbuch** finden, öffnen Sie bitte [ein Problem im Github Repo](https://github.com/huggingface/deep-rl-class/issues)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QR0jZtYreSI5"
   },
   "source": [
    "# Trainieren wir einen Deep-Q-Learning-Agenten, der Atari' Space Invaders 👾 spielt, und laden wir ihn in den Hub hoch.\n",
    "\n",
    "Wir empfehlen den Schülerinnen und Schülern dringend, **Google Colab für die praktischen Übungen zu verwenden, anstatt sie auf ihren Computern auszuführen**.\n",
    "\n",
    "Durch die Verwendung von Google Colab können **Sie sich auf das Lernen und Experimentieren konzentrieren, ohne sich um die technischen Aspekte der Einrichtung Ihrer Umgebungen zu kümmern**.\n",
    "\n",
    "Um diese praktische Übung für den Zertifizierungsprozess zu validieren, müssen Sie Ihr trainiertes Modell an den Hub senden und **ein Ergebnis von >= 200** erhalten.\n",
    "\n",
    "Um Ihr Ergebnis zu ermitteln, gehen Sie zum Leaderboard und suchen Sie Ihr Modell, **das Ergebnis = mean_reward - std of reward**\n",
    "\n",
    "Weitere Informationen über den Zertifizierungsprozess finden Sie in diesem Abschnitt 👉 https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nc8BnyVEc3Ys"
   },
   "source": [
    "## Ein Ratschlag 💡\n",
    "Es ist besser, dieses Colab in einer Kopie auf Ihrem Google Drive auszuführen, so dass Sie **bei Zeitüberschreitungen** immer noch das gespeicherte Notizbuch auf Ihrem Google Drive haben und nicht alles von Grund auf neu ausfüllen müssen.\n",
    "\n",
    "Dazu kannst du entweder \"Strg + S\" oder \"Datei > Kopie in Google Drive speichern\" verwenden.\n",
    "\n",
    "Außerdem werden wir es **90 Minuten lang mit 1M Zeitschritten** trainieren. Die Eingabe von `!nvidia-smi` verrät Ihnen, welche GPU Sie verwenden.\n",
    "\n",
    "Wenn Sie mehr als 10 Millionen Schritte trainieren wollen, wird dies etwa 9 Stunden dauern, was möglicherweise zu einer Zeitüberschreitung von Colab führt. In diesem Fall empfehle ich, das Programm auf Ihrem lokalen Computer (oder irgendwo anders) auszuführen. Klicken Sie einfach auf: Datei>Download\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PU4FVzaoM6fC"
   },
   "source": [
    "## Die GPU einstellen 💪.\n",
    "- Um **das Training des Agenten zu beschleunigen, werden wir einen Grafikprozessor** verwenden. Gehen Sie dazu auf \"Runtime > Change Runtime type\".\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step1.jpg\" alt=\"GPU Schritt 1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KV0NyFdQM9ZG"
   },
   "source": [
    "- Hardware-Beschleuniger > GPU\".\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step2.jpg\" alt=\"GPU Schritt 2\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wS_cVefO-aYg"
   },
   "source": [
    "# RL-Baselines3 Zoo und seine Abhängigkeiten installieren 📚.\n",
    "\n",
    "Wenn Sie \"ERROR: pip's dependency resolver does not currently take into account all the packages that are installed\" sehen, **das ist normal und kein kritischer Fehler**, gibt es einen Versionskonflikt. Aber die Pakete, die wir brauchen, sind installiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hLTwHqIWdnPb"
   },
   "outputs": [],
   "source": [
    "# For now we install this update of RL-Baselines3 Zoo\n",
    "!pip install git+https://github.com/DLR-RM/rl-baselines3-zoo@update/hf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p0xe2sJHdtHy"
   },
   "source": [
    "WENN UND NUR WENN DIE OBIGE VERSION NICHT MEHR EXISTIERT. ENTKOMMENTIEREN UND INSTALLIEREN SIE DIE UNTEN STEHENDE VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N0d6wy-F-f39"
   },
   "outputs": [],
   "source": [
    "#!pip install rl_zoo3==2.0.0a9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8_MllY6Om1eI"
   },
   "outputs": [],
   "source": [
    "!apt-get install swig cmake ffmpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4S9mJiKg6SqC"
   },
   "source": [
    "Um Atari-Spiele in Gymnasium verwenden zu können, müssen wir das Atari-Paket installieren. Und accept-rom-license, um die Rom-Dateien (Spiele-Dateien) herunterzuladen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NsRP-lX1_2fC"
   },
   "outputs": [],
   "source": [
    "!pip install gymnasium[atari]\n",
    "!pip install gymnasium[accept-rom-license]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bTpYcVZVMzUI"
   },
   "source": [
    "## Erstellen einer virtuellen Anzeige 🔽.\n",
    "\n",
    "Während der Arbeit mit dem Notebook müssen wir ein Wiederholungsvideo erstellen. Dazu benötigen wir mit colab **einen virtuellen Bildschirm, um die Umgebung zu rendern** (und somit die Bilder aufzunehmen).\n",
    "\n",
    "Daher wird die folgende Zelle die Librairies installieren und einen virtuellen Bildschirm erstellen und starten 🖥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jV6wjQ7Be7p5"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!apt install python-opengl\n",
    "!apt install xvfb\n",
    "!pip3 install pyvirtualdisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BE5JWP5rQIKf"
   },
   "outputs": [],
   "source": [
    "# Virtual display\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "virtual_display = Display(visible=0, size=(1400, 900))\n",
    "virtual_display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5iPgzluo9z-u"
   },
   "source": [
    "## Trainiere unseren Deep Q-Learning Agent, um Space Invaders zu spielen 👾\n",
    "\n",
    "Um einen Agenten mit RL-Baselines3-Zoo zu trainieren, müssen wir nur zwei Dinge tun:\n",
    "\n",
    "1. Erstellen Sie eine Hyperparameter-Konfigurationsdatei, die unsere Trainings-Hyperparameter mit dem Namen \"dqn.yml\" enthalten wird.\n",
    "\n",
    "Dies ist ein Beispiel für eine Vorlage:\n",
    "\n",
    "```\n",
    "SpaceInvadersNoFrameskip-v4:\n",
    "  env_wrapper:\n",
    "    - stable_baselines3.common.atari_wrappers.AtariWrapper\n",
    "  frame_stack: 4\n",
    "  Richtlinie: 'CnnPolicy'\n",
    "  n_timesteps: !!float 1e6\n",
    "  puffer_größe: 100000\n",
    "  lern_rate: !!float 1e-4\n",
    "  batch_size: 32\n",
    "  lern_starts: 100000\n",
    "  Ziel_aktualisieren_Intervall: 1000\n",
    "  train_freq: 4\n",
    "  gradient_steps: 1\n",
    "  Erkundung_Anteil: 0.1\n",
    "  exploration_final_eps: 0.01\n",
    "  # Wenn True, müssen Sie handle_timeout_termination deaktivieren.\n",
    "  # in den replay_buffer_kwargs\n",
    "  optimize_memory_usage: False\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VjblFSVDQOj"
   },
   "source": [
    "Hier sehen wir das:\n",
    "- Wir verwenden den \"Atari Wrapper\", der die Eingabe vorverarbeitet (Frame-Reduktion, Graustufen, 4 Frames stapeln)\n",
    "- Wir verwenden `CnnPolicy`, da wir Faltungsschichten zur Verarbeitung der Frames verwenden\n",
    "- Wir trainieren es für 10 Millionen `n_Zeitschritte`\n",
    "- Die Größe des Speichers (Experience Replay) beträgt 100000, d.h. die Anzahl der Erfahrungsschritte, die Sie gespeichert haben, um Ihren Agenten erneut zu trainieren.\n",
    "\n",
    "💡 Mein Rat ist, **die Trainingszeitschritte auf 1M zu reduzieren,** was auf einem P100 etwa 90 Minuten dauern wird. !nvidia-smi\" wird Ihnen sagen, welche GPU Sie verwenden. Bei 10 Millionen Schritten wird dies etwa 9 Stunden dauern, was wahrscheinlich zu einer Zeitüberschreitung von Colab führen könnte. Ich empfehle, dies auf Ihrem lokalen Computer (oder irgendwo anders) auszuführen. Klicken Sie einfach auf: Datei>Download\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5qTkbWrkECOJ"
   },
   "source": [
    "Was die Optimierung der Hyperparameter angeht, so rate ich, sich auf diese 3 Hyperparameter zu konzentrieren:\n",
    "- `Lernrate`\n",
    "- Puffergröße (Erfahrungsspeichergröße)`\n",
    "- `Stapelgröße`\n",
    "\n",
    "Als gute Praxis sollten Sie **die Dokumentation lesen, um zu verstehen, was jeder Hyperparameter bewirkt**: https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html#parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hn8bRTHvERRL"
   },
   "source": [
    "2. Wir starten das Training und speichern die Modelle im Ordner \"Logs\" 📁.\n",
    "\n",
    "- Wir definieren den Algorithmus nach `--algo`, speichern das Modell nach `-f` und die Konfiguration der Hyperparameter nach `-c`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xr1TVW4xfbz3"
   },
   "outputs": [],
   "source": [
    "!python -m rl_zoo3.train --algo ________ --env SpaceInvadersNoFrameskip-v4  -f _________  -c _________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SeChoX-3SZfP"
   },
   "source": [
    "#### Lösung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PuocgdokSab9"
   },
   "outputs": [],
   "source": [
    "!python -m rl_zoo3.train --algo dqn  --env SpaceInvadersNoFrameskip-v4 -f logs/ -c dqn.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_dLomIiMKQaf"
   },
   "source": [
    "## Lass uns unseren Agenten auswerten 👀\n",
    "- RL-Baselines3-Zoo bietet `enjoy.py`, ein Python-Skript zur Auswertung unseres Agenten. In den meisten RL-Bibliotheken nennen wir das Auswertungsskript `enjoy.py`.\n",
    "- Lassen wir es für 5000 Zeitschritte auswerten 🔥."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "co5um_KeKbBJ"
   },
   "outputs": [],
   "source": [
    "!python -m rl_zoo3.enjoy  --algo dqn  --env SpaceInvadersNoFrameskip-v4  --no-render  --n-timesteps _________  --folder logs/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q24K1tyWSj7t"
   },
   "source": [
    "#### Lösung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P_uSmwGRSk0z"
   },
   "outputs": [],
   "source": [
    "!python -m rl_zoo3.enjoy  --algo dqn  --env SpaceInvadersNoFrameskip-v4  --no-render  --n-timesteps 5000  --folder logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "liBeTltiHJtr"
   },
   "source": [
    "## Veröffentliche unser trainiertes Modell auf dem Hub 🚀\n",
    "Da wir nun gesehen haben, dass wir nach dem Training gute Ergebnisse erzielt haben, können wir unser trainiertes Modell mit einer Zeile Code auf dem Hub 🤗 veröffentlichen.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit3/space-invaders-model.gif\" alt=\"Space Invaders model\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezbHS1q3HYVV"
   },
   "source": [
    "Mit `rl_zoo3.push_to_hub` **werten Sie aus, zeichnen ein Replay auf, generieren eine Modellkarte Ihres Agenten und schieben sie zum Hub**.\n",
    "\n",
    "Auf diese Weise:\n",
    "- Sie können **unsere Arbeit vorführen** 🔥.\n",
    "- Sie können **Ihren Agenten beim Spielen visualisieren** 👀\n",
    "- Du kannst **einen Agenten mit der Community teilen, den andere benutzen können** 💾\n",
    "- Sie können **auf eine Bestenliste 🏆 zugreifen, um zu sehen, wie gut Ihr Agent im Vergleich zu Ihren Klassenkameraden abschneidet** 👉 https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMSeZRBiHk6X"
   },
   "source": [
    "Um Ihr Modell mit der Gemeinschaft teilen zu können, sind drei weitere Schritte erforderlich:\n",
    "\n",
    "1️⃣ (Falls noch nicht geschehen) Erstellen Sie ein Konto für HF ➡ https://huggingface.co/join\n",
    "\n",
    "2️⃣ Melde dich an und speichere dann dein Authentifizierungs-Token von der Hugging Face Website.\n",
    "- Erstellen Sie ein neues Token (https://huggingface.co/settings/tokens) **mit Schreibrolle**\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/create-token.jpg\" alt=\"HF-Token erstellen\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9O6FI0F8HnzE"
   },
   "source": [
    "- Kopieren Sie das Token\n",
    "- Führen Sie die Zelle unten aus und fügen Sie das Token ein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ppu9yePwHrZX"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.\n",
    "notebook_login()\n",
    "!git config --global credential.helper store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2RVEdunPHs8B"
   },
   "source": [
    "Wenn Sie kein Google Colab oder ein Jupyter Notebook verwenden möchten, müssen Sie stattdessen diesen Befehl verwenden: `huggingface-cli login`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dSLwdmvhHvjw"
   },
   "source": [
    "3️⃣ Wir sind jetzt bereit, unseren geschulten Agenten zum 🤗 Hub 🔥 zu bringen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PW436XnhHw1H"
   },
   "source": [
    "Führen wir die Datei push_to_hub.py aus, um unseren trainierten Agenten in den Hub hochzuladen.\n",
    "\n",
    "`-Repo-Name`: Der Name des Repo\n",
    "\n",
    "`-orga`: Ihr Umarmungsgesicht-Benutzername\n",
    "\n",
    "`-f`: Wo sich der Ordner des trainierten Modells befindet (in unserem Fall `logs`)\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit3/select-id.png\" alt=\"Id auswählen\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ygk2sEktTDEw"
   },
   "outputs": [],
   "source": [
    "!python -m rl_zoo3.push_to_hub  --algo dqn  --env SpaceInvadersNoFrameskip-v4  --repo-name _____________________ -orga _____________________ -f logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "otgpa0rhS9wR"
   },
   "source": [
    "#### Lösung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_HQNlAXuEhci"
   },
   "outputs": [],
   "source": [
    "!python -m rl_zoo3.push_to_hub  --algo dqn  --env SpaceInvadersNoFrameskip-v4  --repo-name dqn-SpaceInvadersNoFrameskip-v4  -orga ThomasSimonini  -f logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0D4F5zsTTJ-L"
   },
   "source": [
    "###."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ff89kd2HL1_s"
   },
   "source": [
    "Herzlichen Glückwunsch 🥳 Sie haben gerade Ihren ersten Deep Q-Learning-Agenten mit RL-Baselines-3 Zoo trainiert und hochgeladen. Das obige Skript sollte einen Link zu einem Modell-Repository wie https://huggingface.co/ThomasSimonini/dqn-SpaceInvadersNoFrameskip-v4 angezeigt haben. Wenn Sie zu diesem Link gehen, können Sie:\n",
    "\n",
    "- Eine **Video-Vorschau Ihres Agenten** auf der rechten Seite sehen.\n",
    "- Klicken Sie auf \"Dateien und Versionen\", um alle Dateien im Repository zu sehen.\n",
    "- Klicken Sie auf \"Use in stable-baselines3\", um ein Codeschnipsel zu erhalten, das zeigt, wie man das Modell lädt.\n",
    "- Eine Modellkarte (Datei `README.md`), die eine Beschreibung des Modells und der verwendeten Hyperparameter enthält.\n",
    "\n",
    "Unter der Haube verwendet der Hub git-basierte Repositories (keine Sorge, wenn Sie nicht wissen, was git ist), was bedeutet, dass Sie das Modell mit neuen Versionen aktualisieren können, wenn Sie experimentieren und Ihren Agenten verbessern.\n",
    "\n",
    "**Vergleiche die Ergebnisse deiner Agenten mit denen deiner Klassenkameraden** mit Hilfe des [leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) 🏆"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fyRKcCYY-dIo"
   },
   "source": [
    "## Laden Sie ein leistungsstarkes trainiertes Modell 🔥.\n",
    "- Das Stable-Baselines3-Team hat **mehr als 150 trainierte Deep Reinforcement Learning-Agenten auf den Hub** hochgeladen.\n",
    "\n",
    "Sie können sie hier finden: 👉 https://huggingface.co/sb3\n",
    "\n",
    "Einige Beispiele:\n",
    "- Asteroiden: https://huggingface.co/sb3/dqn-AsteroidsNoFrameskip-v4\n",
    "- Beam Rider: https://huggingface.co/sb3/dqn-BeamRiderNoFrameskip-v4\n",
    "- Breakout: https://huggingface.co/sb3/dqn-BreakoutNoFrameskip-v4\n",
    "- Road Runner: https://huggingface.co/sb3/dqn-RoadRunnerNoFrameskip-v4\n",
    "\n",
    "Laden wir einen Agenten, der Beam Rider spielt: https://huggingface.co/sb3/dqn-BeamRiderNoFrameskip-v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B-9QVFIROI5Y"
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<video controls autoplay><source src=\"https://huggingface.co/sb3/dqn-BeamRiderNoFrameskip-v4/resolve/main/replay.mp4\" type=\"video/mp4\"></video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ZQNY_r6NJtC"
   },
   "source": [
    "1. Wir laden das Modell mit `rl_zoo3.load_from_hub` herunter und legen es in einem neuen Ordner ab, den wir `rl_trained` nennen können"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OdBNZHy0NGTR"
   },
   "outputs": [],
   "source": [
    "# Download model and save it into the logs/ folder\n",
    "!python -m rl_zoo3.load_from_hub --algo dqn --env BeamRiderNoFrameskip-v4 -orga sb3 -f rl_trained/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFt6hmWsNdBo"
   },
   "source": [
    "2. Lassen Sie uns für 5000 Zeitschritte auswerten, ob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aOxs0rNuN0uS"
   },
   "outputs": [],
   "source": [
    "!python -m rl_zoo3.enjoy --algo dqn --env BeamRiderNoFrameskip-v4 -n 5000  -f rl_trained/ --no-render"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxMDuDfPON57"
   },
   "source": [
    "Warum nicht versuchen, Ihre eigenen **Deep Q-Learning Agent spielen BeamRiderNoFrameskip-v4 zu trainieren? 🏆.**\n",
    "\n",
    "Wenn Sie versuchen wollen, überprüfen Sie https://huggingface.co/sb3/dqn-BeamRiderNoFrameskip-v4#hyperparameters **in der Modellkarte, haben Sie die Hyperparameter des trainierten Agenten.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xL_ZtUgpOuY6"
   },
   "source": [
    "Aber die Suche nach Hyperparametern kann eine schwierige Aufgabe sein. Glücklicherweise werden wir in der nächsten Einheit sehen, wie wir **Optuna für die Optimierung der Hyperparameter 🔥 verwenden können.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-pqaco8W-huW"
   },
   "source": [
    "## Einige zusätzliche Herausforderungen 🏆\n",
    "Die beste Art zu lernen **ist, Dinge selbst auszuprobieren**!\n",
    "\n",
    "In der [Rangliste] (https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) findest du deine Agenten. Kannst du es an die Spitze schaffen?\n",
    "\n",
    "Hier ist eine Liste von Umgebungen, mit denen du deinen Agenten trainieren kannst:\n",
    "- BeamRiderNoFrameskip-v4\n",
    "- BreakoutKeinFrameskip-v4\n",
    "- EnduroKeinFrameskip-v4\n",
    "- PongNoFrameskip-v4\n",
    "\n",
    "Außerdem **wenn Sie lernen wollen, Deep Q-Learning selbst zu implementieren**, sollten Sie sich unbedingt die CleanRL-Implementierung ansehen: https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/dqn_atari.py\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/atari-envs.gif\" alt=\"Umgebungen\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "paS-XKo4-kmu"
   },
   "source": [
    "________________________________________________________________________\n",
    "Herzlichen Glückwunsch zum Abschluss dieses Kapitels!\n",
    "\n",
    "Wenn du dich immer noch verwirrt fühlst mit all diesen Elementen...das ist völlig normal! **So ging es mir und allen anderen, die RL studiert haben.\n",
    "\n",
    "Nimm dir Zeit, um den Stoff wirklich zu **verstehen, bevor du weitermachst und die zusätzlichen Herausforderungen ausprobierst**. Es ist wichtig, diese Elemente zu beherrschen und eine solide Grundlage zu haben.\n",
    "\n",
    "In der nächsten Einheit **werden wir etwas über [Optuna](https://optuna.org/)** lernen. Eine der wichtigsten Aufgaben beim Deep Reinforcement Learning ist es, einen guten Satz von Trainingshyperparametern zu finden. Und Optuna ist eine Bibliothek, die Ihnen hilft, die Suche zu automatisieren.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WRx7tO7-mvC"
   },
   "source": [
    "\n",
    "\n",
    "### Dies ist ein Kurs, der mit Ihnen aufgebaut wurde 👷🏿‍♀️\n",
    "\n",
    "Schließlich wollen wir den Kurs mit Hilfe Ihres Feedbacks verbessern und aktualisieren. Wenn Sie welche haben, füllen Sie bitte dieses Formular aus 👉 https://forms.gle/3HgA7bEHwAmmLfwh9\n",
    "\n",
    "Wir versuchen ständig, unsere Tutorials zu verbessern. **Wenn Sie also Probleme in diesem Notizbuch finden**, öffnen Sie bitte [einen Fehler im Github Repo] (https://github.com/huggingface/deep-rl-class/issues)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kc3udPT-RcXc"
   },
   "source": [
    "Wir sehen uns bei Bonuseinheit 2! 🔥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fS3Xerx0fIMV"
   },
   "source": [
    "### Keep Learning, Stay Awesome 🤗"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
