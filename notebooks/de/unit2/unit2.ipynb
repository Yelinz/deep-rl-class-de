{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/notebooks/unit2/unit2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"In Colab √∂ffnen\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njb_ProuHiOe"
   },
   "source": [
    "# Einheit 2: Q-Learning mit FrozenLake-v1 ‚õÑ und Taxi-v3 üöï\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/thumbnail.jpg\" alt=\"Unit 2 Thumbnail\">\n",
    "\n",
    "In diesem Notizbuch **programmieren Sie Ihren ersten Reinforcement Learning-Agenten von Grund auf**, um FrozenLake ‚ùÑÔ∏è mit Q-Learning zu spielen, ihn mit der Community zu teilen und mit verschiedenen Konfigurationen zu experimentieren.\n",
    "\n",
    "‚¨áÔ∏è Hier ist ein Beispiel daf√ºr, was **Sie in nur wenigen Minuten erreichen werden.** ‚¨áÔ∏è\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vRU_vXBrl1Jx"
   },
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/envs.gif\" alt=\"Umgebungen\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DPTBOv9HYLZ2"
   },
   "source": [
    "###üéÆ Umgebungen:\n",
    "\n",
    "- [FrozenLake-v1](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)\n",
    "- [Taxi-v3](https://gymnasium.farama.org/environments/toy_text/taxi/)\n",
    "\n",
    "###üìö RL-Library:\n",
    "\n",
    "- Python und NumPy\n",
    "- [Gymnasium](https://gymnasium.farama.org/)\n",
    "\n",
    "Wir sind st√§ndig bem√ºht, unsere Tutorials zu verbessern. **Wenn Sie also Probleme in diesem Notizbuch finden**, √∂ffnen Sie bitte [einen Fehler im GitHub Repo](https://github.com/huggingface/deep-rl-class/issues)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4i6tjI2tHQ8j"
   },
   "source": [
    "## Ziele dieses Notizbuchs üèÜ\n",
    "\n",
    "Am Ende des Notizbuchs werden Sie:\n",
    "\n",
    "- In der Lage sein, **Gymnasium**, die Umgebungsbibliothek, zu benutzen.\n",
    "- In der Lage sein, einen Q-Learning-Agenten von Grund auf zu programmieren.\n",
    "- In der Lage sein, **Ihren trainierten Agenten und den Code auf den Hub** mit einer sch√∂nen Videowiedergabe und einer Bewertung zu pushen üî•.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "viNzVbVaYvY3"
   },
   "source": [
    "## Dieses Notizbuch stammt aus dem Kurs Deep Reinforcement Learning\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/deep-rl-course-illustration.jpg\" alt=\"Deep RL Course illustration\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6p5HnEefISCB"
   },
   "source": [
    "In diesem kostenlosen Kurs lernen Sie:\n",
    "\n",
    "- üìñ Deep Reinforcement Learning in **Theorie und Praxis** studieren.\n",
    "- üßë‚Äçüíª Lernen Sie, **ber√ºhmte Deep RL-Bibliotheken** wie Stable Baselines3, RL Baselines3 Zoo, CleanRL und Sample Factory 2.0 zu verwenden.\n",
    "- ü§ñ Trainieren Sie **Agenten in einzigartigen Umgebungen**.\n",
    "\n",
    "Und mehr, siehe üìö den Lehrplan üëâ https://simoninithomas.github.io/deep-rl-course\n",
    "\n",
    "Vergessen Sie nicht, sich **<a href=\"http://eepurl.com/ic5ZUD\">f√ºr den Kurs anzumelden</a>** (wir sammeln Ihre E-Mail, um Ihnen **die Links zu senden, wenn die einzelnen Einheiten ver√∂ffentlicht werden, und Sie √ºber die Herausforderungen und Aktualisierungen zu informieren).**\n",
    "\n",
    "\n",
    "Der beste Weg, um in Kontakt zu bleiben, ist, unserem Discord-Server beizutreten, um sich mit der Community und mit uns auszutauschen üëâüèª https://discord.gg/ydHrjt3WP5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-mo_6rXIjRi"
   },
   "source": [
    "## Voraussetzungen üèóÔ∏è\n",
    "\n",
    "Bevor Sie sich mit dem Notebook besch√§ftigen, m√ºssen Sie:\n",
    "\n",
    "üî≤ üìö **Studieren Sie [Q-Learning by reading Unit 2](https://huggingface.co/deep-rl-course/unit2/introduction)** ü§ó"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2ONOODsyrMU"
   },
   "source": [
    "## Eine kleine Zusammenfassung von Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V68VveLacfxJ"
   },
   "source": [
    "*Q-Learning* **ist der RL-Algorithmus, der**:\n",
    "\n",
    "- Eine *Q-Funktion* trainiert, eine **Aktionswertfunktion**, die im internen Speicher durch eine *Q-Tabelle* **kodiert wird, die alle Werte der Zustands-Aktionspaare enth√§lt**.\n",
    "\n",
    "- Wenn ein Zustand und eine Aktion gegeben sind, sucht unsere Q-Funktion **in der Q-Tabelle nach dem entsprechenden Wert**.\n",
    "    \n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-function-2.jpg\" alt=\"Q-Funktion\" width=\"100%\"/>\n",
    "\n",
    "- Wenn das Training abgeschlossen ist, **haben wir eine optimale Q-Funktion, also eine optimale Q-Tabelle.**\n",
    "    \n",
    "- Und wenn wir **eine optimale Q-Funktion** haben, haben wir\n",
    "haben wir eine optimale Politik, da wir **f√ºr jeden Zustand die beste Aktion kennen, die zu ergreifen ist.**\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/link-value-policy.jpg\" alt=\"Link value policy\" width=\"100%\"/>\n",
    "\n",
    "\n",
    "Aber am Anfang ist unsere **Q-Tabelle nutzlos, da sie f√ºr jedes Zustands-Aktionspaar einen beliebigen Wert angibt (meistens initialisieren wir die Q-Tabelle mit 0 Werten)**. Aber wenn wir die Umgebung erkunden und unsere Q-Tabelle aktualisieren, wird sie uns immer bessere Ann√§herungen liefern\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/q-learning.jpeg\" alt=\"q-learning.jpeg\" width=\"100%\"/>\n",
    "\n",
    "Dies ist der Pseudocode f√ºr das Q-Learning:\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg\" alt=\"Q-Learning\" width=\"100%\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HEtx8Y8MqKfH"
   },
   "source": [
    "# Programmieren wir unseren ersten Reinforcement Learning Algorithmus üöÄ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kdxb1IhzTn0v"
   },
   "source": [
    "Um dieses Hands-On f√ºr den [Zertifizierungsprozess] (https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process) zu validieren, m√ºssen Sie Ihr trainiertes Taximodell an den Hub senden und **ein Ergebnis von >= 4,5** erhalten.\n",
    "\n",
    "Um Ihr Ergebnis zu finden, gehen Sie zur [Bestenliste] (https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) und suchen Sie Ihr Modell, **das Ergebnis = mean_reward - std of reward**\n",
    "\n",
    "Weitere Informationen √ºber den Zertifizierungsprozess finden Sie in diesem Abschnitt üëâ https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4gpxC1_kqUYe"
   },
   "source": [
    "## Abh√§ngigkeiten installieren und einen virtuellen Bildschirm erstellen üîΩ.\n",
    "\n",
    "Im Notebook m√ºssen wir ein Wiederholungsvideo erstellen. Dazu ben√∂tigen wir mit Colab **einen virtuellen Bildschirm, um die Umgebung zu rendern** (und somit die Bilder aufzunehmen).\n",
    "\n",
    "Daher wird die folgende Zelle die Bibliotheken installieren und einen virtuellen Bildschirm erstellen und starten üñ•\n",
    "\n",
    "Wir werden mehrere Bibliotheken installieren:\n",
    "\n",
    "- `gymnasium`: Enth√§lt die Umgebungen FrozenLake-v1 ‚õÑ und Taxi-v3 üöï.\n",
    "- pygame\": Wird f√ºr die FrozenLake-v1- und Taxi-v3-Benutzeroberfl√§che verwendet.\n",
    "- `numpy`: Wird f√ºr die Handhabung unserer Q-Tabelle verwendet.\n",
    "\n",
    "Der Hugging Face Hub ü§ó dient als zentraler Ort, an dem jeder Modelle und Datens√§tze teilen und erforschen kann. Er bietet Versionierung, Metriken, Visualisierungen und andere Funktionen, die eine einfache Zusammenarbeit mit anderen erm√∂glichen.\n",
    "\n",
    "Sie k√∂nnen alle verf√ºgbaren Deep-RL-Modelle (sofern sie Q Learning verwenden) hier sehen üëâ https://huggingface.co/models?other=q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9XaULfDZDvrC"
   },
   "outputs": [],
   "source": [
    "!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n71uTX7qqzz2"
   },
   "outputs": [],
   "source": [
    "!sudo apt-get update\n",
    "!sudo apt-get install -y python3-opengl\n",
    "!apt install ffmpeg xvfb\n",
    "!pip3 install pyvirtualdisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K6XC13pTfFiD"
   },
   "source": [
    "Um sicherzustellen, dass die neu installierten Bibliotheken verwendet werden, **ist es manchmal erforderlich, die Laufzeit des Notebooks neu zu starten**. Die n√§chste Zelle wird die **Laufzeitumgebung zum Absturz bringen, so dass Sie eine neue Verbindung herstellen und den Code von hier aus ausf√ºhren m√ºssen**. Dank dieses Tricks **k√∂nnen wir unseren virtuellen Bildschirm ausf√ºhren**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3kuZbWAkfHdg"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DaY1N4dBrabi"
   },
   "outputs": [],
   "source": [
    "# Virtual display\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "virtual_display = Display(visible=0, size=(1400, 900))\n",
    "virtual_display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W-7f-Swax_9x"
   },
   "source": [
    "## Importieren Sie die Pakete üì¶\n",
    "\n",
    "Zus√§tzlich zu den installierten Bibliotheken verwenden wir auch:\n",
    "\n",
    "- `random`: Um Zufallszahlen zu erzeugen (die f√ºr die Epsilon-Greedy-Politik n√ºtzlich sind).\n",
    "- `imageio`: Zur Erzeugung eines Wiederholungsvideos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VcNvOAQlysBJ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import random\n",
    "import imageio\n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "import pickle5 as pickle\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xp4-bXKIy1mQ"
   },
   "source": [
    "Wir sind jetzt bereit, unseren Q-Learning-Algorithmus zu programmieren üî•."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xya49aNJWVvv"
   },
   "source": [
    "# Teil 1: Gefrorener See ‚õÑ (nicht rutschige Version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NAvihuHdy9tw"
   },
   "source": [
    "## Erstellen und verstehen [FrozenLake-Umgebung ‚õÑ]((https://gymnasium.farama.org/environments/toy_text/frozen_lake/)\n",
    "---\n",
    "\n",
    "üí° Eine gute Angewohnheit, wenn Sie anfangen, eine Umgebung zu benutzen, ist es, ihre Dokumentation zu √ºberpr√ºfen\n",
    "\n",
    "üëâ https://gymnasium.farama.org/environments/toy_text/frozen_lake/\n",
    "\n",
    "---\n",
    "\n",
    "Wir werden unseren Q-Learning-Agenten darauf trainieren, **vom Startzustand (S) zum Zielzustand (G) zu navigieren, indem wir nur auf gefrorenen Kacheln (F) laufen und L√∂cher (H)** vermeiden.\n",
    "\n",
    "Wir k√∂nnen zwei Gr√∂√üen von Umgebungen haben:\n",
    "\n",
    "- `map_name=\"4x4\"`: eine 4x4-Gitterversion\n",
    "- `map_name=\"8x8\"`: eine 8x8-Gitterversion\n",
    "\n",
    "\n",
    "Die Umgebung hat zwei Modi:\n",
    "\n",
    "- `is_slippery=False`: Der Agent bewegt sich immer **in die beabsichtigte Richtung**, da der gefrorene See nicht rutschig ist (deterministisch).\n",
    "- is_slippery=True`: Der Agent **bewegt sich aufgrund der glatten Beschaffenheit des gefrorenen Sees nicht immer in die beabsichtigte Richtung** (stochastisch)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UaW_LHfS0PY2"
   },
   "source": [
    "F√ºr den Moment halten wir es einfach mit der 4x4 Karte und nicht rutschig.\n",
    "Wir f√ºgen einen Parameter namens `render_mode` hinzu, der angibt, wie die Umgebung visualisiert werden soll. In unserem Fall, weil wir **am Ende ein Video der Umgebung aufnehmen wollen, m√ºssen wir render_mode auf rgb_array** setzen.\n",
    "\n",
    "Wie [in der Dokumentation erkl√§rt](https://gymnasium.farama.org/api/env/#gymnasium.Env.render) \"rgb_array\": Gibt ein einzelnes Bild zur√ºck, das den aktuellen Zustand der Umgebung darstellt. Ein Frame ist ein np.ndarray mit der Form (x, y, 3), das RGB-Werte f√ºr ein x-mal-y-Pixelbild darstellt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IzJnb8O3y8up"
   },
   "outputs": [],
   "source": [
    "# Create the FrozenLake-v1 environment using 4x4 map and non-slippery version and render_mode=\"rgb_array\"\n",
    "env = gym.make() # TODO use the correct parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ji_UrI5l2zzn"
   },
   "source": [
    "### L√∂sung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jNxUbPMP0akP"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False, render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KASNViqL4tZn"
   },
   "source": [
    "Sie k√∂nnen Ihr eigenes benutzerdefiniertes Raster wie dieses erstellen:\n",
    "\n",
    "```python\n",
    "desc=[\"SFFF\", \"FHFH\", \"FFFH\", \"HFFG\"]\n",
    "gym.make('FrozenLake-v1', desc=desc, is_slippery=True)\n",
    "```\n",
    "\n",
    "aber wir werden vorerst die Standardumgebung verwenden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXbTfdeJ1Xi9"
   },
   "source": [
    "### Mal sehen, wie die Umwelt aussieht:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZNPG0g_UGCfh"
   },
   "outputs": [],
   "source": [
    "# We create our environment with gym.make(\"<name_of_the_environment>\")- `is_slippery=False`: The agent always moves in the intended direction due to the non-slippery nature of the frozen lake (deterministic).\n",
    "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
    "print(\"Observation Space\", env.observation_space)\n",
    "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2MXc15qFE0M9"
   },
   "source": [
    "Mit \"Observation Space Shape Discrete(16)\" sehen wir, dass die Beobachtung eine ganze Zahl ist, die die **aktuelle Position des Agenten als current_row * ncols + current_col darstellt (wobei sowohl row als auch col bei 0 beginnen)**.\n",
    "\n",
    "Zum Beispiel kann die Zielposition in der 4x4-Karte wie folgt berechnet werden: 3 * 4 + 3 = 15. Die Anzahl der m√∂glichen Beobachtungen ist abh√§ngig von der Gr√∂√üe der Karte. **Die 4x4-Karte hat zum Beispiel 16 m√∂gliche Beobachtungen**.\n",
    "\n",
    "\n",
    "So sieht zum Beispiel der Zustand = 0 aus:\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/frozenlake.png\" alt=\"FrozenLake\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "We5WqOBGLoSm"
   },
   "outputs": [],
   "source": [
    "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
    "print(\"Action Space Shape\", env.action_space.n)\n",
    "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MyxXwkI2Magx"
   },
   "source": [
    "Der Aktionsraum (die Menge der m√∂glichen Aktionen, die der Agent ausf√ºhren kann) ist diskret mit 4 verf√ºgbaren Aktionen üéÆ:\n",
    "- 0: LINKS GEHEN\n",
    "- 1: ABW√ÑRTS GEHEN\n",
    "- 2: RECHTS GEHEN\n",
    "- 3: NACH OBEN GEHEN\n",
    "\n",
    "Belohnungsfunktion üí∞:\n",
    "- Erreichen des Ziels: +1\n",
    "- Loch erreichen: 0\n",
    "- Gefrorenes Ziel erreichen: 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1pFhWblk3Awr"
   },
   "source": [
    "## Erstellen und Initialisieren der Q-Tabelle üóÑÔ∏è\n",
    "\n",
    "(üëÄ Schritt 1 des Pseudocodes)\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg\" alt=\"Q-Learning\" width=\"100%\"/>\n",
    "\n",
    "\n",
    "Es ist an der Zeit, unsere Q-Tabelle zu initialisieren! Um zu wissen, wie viele Zeilen (Zust√§nde) und Spalten (Aktionen) wir verwenden sollen, m√ºssen wir den Aktions- und Beobachtungsraum kennen. Wir kennen ihre Werte bereits von fr√ºher, aber wir wollen sie programmatisch erhalten, damit unser Algorithmus f√ºr verschiedene Umgebungen verallgemeinert werden kann. Gym bietet uns eine M√∂glichkeit, dies zu tun: `env.action_space.n` und `env.observation_space.n`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y3ZCdluj3k0l"
   },
   "outputs": [],
   "source": [
    "state_space =\n",
    "print(\"There are \", state_space, \" possible states\")\n",
    "\n",
    "action_space =\n",
    "print(\"There are \", action_space, \" possible actions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rCddoOXM3UQH"
   },
   "outputs": [],
   "source": [
    "# Let's create our Qtable of size (state_space, action_space) and initialized each values at 0 using np.zeros. np.zeros needs a tuple (a,b)\n",
    "def initialize_q_table(state_space, action_space):\n",
    "  Qtable =\n",
    "  return Qtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9YfvrqRt3jdR"
   },
   "outputs": [],
   "source": [
    "Qtable_frozenlake = initialize_q_table(state_space, action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67OdoKL63eDD"
   },
   "source": [
    "### L√∂sung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HuTKv3th3ohG"
   },
   "outputs": [],
   "source": [
    "state_space = env.observation_space.n\n",
    "print(\"There are \", state_space, \" possible states\")\n",
    "\n",
    "action_space = env.action_space.n\n",
    "print(\"There are \", action_space, \" possible actions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lnrb_nX33fJo"
   },
   "outputs": [],
   "source": [
    "# Let's create our Qtable of size (state_space, action_space) and initialized each values at 0 using np.zeros\n",
    "def initialize_q_table(state_space, action_space):\n",
    "  Qtable = np.zeros((state_space, action_space))\n",
    "  return Qtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y0WlgkVO3Jf9"
   },
   "outputs": [],
   "source": [
    "Qtable_frozenlake = initialize_q_table(state_space, action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Atll4Z774gri"
   },
   "source": [
    "## Definieren Sie die gierige Politik ü§ñ.\n",
    "\n",
    "Denken Sie daran, dass wir zwei Strategien haben, da Q-Learning ein **off-policy** Algorithmus ist. Das bedeutet, dass wir eine **unterschiedliche Strategie f√ºr das Handeln und die Aktualisierung der Wertfunktion** verwenden.\n",
    "\n",
    "- Epsilon-Greedy-Strategie (handelnde Strategie)\n",
    "- Greedy-Politik (Aktualisierungspolitik)\n",
    "\n",
    "Die Greedy-Policy wird auch die endg√ºltige Policy sein, die wir haben, wenn der Q-Learning-Agent das Training abgeschlossen hat. Die Greedy-Politik wird verwendet, um eine Aktion anhand der Q-Tabelle auszuw√§hlen.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/off-on-4.jpg\" alt=\"Q-Learning\" width=\"100%\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E3SCLmLX5bWG"
   },
   "outputs": [],
   "source": [
    "def greedy_policy(Qtable, state):\n",
    "  # Exploitation: take the action with the highest state, action value\n",
    "  action =\n",
    "\n",
    "  return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B2_-8b8z5k54"
   },
   "source": [
    "#### L√∂sung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "se2OzWGW5kYJ"
   },
   "outputs": [],
   "source": [
    "def greedy_policy(Qtable, state):\n",
    "  # Exploitation: take the action with the highest state, action value\n",
    "  action = np.argmax(Qtable[state][:])\n",
    "\n",
    "  return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "flILKhBU3yZ7"
   },
   "source": [
    "##Definieren Sie die Epsilon-Greedy-Politik ü§ñ.\n",
    "\n",
    "Epsilon-Greedy ist die Trainingsstrategie, die den Kompromiss zwischen Erkundung und Ausbeutung behandelt.\n",
    "\n",
    "Die Idee mit Epsilon-Greedy:\n",
    "\n",
    "- Mit *Wahrscheinlichkeit 1 - …õ* : **wir machen Exploitation** (d.h. unser Agent w√§hlt die Aktion mit dem h√∂chsten Wert des Zustands-Aktionspaares).\n",
    "\n",
    "- Mit *Wahrscheinlichkeit …õ*: **Exploration** (wir versuchen eine zuf√§llige Aktion).\n",
    "\n",
    "Mit fortschreitendem Training wird der Epsilon-Wert schrittweise **verringert, da wir immer weniger Exploration und mehr Exploitation ben√∂tigen**.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-4.jpg\" alt=\"Q-Learning\" width=\"100%\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Bj7x3in3_Pq"
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(Qtable, state, epsilon):\n",
    "  # Randomly generate a number between 0 and 1\n",
    "  random_num =\n",
    "  # if random_num > greater than epsilon --> exploitation\n",
    "  if random_num > epsilon:\n",
    "    # Take the action with the highest value given a state\n",
    "    # np.argmax can be useful here\n",
    "    action =\n",
    "  # else --> exploration\n",
    "  else:\n",
    "    action = # Take a random action\n",
    "\n",
    "  return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8R5ej1fS4P2V"
   },
   "source": [
    "#### L√∂sung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cYxHuckr4LiG"
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(Qtable, state, epsilon):\n",
    "  # Randomly generate a number between 0 and 1\n",
    "  random_num = random.uniform(0,1)\n",
    "  # if random_num > greater than epsilon --> exploitation\n",
    "  if random_num > epsilon:\n",
    "    # Take the action with the highest value given a state\n",
    "    # np.argmax can be useful here\n",
    "    action = greedy_policy(Qtable, state)\n",
    "  # else --> exploration\n",
    "  else:\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "  return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hW80DealcRtu"
   },
   "source": [
    "## Definieren Sie die Hyperparameter ‚öôÔ∏è\n",
    "\n",
    "Die Hyperparameter, die sich auf die Exploration beziehen, geh√∂ren zu den wichtigsten Parametern.\n",
    "\n",
    "- Wir m√ºssen sicherstellen, dass unser Agent **genug vom Zustandsraum erforscht**, um eine gute Wertann√§herung zu lernen. Um dies zu erreichen, m√ºssen wir einen progressiven Verfall von epsilon haben.\n",
    "- Wenn man epsilon zu schnell verringert (zu hohe decay_rate), **geht man das Risiko ein, dass der Agent feststeckt**, da er den Zustandsraum nicht ausreichend erforscht hat und daher das Problem nicht l√∂sen kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y1tWn0tycWZ1"
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "n_training_episodes = 10000  # Total training episodes\n",
    "learning_rate = 0.7          # Learning rate\n",
    "\n",
    "# Evaluation parameters\n",
    "n_eval_episodes = 100        # Total number of test episodes\n",
    "\n",
    "# Environment parameters\n",
    "env_id = \"FrozenLake-v1\"     # Name of the environment\n",
    "max_steps = 99               # Max steps per episode\n",
    "gamma = 0.95                 # Discounting rate\n",
    "eval_seed = []               # The evaluation seed of the environment\n",
    "\n",
    "# Exploration parameters\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.05            # Minimum exploration probability\n",
    "decay_rate = 0.0005            # Exponential decay rate for exploration prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cDb7Tdx8atfL"
   },
   "source": [
    "## Erstellen Sie die Trainingsschleifenmethode\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg\" alt=\"Q-Learning\" width=\"100%\"/>\n",
    "\n",
    "Die Trainingsschleife sieht folgenderma√üen aus:\n",
    "\n",
    "```\n",
    "F√ºr eine Episode in der Gesamtheit der Trainingsepisoden:\n",
    "\n",
    "Epsilon reduzieren (da wir immer weniger Erkundung brauchen)\n",
    "Die Umgebung zur√ºcksetzen\n",
    "\n",
    "  F√ºr Schritt in maximalen Zeitschritten:    \n",
    "    W√§hlen Sie die Aktion At mit epsilon gierig Politik\n",
    "    Ausf√ºhren der Aktion (a) und Beobachten des Ergebniszustands (s') und der Belohnung (r)\n",
    "    Aktualisierung des Q-Wertes Q(s,a) unter Verwendung der Bellman-Gleichung Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "    Wenn fertig, beende die Episode\n",
    "    Unser n√§chster Zustand ist der neue Zustand\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "paOynXy3aoJW"
   },
   "outputs": [],
   "source": [
    "def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n",
    "  for episode in tqdm(range(n_training_episodes)):\n",
    "    # Reduce epsilon (because we need less and less exploration)\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
    "    # Reset the environment\n",
    "    state, info = env.reset()\n",
    "    step = 0\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "\n",
    "    # repeat\n",
    "    for step in range(max_steps):\n",
    "      # Choose the action At using epsilon greedy policy\n",
    "      action =\n",
    "\n",
    "      # Take action At and observe Rt+1 and St+1\n",
    "      # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "      new_state, reward, terminated, truncated, info =\n",
    "\n",
    "      # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "      Qtable[state][action] =\n",
    "\n",
    "      # If terminated or truncated finish the episode\n",
    "      if terminated or truncated:\n",
    "        break\n",
    "\n",
    "      # Our next state is the new state\n",
    "      state = new_state\n",
    "  return Qtable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pnpk2ePoem3r"
   },
   "source": [
    "#### L√∂sung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IyZaYbUAeolw"
   },
   "outputs": [],
   "source": [
    "def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n",
    "  for episode in tqdm(range(n_training_episodes)):\n",
    "    # Reduce epsilon (because we need less and less exploration)\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
    "    # Reset the environment\n",
    "    state, info = env.reset()\n",
    "    step = 0\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "\n",
    "    # repeat\n",
    "    for step in range(max_steps):\n",
    "      # Choose the action At using epsilon greedy policy\n",
    "      action = epsilon_greedy_policy(Qtable, state, epsilon)\n",
    "\n",
    "      # Take action At and observe Rt+1 and St+1\n",
    "      # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "      new_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "      # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "      Qtable[state][action] = Qtable[state][action] + learning_rate * (reward + gamma * np.max(Qtable[new_state]) - Qtable[state][action])\n",
    "\n",
    "      # If terminated or truncated finish the episode\n",
    "      if terminated or truncated:\n",
    "        break\n",
    "\n",
    "      # Our next state is the new state\n",
    "      state = new_state\n",
    "  return Qtable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WLwKQ4tUdhGI"
   },
   "source": [
    "## Trainieren Sie den Q-Learning-Agenten üèÉ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DPBxfjJdTCOH"
   },
   "outputs": [],
   "source": [
    "Qtable_frozenlake = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_frozenlake)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yVeEhUCrc30L"
   },
   "source": [
    "## Mal sehen, wie unsere Q-Learning-Tabelle jetzt aussieht üëÄ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nmfchsTITw4q"
   },
   "outputs": [],
   "source": [
    "Qtable_frozenlake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pUrWkxsHccXD"
   },
   "source": [
    "## Die Bewertungsmethode üìù\n",
    "\n",
    "- Wir haben die Auswertungsmethode definiert, mit der wir unseren Q-Learning-Agenten testen werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jNl0_JO2cbkm"
   },
   "outputs": [],
   "source": [
    "def evaluate_agent(env, max_steps, n_eval_episodes, Q, seed):\n",
    "  \"\"\"\n",
    "  Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n",
    "  :param env: The evaluation environment\n",
    "  :param max_steps: Maximum number of steps per episode\n",
    "  :param n_eval_episodes: Number of episode to evaluate the agent\n",
    "  :param Q: The Q-table\n",
    "  :param seed: The evaluation seed array (for taxi-v3)\n",
    "  \"\"\"\n",
    "  episode_rewards = []\n",
    "  for episode in tqdm(range(n_eval_episodes)):\n",
    "    if seed:\n",
    "      state, info = env.reset(seed=seed[episode])\n",
    "    else:\n",
    "      state, info = env.reset()\n",
    "    step = 0\n",
    "    truncated = False\n",
    "    terminated = False\n",
    "    total_rewards_ep = 0\n",
    "\n",
    "    for step in range(max_steps):\n",
    "      # Take the action (index) that have the maximum expected future reward given that state\n",
    "      action = greedy_policy(Q, state)\n",
    "      new_state, reward, terminated, truncated, info = env.step(action)\n",
    "      total_rewards_ep += reward\n",
    "\n",
    "      if terminated or truncated:\n",
    "        break\n",
    "      state = new_state\n",
    "    episode_rewards.append(total_rewards_ep)\n",
    "  mean_reward = np.mean(episode_rewards)\n",
    "  std_reward = np.std(episode_rewards)\n",
    "\n",
    "  return mean_reward, std_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0jJqjaoAnxUo"
   },
   "source": [
    "## Bewerten Sie unseren Q-Learning-Agenten üìà\n",
    "\n",
    "- Normalerweise sollte man eine mittlere Belohnung von 1,0 haben.\n",
    "- Die **Umgebung ist relativ einfach**, da der Zustandsraum sehr klein ist (16). Man kann versuchen, sie durch die schl√ºpfrige Version zu ersetzen (https://gymnasium.farama.org/environments/toy_text/frozen_lake/), was Stochastizit√§t einf√ºhrt und die Umgebung komplexer macht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fAgB7s0HEFMm"
   },
   "outputs": [],
   "source": [
    "# Evaluate our Agent\n",
    "mean_reward, std_reward = evaluate_agent(env, max_steps, n_eval_episodes, Qtable_frozenlake, eval_seed)\n",
    "print(f\"Mean_reward={mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yxaP3bPdg1DV"
   },
   "source": [
    "## Ver√∂ffentliche unser trainiertes Modell auf dem Hub üî•\n",
    "\n",
    "Nun, da wir nach dem Training gute Ergebnisse gesehen haben, **k√∂nnen wir unser trainiertes Modell mit einer Zeile Code auf dem Hub ü§ó ver√∂ffentlichen**.\n",
    "\n",
    "Hier ist ein Beispiel f√ºr eine Model Card:\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/modelcard.png\" alt=\"Modellkarte\" width=\"100%\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kv0k1JQjpMq3"
   },
   "source": [
    "Unter der Haube verwendet der Hub Git-basierte Repositories (keine Sorge, wenn Sie nicht wissen, was Git ist), was bedeutet, dass Sie das Modell mit neuen Versionen aktualisieren k√∂nnen, wenn Sie experimentieren und Ihren Agenten verbessern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QZ5LrR-joIHD"
   },
   "source": [
    "#### Dieser Code darf nicht ver√§ndert werden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jex3i9lZ8ksX"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, snapshot_download\n",
    "from huggingface_hub.repocard import metadata_eval_result, metadata_save\n",
    "\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qo57HBn3W74O"
   },
   "outputs": [],
   "source": [
    "def record_video(env, Qtable, out_directory, fps=1):\n",
    "  \"\"\"\n",
    "  Generate a replay video of the agent\n",
    "  :param env\n",
    "  :param Qtable: Qtable of our agent\n",
    "  :param out_directory\n",
    "  :param fps: how many frame per seconds (with taxi-v3 and frozenlake-v1 we use 1)\n",
    "  \"\"\"\n",
    "  images = []\n",
    "  terminated = False\n",
    "  truncated = False\n",
    "  state, info = env.reset(seed=random.randint(0,500))\n",
    "  img = env.render()\n",
    "  images.append(img)\n",
    "  while not terminated or truncated:\n",
    "    # Take the action (index) that have the maximum expected future reward given that state\n",
    "    action = np.argmax(Qtable[state][:])\n",
    "    state, reward, terminated, truncated, info = env.step(action) # We directly put next_state = state for recording logic\n",
    "    img = env.render()\n",
    "    images.append(img)\n",
    "  imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4mdUTKkGnUd"
   },
   "outputs": [],
   "source": [
    "def push_to_hub(\n",
    "    repo_id, model, env, video_fps=1, local_repo_path=\"hub\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate, Generate a video and Upload a model to Hugging Face Hub.\n",
    "    This method does the complete pipeline:\n",
    "    - It evaluates the model\n",
    "    - It generates the model card\n",
    "    - It generates a replay video of the agent\n",
    "    - It pushes everything to the Hub\n",
    "\n",
    "    :param repo_id: repo_id: id of the model repository from the Hugging Face Hub\n",
    "    :param env\n",
    "    :param video_fps: how many frame per seconds to record our video replay\n",
    "    (with taxi-v3 and frozenlake-v1 we use 1)\n",
    "    :param local_repo_path: where the local repository is\n",
    "    \"\"\"\n",
    "    _, repo_name = repo_id.split(\"/\")\n",
    "\n",
    "    eval_env = env\n",
    "    api = HfApi()\n",
    "\n",
    "    # Step 1: Create the repo\n",
    "    repo_url = api.create_repo(\n",
    "        repo_id=repo_id,\n",
    "        exist_ok=True,\n",
    "    )\n",
    "\n",
    "    # Step 2: Download files\n",
    "    repo_local_path = Path(snapshot_download(repo_id=repo_id))\n",
    "\n",
    "    # Step 3: Save the model\n",
    "    if env.spec.kwargs.get(\"map_name\"):\n",
    "        model[\"map_name\"] = env.spec.kwargs.get(\"map_name\")\n",
    "        if env.spec.kwargs.get(\"is_slippery\", \"\") == False:\n",
    "            model[\"slippery\"] = False\n",
    "\n",
    "    # Pickle the model\n",
    "    with open((repo_local_path) / \"q-learning.pkl\", \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "    # Step 4: Evaluate the model and build JSON with evaluation metrics\n",
    "    mean_reward, std_reward = evaluate_agent(\n",
    "        eval_env, model[\"max_steps\"], model[\"n_eval_episodes\"], model[\"qtable\"], model[\"eval_seed\"]\n",
    "    )\n",
    "\n",
    "    evaluate_data = {\n",
    "        \"env_id\": model[\"env_id\"],\n",
    "        \"mean_reward\": mean_reward,\n",
    "        \"n_eval_episodes\": model[\"n_eval_episodes\"],\n",
    "        \"eval_datetime\": datetime.datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "    # Write a JSON file called \"results.json\" that will contain the\n",
    "    # evaluation results\n",
    "    with open(repo_local_path / \"results.json\", \"w\") as outfile:\n",
    "        json.dump(evaluate_data, outfile)\n",
    "\n",
    "    # Step 5: Create the model card\n",
    "    env_name = model[\"env_id\"]\n",
    "    if env.spec.kwargs.get(\"map_name\"):\n",
    "        env_name += \"-\" + env.spec.kwargs.get(\"map_name\")\n",
    "\n",
    "    if env.spec.kwargs.get(\"is_slippery\", \"\") == False:\n",
    "        env_name += \"-\" + \"no_slippery\"\n",
    "\n",
    "    metadata = {}\n",
    "    metadata[\"tags\"] = [env_name, \"q-learning\", \"reinforcement-learning\", \"custom-implementation\"]\n",
    "\n",
    "    # Add metrics\n",
    "    eval = metadata_eval_result(\n",
    "        model_pretty_name=repo_name,\n",
    "        task_pretty_name=\"reinforcement-learning\",\n",
    "        task_id=\"reinforcement-learning\",\n",
    "        metrics_pretty_name=\"mean_reward\",\n",
    "        metrics_id=\"mean_reward\",\n",
    "        metrics_value=f\"{mean_reward:.2f} +/- {std_reward:.2f}\",\n",
    "        dataset_pretty_name=env_name,\n",
    "        dataset_id=env_name,\n",
    "    )\n",
    "\n",
    "    # Merges both dictionaries\n",
    "    metadata = {**metadata, **eval}\n",
    "\n",
    "    model_card = f\"\"\"\n",
    "  # **Q-Learning** Agent playing1 **{env_id}**\n",
    "  This is a trained model of a **Q-Learning** agent playing **{env_id}** .\n",
    "\n",
    "  ## Usage\n",
    "\n",
    "  ```python\n",
    "\n",
    "  model = load_from_hub(repo_id=\"{repo_id}\", filename=\"q-learning.pkl\")\n",
    "\n",
    "  # Don't forget to check if you need to add additional attributes (is_slippery=False etc)\n",
    "  env = gym.make(model[\"env_id\"])\n",
    "  ```\n",
    "  \"\"\"\n",
    "\n",
    "    evaluate_agent(env, model[\"max_steps\"], model[\"n_eval_episodes\"], model[\"qtable\"], model[\"eval_seed\"])\n",
    "\n",
    "    readme_path = repo_local_path / \"README.md\"\n",
    "    readme = \"\"\n",
    "    print(readme_path.exists())\n",
    "    if readme_path.exists():\n",
    "        with readme_path.open(\"r\", encoding=\"utf8\") as f:\n",
    "            readme = f.read()\n",
    "    else:\n",
    "        readme = model_card\n",
    "\n",
    "    with readme_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(readme)\n",
    "\n",
    "    # Save our metrics to Readme metadata\n",
    "    metadata_save(readme_path, metadata)\n",
    "\n",
    "    # Step 6: Record a video\n",
    "    video_path = repo_local_path / \"replay.mp4\"\n",
    "    record_video(env, model[\"qtable\"], video_path, video_fps)\n",
    "\n",
    "    # Step 7. Push everything to the Hub\n",
    "    api.upload_folder(\n",
    "        repo_id=repo_id,\n",
    "        folder_path=repo_local_path,\n",
    "        path_in_repo=\".\",\n",
    "    )\n",
    "\n",
    "    print(\"Your model is pushed to the Hub. You can view your model here: \", repo_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81J6cet_ogSS"
   },
   "source": [
    "### .\n",
    "\n",
    "Mit \"push_to_hub\" **werten Sie aus, zeichnen ein Replay auf, generieren eine Modellkarte Ihres Agenten und schieben sie an den Hub**.\n",
    "\n",
    "This way:\n",
    "- Sie k√∂nnen **unsere Arbeit vorf√ºhren** üî•.\n",
    "- Sie k√∂nnen **Ihren Agenten beim Spielen visualisieren** üëÄ\n",
    "- Du kannst **einen Agenten mit der Community teilen, den andere nutzen k√∂nnen** üíæ\n",
    "- Sie k√∂nnen **auf eine Bestenliste zugreifen üèÜ um zu sehen, wie gut Ihr Agent im Vergleich zu Ihren Klassenkameraden abschneidet** üëâ https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cWnFC0iZooTw"
   },
   "source": [
    "Um Ihr Modell mit der Gemeinschaft teilen zu k√∂nnen, sind drei weitere Schritte erforderlich:\n",
    "\n",
    "1Ô∏è‚É£ (Falls noch nicht geschehen) Erstellen Sie ein Konto f√ºr HF ‚û° https://huggingface.co/join\n",
    "\n",
    "2Ô∏è‚É£ Melde dich an und speichere dann dein Authentifizierungs-Token von der Hugging Face Website.\n",
    "- Erstellen Sie ein neues Token (https://huggingface.co/settings/tokens) **mit Schreibrolle**\n",
    "\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/create-token.jpg\" alt=\"HF-Token erstellen\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QB5nIcxR8paT"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GyWc1x3-o3xG"
   },
   "source": [
    "Wenn Sie kein Google Colab oder ein Jupyter Notebook verwenden m√∂chten, m√ºssen Sie stattdessen diesen Befehl verwenden: `huggingface-cli login` (oder `login`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gc5AfUeFo3xH"
   },
   "source": [
    "3Ô∏è‚É£ Wir sind jetzt bereit, unseren trainierten Agenten mit der Funktion \"push_to_hub()\" an den ü§ó Hub üî• zu senden.\n",
    "\n",
    "- Erstellen wir **das Modellw√∂rterbuch, das die Hyperparameter und die Q_table** enth√§lt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FiMqxqVHg0I4"
   },
   "outputs": [],
   "source": [
    "model = {\n",
    "    \"env_id\": env_id,\n",
    "    \"max_steps\": max_steps,\n",
    "    \"n_training_episodes\": n_training_episodes,\n",
    "    \"n_eval_episodes\": n_eval_episodes,\n",
    "    \"eval_seed\": eval_seed,\n",
    "\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"gamma\": gamma,\n",
    "\n",
    "    \"max_epsilon\": max_epsilon,\n",
    "    \"min_epsilon\": min_epsilon,\n",
    "    \"decay_rate\": decay_rate,\n",
    "\n",
    "    \"qtable\": Qtable_frozenlake\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9kld-AEso3xH"
   },
   "source": [
    "F√ºllen wir die Funktion \"push_to_hub\":\n",
    "\n",
    "- `repo_id`: der Name des Hugging Face Hub Repository, das erstellt/aktualisiert werden soll `\n",
    "(repo_id = {Benutzername}/{repo_name})`\n",
    "üí° Eine gute `repo_id` ist `{username}/q-{env_id}`\n",
    "- model\": unser Modellw√∂rterbuch mit den Hyperparametern und der Q-Tabelle.\n",
    "- `env`: die Umgebung.\n",
    "- `commit_message`: die Nachricht der √úbergabe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5sBo2umnXpPd"
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RpOTtSt83kPZ"
   },
   "outputs": [],
   "source": [
    "username = \"\" # FILL THIS\n",
    "repo_name = \"q-FrozenLake-v1-4x4-noSlippery\"\n",
    "push_to_hub(\n",
    "    repo_id=f\"{username}/{repo_name}\",\n",
    "    model=model,\n",
    "    env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E2875IGsprzq"
   },
   "source": [
    "Herzlichen Gl√ºckwunsch ü•≥ Sie haben soeben Ihren ersten Reinforcement Learning Agent von Grund auf implementiert, trainiert und hochgeladen.\n",
    "FrozenLake-v1 no_slippery ist eine sehr einfache Umgebung, versuchen wir eine schwierigere üî•."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18lN8Bz7yvLt"
   },
   "source": [
    "# Teil 2: Taxi-v3 üöñ\n",
    "\n",
    "## Erstellen und verstehen [Taxi-v3 üöï](https://gymnasium.farama.org/environments/toy_text/taxi/)\n",
    "---\n",
    "\n",
    "üí° Eine gute Angewohnheit, wenn du anf√§ngst, eine Umgebung zu benutzen, ist es, ihre Dokumentation zu √ºberpr√ºfen.\n",
    "\n",
    "üëâ https://gymnasium.farama.org/environments/toy_text/taxi/\n",
    "\n",
    "---\n",
    "\n",
    "In `Taxi-v3` üöï gibt es vier bestimmte Orte in der Gitterwelt, die mit R(ed), G(reen), Y(ellow) und B(lue) bezeichnet werden.\n",
    "\n",
    "Zu Beginn der Episode **startet das Taxi an einem zuf√§lligen Platz** und der Fahrgast befindet sich an einem zuf√§lligen Ort. Das Taxi f√§hrt zum Standort des Fahrgastes, **nimmt den Fahrgast auf**, f√§hrt zum Zielort des Fahrgastes (einem anderen der vier angegebenen Orte) und setzt den Fahrgast dann **ab**. Sobald der Fahrgast abgesetzt ist, endet die Episode.\n",
    "\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/taxi.png\" alt=\"Taxi\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gL0wpeO8gpej"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\", render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gBOaXgtsrmtT"
   },
   "source": [
    "Es gibt **500 diskrete Zust√§nde, da es 25 Taxipositionen, 5 m√∂gliche Standorte des Fahrgastes** (einschlie√ülich des Falles, in dem sich der Fahrgast im Taxi befindet) und **4 Zielstandorte** gibt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_TPNaGSZrgqA"
   },
   "outputs": [],
   "source": [
    "state_space = env.observation_space.n\n",
    "print(\"There are \", state_space, \" possible states\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CdeeZuokrhit"
   },
   "outputs": [],
   "source": [
    "action_space = env.action_space.n\n",
    "print(\"There are \", action_space, \" possible actions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R1r50Advrh5Q"
   },
   "source": [
    "Der Aktionsraum (die Menge der m√∂glichen Aktionen, die der Agent ausf√ºhren kann) ist diskret mit **6 verf√ºgbaren Aktionen üéÆ**:\n",
    "\n",
    "- 0: nach S√ºden gehen\n",
    "- 1: nach Norden ziehen\n",
    "- 2: nach Osten ziehen\n",
    "- 3: nach Westen fahren\n",
    "- 4: Fahrgast aufnehmen\n",
    "- 5: Fahrgast absetzen\n",
    "\n",
    "Belohnungsfunktion üí∞:\n",
    "\n",
    "-1 pro Schritt, wenn keine andere Belohnung ausgel√∂st wird.\n",
    "- +20 Fahrgast abliefern.\n",
    "-10 Unerlaubtes Ausf√ºhren der Aktionen \"Abholen\" und \"Absetzen\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "US3yDXnEtY9I"
   },
   "outputs": [],
   "source": [
    "# Create our Q table with state_size rows and action_size columns (500x6)\n",
    "Qtable_taxi = initialize_q_table(state_space, action_space)\n",
    "print(Qtable_taxi)\n",
    "print(\"Q-table shape: \", Qtable_taxi .shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gUMKPH0_LJyH"
   },
   "source": [
    "## Definieren Sie die Hyperparameter ‚öôÔ∏è\n",
    "\n",
    "‚ö† VER√ÑNDERN SIE NICHT EVAL_SEED: Das eval_seed-Array **erm√∂glicht es uns, Ihren Agenten mit denselben Taxistartpositionen f√ºr jeden Klassenkameraden zu bewerten**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AB6n__hhg7YS"
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "n_training_episodes = 25000   # Total training episodes\n",
    "learning_rate = 0.7           # Learning rate\n",
    "\n",
    "# Evaluation parameters\n",
    "n_eval_episodes = 100        # Total number of test episodes\n",
    "\n",
    "# DO NOT MODIFY EVAL_SEED\n",
    "eval_seed = [16,54,165,177,191,191,120,80,149,178,48,38,6,125,174,73,50,172,100,148,146,6,25,40,68,148,49,167,9,97,164,176,61,7,54,55,\n",
    " 161,131,184,51,170,12,120,113,95,126,51,98,36,135,54,82,45,95,89,59,95,124,9,113,58,85,51,134,121,169,105,21,30,11,50,65,12,43,82,145,152,97,106,55,31,85,38,\n",
    " 112,102,168,123,97,21,83,158,26,80,63,5,81,32,11,28,148] # Evaluation seed, this ensures that all classmates agents are trained on the same taxi starting position\n",
    "                                                          # Each seed has a specific starting state\n",
    "\n",
    "# Environment parameters\n",
    "env_id = \"Taxi-v3\"           # Name of the environment\n",
    "max_steps = 99               # Max steps per episode\n",
    "gamma = 0.95                 # Discounting rate\n",
    "\n",
    "# Exploration parameters\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.05           # Minimum exploration probability\n",
    "decay_rate = 0.005            # Exponential decay rate for exploration prob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1TMORo1VLTsX"
   },
   "source": [
    "## Trainieren Sie unseren Q-Learning-Agenten üèÉ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WwP3Y2z2eS-K"
   },
   "outputs": [],
   "source": [
    "Qtable_taxi = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_taxi)\n",
    "Qtable_taxi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wPdu0SueLVl2"
   },
   "source": [
    "## Erstellen Sie ein Modellw√∂rterbuch üíæ und ver√∂ffentlichen Sie unser trainiertes Modell auf dem Hub üî•.\n",
    "\n",
    "- Wir erstellen ein Modell-W√∂rterbuch, das alle Trainings-Hyperparameter f√ºr die Reproduzierbarkeit und die Q-Tabelle enthalten wird.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0a1FpE_3hNYr"
   },
   "outputs": [],
   "source": [
    "model = {\n",
    "    \"env_id\": env_id,\n",
    "    \"max_steps\": max_steps,\n",
    "    \"n_training_episodes\": n_training_episodes,\n",
    "    \"n_eval_episodes\": n_eval_episodes,\n",
    "    \"eval_seed\": eval_seed,\n",
    "\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"gamma\": gamma,\n",
    "\n",
    "    \"max_epsilon\": max_epsilon,\n",
    "    \"min_epsilon\": min_epsilon,\n",
    "    \"decay_rate\": decay_rate,\n",
    "\n",
    "    \"qtable\": Qtable_taxi\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dhQtiQozhOn1"
   },
   "outputs": [],
   "source": [
    "username = \"\" # FILL THIS\n",
    "repo_name = \"\" # FILL THIS\n",
    "push_to_hub(\n",
    "    repo_id=f\"{username}/{repo_name}\",\n",
    "    model=model,\n",
    "    env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZgSdjgbIpRti"
   },
   "source": [
    "Jetzt, wo es auf dem Hub ist, kannst du die Ergebnisse deines Taxi-v3 mit deinen Klassenkameraden in der Bestenliste vergleichen üèÜ üëâ https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard\n",
    "\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/taxi-leaderboard.png\" alt=\"Taxi-Rangliste\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bzgIO70c0bu2"
   },
   "source": [
    "# Teil 3: Laden vom Hub üîΩ.\n",
    "\n",
    "Das Erstaunliche an Hugging Face Hub ü§ó ist, dass du ganz einfach leistungsstarke Modelle aus der Community laden kannst.\n",
    "\n",
    "Das Laden eines gespeicherten Modells aus dem Hub ist wirklich einfach:\n",
    "\n",
    "1. Du gehst auf https://huggingface.co/models?other=q-learning, um die Liste aller gespeicherten q-learning-Modelle zu sehen.\n",
    "2. Sie w√§hlen eines aus und kopieren seine repo_id\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/copy-id.png\" alt=\"ID kopieren\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTth6thRoC6X"
   },
   "source": [
    "3. Dann m√ºssen wir nur `load_from_hub` mit verwenden:\n",
    "- Die Repo_id\n",
    "- Der Dateiname: das gespeicherte Modell innerhalb der Repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EtrfoTaBoNrd"
   },
   "source": [
    "#### Dieser Code darf nicht ver√§ndert werden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eo8qEzNtCaVI"
   },
   "outputs": [],
   "source": [
    "from urllib.error import HTTPError\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "\n",
    "def load_from_hub(repo_id: str, filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Download a model from Hugging Face Hub.\n",
    "    :param repo_id: id of the model repository from the Hugging Face Hub\n",
    "    :param filename: name of the model zip file from the repository\n",
    "    \"\"\"\n",
    "    # Get the model from the Hub, download and cache the model on your local disk\n",
    "    pickle_model = hf_hub_download(\n",
    "        repo_id=repo_id,\n",
    "        filename=filename\n",
    "    )\n",
    "\n",
    "    with open(pickle_model, 'rb') as f:\n",
    "      downloaded_model_file = pickle.load(f)\n",
    "\n",
    "    return downloaded_model_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b_sM2gNioPZH"
   },
   "source": [
    "### ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JUm9lz2gCQcU"
   },
   "outputs": [],
   "source": [
    "model = load_from_hub(repo_id=\"ThomasSimonini/q-Taxi-v3\", filename=\"q-learning.pkl\") # Try to use another model\n",
    "\n",
    "print(model)\n",
    "env = gym.make(model[\"env_id\"])\n",
    "\n",
    "evaluate_agent(env, model[\"max_steps\"], model[\"n_eval_episodes\"], model[\"qtable\"], model[\"eval_seed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O7pL8rg1MulN"
   },
   "outputs": [],
   "source": [
    "model = load_from_hub(repo_id=\"ThomasSimonini/q-FrozenLake-v1-no-slippery\", filename=\"q-learning.pkl\") # Try to use another model\n",
    "\n",
    "env = gym.make(model[\"env_id\"], is_slippery=False)\n",
    "\n",
    "evaluate_agent(env, model[\"max_steps\"], model[\"n_eval_episodes\"], model[\"qtable\"], model[\"eval_seed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQAwLnYFPk-s"
   },
   "source": [
    "## Einige zus√§tzliche Herausforderungen üèÜ\n",
    "\n",
    "Die beste Art zu lernen **ist, Dinge selbst auszuprobieren**! Wie Sie gesehen haben, ist der derzeitige Agent nicht besonders gut. Als ersten Vorschlag k√∂nnen Sie f√ºr mehr Schritte trainieren. Bei 1.000.000 Schritten haben wir tolle Ergebnisse gesehen!\n",
    "\n",
    "In der [Rangliste] (https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) findest du deine Agenten. K√∂nnen Sie sich an die Spitze setzen?\n",
    "\n",
    "Hier sind einige Ideen, um in der Rangliste aufzusteigen:\n",
    "\n",
    "* Trainiere mehr Schritte\n",
    "* Probiere verschiedene Hyperparameter aus, indem du dir ansiehst, was deine Klassenkameraden gemacht haben.\n",
    "* Pushe dein neu trainiertes Modell** auf dem Hub üî•.\n",
    "\n",
    "Sind Ihnen das Laufen auf Eis und das Fahren von Taxis zu langweilig? Versuche, die **Umgebung** zu ver√§ndern, warum nicht die rutschige Version von FrozenLake-v1 benutzen? Schau dir an, wie sie funktionieren [mit Hilfe der Turnhallendokumentation] (https://gymnasium.farama.org/) und hab Spa√ü üéâ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-fW-EU5WejJ"
   },
   "source": [
    "_____________________________________________________________________\n",
    "Herzlichen Gl√ºckwunsch ü•≥, Sie haben soeben Ihren ersten Reinforcement Learning Agent implementiert, trainiert und hochgeladen.\n",
    "\n",
    "Das Verst√§ndnis von Q-Learning ist ein **wichtiger Schritt zum Verst√§ndnis wertbasierter Methoden**.\n",
    "\n",
    "In der n√§chsten Einheit mit Deep Q-Learning werden wir sehen, dass das Erstellen und Aktualisieren einer Q-Tabelle zwar eine gute Strategie war - **aber nicht skalierbar ist.**\n",
    "\n",
    "Stellen Sie sich zum Beispiel vor, Sie erstellen einen Agenten, der lernt, Doom zu spielen.\n",
    "\n",
    "<img src=\"https://vizdoom.cs.put.edu.pl/user/pages/01.tutorial/basic.png\" alt=\"Doom\"/>\n",
    "\n",
    "Doom ist eine gro√üe Umgebung mit einem riesigen Zustandsraum (Millionen von verschiedenen Zust√§nden). Das Erstellen und Aktualisieren einer Q-Tabelle f√ºr diese Umgebung w√§re nicht effizient.\n",
    "\n",
    "Deshalb werden wir uns in der n√§chsten Einheit mit Deep Q-Learning besch√§ftigen, einem Algorithmus, **bei dem wir ein neuronales Netz verwenden, das bei einem bestimmten Zustand die verschiedenen Q-Werte f√ºr jede Aktion ann√§hert**.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/atari-envs.gif\" alt=\"Umgebungen\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BjLhT70TEZIn"
   },
   "source": [
    "Wir sehen uns in Referat 3! üî•\n",
    "\n",
    "## Lernt weiter, bleibt fantastisch ü§ó"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "67OdoKL63eDD",
    "B2_-8b8z5k54",
    "8R5ej1fS4P2V",
    "Pnpk2ePoem3r"
   ],
   "include_colab_link": true,
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
