{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njb_ProuHiOe"
   },
   "source": [
    "# Einheit 2: Q-Learning mit FrozenLake-v1 ⛄ und Taxi-v3 🚕\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/thumbnail.jpg\" alt=\"Unit 2 Thumbnail\">\n",
    "\n",
    "In diesem Notizbuch **programmieren Sie Ihren ersten Reinforcement Learning-Agenten von Grund auf**, um FrozenLake ❄️ mit Q-Learning zu spielen, ihn mit der Community zu teilen und mit verschiedenen Konfigurationen zu experimentieren.\n",
    "\n",
    "⬇️ Hier ist ein Beispiel dafür, was **Sie in nur wenigen Minuten erreichen werden.** ⬇️\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vRU_vXBrl1Jx"
   },
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/envs.gif\" alt=\"Umgebungen\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DPTBOv9HYLZ2"
   },
   "source": [
    "### 🎮 Umgebungen:\n",
    "\n",
    "- [FrozenLake-v1](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)\n",
    "- [Taxi-v3](https://gymnasium.farama.org/environments/toy_text/taxi/)\n",
    "\n",
    "### 📚 RL-Library:\n",
    "\n",
    "- Python und NumPy\n",
    "- [Gymnasium](https://gymnasium.farama.org/)\n",
    "\n",
    "Wir sind ständig bemüht, unsere Tutorials zu verbessern. **Wenn Sie also Probleme in diesem Notizbuch finden**, öffnen Sie bitte [einen Fehler im GitHub Repo](https://github.com/huggingface/deep-rl-class/issues)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4i6tjI2tHQ8j"
   },
   "source": [
    "## Ziele dieses Notizbuchs 🏆\n",
    "\n",
    "Am Ende des Notizbuchs werden Sie:\n",
    "\n",
    "- In der Lage sein, **Gymnasium**, die Umgebungsbibliothek, zu benutzen.\n",
    "- In der Lage sein, einen Q-Learning-Agenten von Grund auf zu programmieren.\n",
    "- In der Lage sein, **Ihren trainierten Agenten und den Code auf den Hub** mit einer schönen Videowiedergabe und einer Bewertung zu pushen 🔥.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "viNzVbVaYvY3"
   },
   "source": [
    "## Dieses Notizbuch stammt aus dem Kurs Deep Reinforcement Learning\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/deep-rl-course-illustration.jpg\" alt=\"Deep RL Course illustration\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6p5HnEefISCB"
   },
   "source": [
    "In diesem kostenlosen Kurs lernen Sie:\n",
    "\n",
    "- 📖 Deep Reinforcement Learning in **Theorie und Praxis** studieren.\n",
    "- 🧑‍💻 Lernen Sie, **berühmte Deep RL-Bibliotheken** wie Stable Baselines3, RL Baselines3 Zoo, CleanRL und Sample Factory 2.0 zu verwenden.\n",
    "- 🤖 Trainieren Sie **Agenten in einzigartigen Umgebungen**.\n",
    "\n",
    "Und mehr, siehe 📚 den Lehrplan 👉 https://simoninithomas.github.io/deep-rl-course\n",
    "\n",
    "Vergessen Sie nicht, sich **<a href=\"http://eepurl.com/ic5ZUD\">für den Kurs anzumelden</a>** (wir sammeln Ihre E-Mail, um Ihnen **die Links zu senden, wenn die einzelnen Einheiten veröffentlicht werden, und Sie über die Herausforderungen und Aktualisierungen zu informieren).**\n",
    "\n",
    "\n",
    "Der beste Weg, um in Kontakt zu bleiben, ist, unserem Discord-Server beizutreten, um sich mit der Community und mit uns auszutauschen 👉🏻 https://discord.gg/ydHrjt3WP5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Einführung in das Q-Learning\n",
    "\n",
    "In der ersten Einheit dieses Kurses haben wir etwas über Reinforcement Learning (RL), den RL-Prozess und die verschiedenen Methoden zur Lösung eines RL-Problems gelernt. Wir haben auch **unsere ersten Agenten trainiert und sie in den Hugging Face Hub hochgeladen.**\n",
    "\n",
    "In dieser Einheit werden wir **eine der Methoden des Reinforcement Learning vertiefen: wertbasierte Methoden** und unseren ersten RL-Algorithmus untersuchen: **Q-Learning.**\n",
    "\n",
    "Wir werden auch **unseren ersten RL-Agenten von Grund auf** implementieren, einen Q-Learning-Agenten, und ihn in zwei Umgebungen trainieren:\n",
    "\n",
    "1. Frozen-Lake-v1 (rutschfeste Version): Hier muss unser Agent **vom Startzustand (S) zum Zielzustand (G)** gelangen, indem er nur auf gefrorenen Kacheln (F) läuft und Löcher (H) vermeidet.\n",
    "2. Ein autonomes Taxi: Unser Agent muss **lernen, sich in einer Stadt zurechtzufinden**, um **seine Fahrgäste von Punkt A nach Punkt B zu befördern**.\n",
    "\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/envs.gif\" alt=\"Umgebungen\"/>\n",
    "\n",
    "Konkret werden wir:\n",
    "\n",
    "- Lernen Sie **wertbasierte Methoden** kennen.\n",
    "- Lernen Sie die **Unterschiede zwischen Monte Carlo und Temporal Difference Learning** kennen.\n",
    "- Studium und Implementierung **unseres ersten RL-Algorithmus**: Q-Lernen.\n",
    "\n",
    "Diese Einheit ist **grundlegend, wenn Sie in der Lage sein wollen, an Deep Q-Learning** zu arbeiten: der erste Deep RL-Algorithmus, der Atari-Spiele spielte und bei einigen von ihnen das menschliche Niveau schlug (Breakout, Space Invaders, etc.).\n",
    "\n",
    "Also lasst uns anfangen! 🚀\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Was ist RL? Eine kurze Zusammenfassung\n",
    "\n",
    "In RL bauen wir einen Agenten, der **kluge Entscheidungen** treffen kann. Zum Beispiel einen Agenten, der **lernt, ein Videospiel zu spielen** oder einen Handelsagenten, der **lernt, seinen Gewinn zu maximieren**, indem er entscheidet, **welche Aktien er kaufen und wann er verkaufen soll**.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/rl-process.jpg\" alt=\"RL-Prozess\"/>\n",
    "\n",
    "\n",
    "Um intelligente Entscheidungen zu treffen, lernt unser Agent von der Umwelt, indem er **durch Versuch und Irrtum** mit ihr interagiert und (positive oder negative) Belohnungen **als einzigartiges Feedback** erhält.\n",
    "\n",
    "Sein Ziel **ist es, seine erwartete kumulative Belohnung** zu maximieren (aufgrund der Belohnungshypothese).\n",
    "\n",
    "**Der Entscheidungsprozess des Agenten wird als Policy π bezeichnet:** In einem gegebenen Zustand gibt eine Policy eine Aktion oder eine Wahrscheinlichkeitsverteilung über Aktionen aus. Das heißt, bei einer Beobachtung der Umgebung gibt eine Strategie eine Aktion (oder mehrere Wahrscheinlichkeiten für jede Aktion) vor, die der Agent ausführen sollte.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/policy.jpg\" alt=\"Policy\"/>\n",
    "\n",
    "**Unser Ziel ist es, eine optimale Strategie π* ** zu finden, d. h. eine Strategie, die zur besten erwarteten kumulativen Belohnung führt.\n",
    "\n",
    "Und um diese optimale Policy zu finden (und damit das RL-Problem zu lösen), gibt es **zwei Haupttypen von RL-Methoden**:\n",
    "\n",
    "- *Policybasierte Methoden*: **Direktes Trainieren der Strategie**, um zu lernen, welche Aktion bei einem bestimmten Zustand zu ergreifen ist.\n",
    "- *Wertbasierte Methoden*: **Trainieren eine Wertfunktion**, um zu lernen, **welcher Zustand wertvoller ist**, und verwenden diese Wertfunktion **um die Aktion zu ergreifen, die zu diesem Zustand führt**.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/two-approaches.jpg\" alt=\"Zwei RL-Ansätze\"/>\n",
    "\n",
    "Und in dieser Einheit **werden wir tiefer in die wertbasierten Methoden eintauchen**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Die Bellman-Gleichung: Vereinfachung der Wertberechnung\n",
    "\n",
    "Die Bellman-Gleichung **vereinfacht unsere Zustandswert- oder Zustands-Aktionswert-Berechnung**.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/bellman.jpg\" alt=\"Bellman equation\"/>\n",
    "\n",
    "Mit dem, was wir bisher gelernt haben, wissen wir, dass wir, wenn wir \\\\(V(S_t)\\\\) (den Wert eines Zustands) berechnen, die Belohnung ab diesem Zustand berechnen und dann die Strategie für immer weiter verfolgen müssen. **(Die Policy, die wir im folgenden Beispiel definiert haben, ist eine Greedy-Policy; zur Vereinfachung lassen wir die Belohnung unberücksichtigt).\n",
    "\n",
    "Um also \\\\(V(S_t)\\\\) zu berechnen, müssen wir die Summe der erwarteten Belohnungen berechnen. Daraus folgt:\n",
    "<figure>\n",
    "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/bellman2.jpg\" alt=\"Bellman equation\"/>\n",
    "  <figcaption>To calculate the value of State 1: the sum of rewards if the agent started in that state and then followed the greedy policy (taking actions that leads to the best states values) for all the time steps.</figcaption>\n",
    "</figure>\n",
    "Zur Berechnung von \\\\(V(S_{t+1})\\\\) müssen wir dann die Belohnung ab diesem Zustand \\\\(S_{t+1}\\) berechnen.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/bellman3.jpg\" alt=\"Bellman equation\"/>\n",
    "  <figcaption>To calculate the value of State 2: the sum of rewards <b>if the agent started in that state</b>, and then followed the <b>policy for all the time steps.</b></figcaption>\n",
    "</figure>\n",
    "\n",
    "Sie haben vielleicht bemerkt, dass wir die Berechnung des Wertes der verschiedenen Zustände wiederholen, was mühsam sein kann, wenn Sie dies für jeden Zustandswert oder Zustandsaktionswert tun müssen.\n",
    "\n",
    "Anstatt die erwartete Belohnung für jeden Zustand oder jedes Zustands-Aktionspaar zu berechnen, **können wir die Bellman-Gleichung verwenden** (Hinweis: Wenn Sie wissen, was Dynamische Programmierung ist, ist dies sehr ähnlich! wenn Sie nicht wissen, was es ist, keine Sorge!)\n",
    "\n",
    "Die Bellman-Gleichung ist eine rekursive Gleichung, die wie folgt funktioniert: Anstatt für jeden Zustand von vorne zu beginnen und die Belohnung zu berechnen, können wir den Wert eines jeden Zustands als:\n",
    "\n",
    "**Die unmittelbare Belohnung \\\\(R_{t+1}\\) + der Rabattierte Wert des darauf folgenden Zustands ( \\\\(gamma * V(S_{t+1}) \\\\) ) .**\n",
    "\n",
    "<figure>\n",
    "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/bellman4.jpg\" alt=\"Bellman equation\"/>\n",
    "</figure>\n",
    "\n",
    "Wenn wir zu unserem Beispiel zurückkehren, können wir sagen, dass der Wert von Zustand 1 gleich der erwarteten kumulativen Belohnung ist, wenn wir bei diesem Zustand beginnen.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/bellman2.jpg\" alt=\"Bellman equation\"/>\n",
    "\n",
    "\n",
    "Um den Wert von Zustand 1 zu berechnen: die Summe der Belohnungen, **wenn der Agent in diesem Zustand 1** beginnen und dann die **Policy für alle Zeitschritte verfolgen würde.**\n",
    "\n",
    "Dies ist äquivalent zu \\\\(V(S_{t})\\\\) = Sofortige Belohnung \\\\(R_{t+1}\\\\) + Rabattierter Wert des nächsten Zustands \\\\(\\gamma * V(S_{t+1})\\\\\\)\n",
    "\n",
    "\n",
    "<figure>\n",
    "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/bellman6.jpg\" alt=\"Bellman equation\"/>\n",
    "  <figcaption>For simplification, here we don’t discount so gamma = 1.</figcaption>\n",
    "</figure>\n",
    "\n",
    "Der Einfachheit halber wird hier nicht Rabattiert, also ist gamma = 1.\n",
    "Sie werden jedoch im Abschnitt Q-Learning dieser Einheit ein Beispiel mit gamma = 0,99 untersuchen.\n",
    "\n",
    "- Der Wert von \\\\(V(S_{t+1}) \\\\) = unmittelbare Belohnung \\\\(R_{t+2}\\) + Rabattierter Wert des nächsten Zustands ( \\\\\\(gamma * V(S_{t+2})\\\\\\) ).\n",
    "- Und so weiter.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Zusammenfassend lässt sich sagen, dass die Idee der Bellman-Gleichung darin besteht, dass wir, anstatt jeden Wert als Summe der erwarteten Belohnung zu berechnen, **was ein langwieriger Prozess ist**, den Wert als **die Summe der unmittelbaren Belohnung + des Rabattierten Wertes des folgenden Zustands berechnen.**\n",
    "\n",
    "Bevor wir zum nächsten Abschnitt übergehen, denken Sie über die Rolle von Gamma in der Bellman-Gleichung nach. Was passiert, wenn der Wert von gamma sehr niedrig ist (z. B. 0,1 oder sogar 0)? Was passiert, wenn der Wert 1 ist? Was passiert, wenn der Wert sehr hoch ist, z. B. eine Million?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-mo_6rXIjRi"
   },
   "source": [
    "## Voraussetzungen 🏗️\n",
    "\n",
    "Bevor Sie sich mit dem Notebook beschäftigen, müssen Sie:\n",
    "\n",
    "🔲 📚 **Studieren Sie [Q-Learning by reading Unit 2](https://huggingface.co/deep-rl-course/unit2/introduction)** 🤗"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2ONOODsyrMU"
   },
   "source": [
    "## Eine kleine Zusammenfassung von Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V68VveLacfxJ"
   },
   "source": [
    "*Q-Learning* **ist der RL-Algorithmus, der**:\n",
    "\n",
    "- Eine *Q-Funktion* trainiert, eine **Aktionswertfunktion**, die im internen Speicher durch eine *Q-Tabelle* **kodiert wird, die alle Werte der Zustands-Aktionspaare enthält**.\n",
    "\n",
    "- Wenn ein Zustand und eine Aktion gegeben sind, sucht unsere Q-Funktion **in der Q-Tabelle nach dem entsprechenden Wert**.\n",
    "    \n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-function-2.jpg\" alt=\"Q-Funktion\" width=\"100%\"/>\n",
    "\n",
    "- Wenn das Training abgeschlossen ist, **haben wir eine optimale Q-Funktion, also eine optimale Q-Tabelle.**\n",
    "    \n",
    "- Und wenn wir **eine optimale Q-Funktion** haben, haben wir\n",
    "haben wir eine optimale Policy, da wir **für jeden Zustand die beste Aktion kennen, die zu ergreifen ist.**\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/link-value-policy.jpg\" alt=\"Link value policy\" width=\"100%\"/>\n",
    "\n",
    "\n",
    "Aber am Anfang ist unsere **Q-Tabelle nutzlos, da sie für jedes Zustands-Aktionspaar einen beliebigen Wert angibt (meistens initialisieren wir die Q-Tabelle mit 0 Werten)**. Aber wenn wir die Umgebung erkunden und unsere Q-Tabelle aktualisieren, wird sie uns immer bessere Annäherungen liefern\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/q-learning.jpeg\" alt=\"q-learning.jpeg\" width=\"100%\"/>\n",
    "\n",
    "Dies ist der Pseudocode für das Q-Learning:\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg\" alt=\"Q-Learning\" width=\"100%\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo vs. Temporales Differenzlernen\n",
    "\n",
    "Der letzte Punkt, den wir besprechen müssen, bevor wir uns dem Q-Learning zuwenden, sind die beiden Lernstrategien.\n",
    "\n",
    "Denken Sie daran, dass ein RL-Agent **durch die Interaktion mit seiner Umgebung lernt.** Die Idee ist, dass **der Agent aufgrund der Erfahrung und der erhaltenen Belohnung seine Wertfunktion oder Strategie aktualisiert.**\n",
    "\n",
    "Monte Carlo und Temporal Difference Learning sind zwei verschiedene **Strategien, wie wir unsere Wertfunktion oder unsere Policy-Funktion trainieren können.** Beide **nutzen Erfahrung, um das RL-Problem zu lösen.**\n",
    "\n",
    "Auf der einen Seite verwendet Monte Carlo **eine ganze Episode an Erfahrung, bevor es lernt.** Auf der anderen Seite verwendet Temporal Difference **nur einen Schritt ( \\\\(S_t, A_t, R_{t+1}, S_{t+1}\\) ) zum Lernen.**\n",
    "\n",
    "Wir werden beide Methoden anhand eines Beispiels für eine wertbasierte Methode erläutern.\n",
    "\n",
    "## Monte Carlo: Lernen am Ende der Episode\n",
    "\n",
    "Monte Carlo wartet bis zum Ende der Episode, berechnet \\\\(G_t\\\\) (Rückgabe) und verwendet es als **Ziel für die Aktualisierung von \\\\(V(S_t)\\\\).**\n",
    "\n",
    "Es ist also eine **vollständige Episode der Interaktion erforderlich, bevor wir unsere Wertfunktion aktualisieren können.**\n",
    "\n",
    "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/monte-carlo-approach.jpg\" alt=\"Monte Carlo\"/>\n",
    "\n",
    "\n",
    "Wenn wir ein Beispiel nehmen:\n",
    "\n",
    "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/MC-2.jpg\" alt=\"Monte Carlo\"/>\n",
    "\n",
    "\n",
    "- Wir beginnen die Episode immer **am gleichen Startpunkt.**\n",
    "- **Der Agent unternimmt Aktionen unter Verwendung der Policy**. Zum Beispiel mit einer Epsilon Greedy Strategy, einer Strategie, die zwischen Exploration (zufällige Aktionen) und Ausbeutung wechselt.\n",
    "- Wir erhalten **die Belohnung und den nächsten Zustand**.\n",
    "- Wir beenden die Episode, wenn die Katze die Maus frisst oder wenn sich die Maus > 10 Schritte bewegt.\n",
    "\n",
    "- Am Ende der Episode haben wir **eine Liste von Zustands-, Aktions-, Belohnungs- und Folgezustands-Tupeln**\n",
    "Zum Beispiel [[Zustandskachel 3 unten, Nach links gehen, +1, Zustandskachel 2 unten], [Zustandskachel 2 unten, Nach links gehen, +0, Zustandskachel 1 unten]...]\n",
    "\n",
    "- **Der Agent wird die Gesamtbelohnungen \\\\(G_t\\\\)** addieren (um zu sehen, wie gut er abgeschnitten hat).\n",
    "- Dann **aktualisiert er \\\\(V(s_t)\\\\) anhand der Formel**\n",
    "\n",
    "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/MC-3.jpg\" alt=\"Monte Carlo\"/>\n",
    "\n",
    "- Dann **starten Sie ein neues Spiel mit diesem neuen Wissen**\n",
    "\n",
    "Indem man mehr und mehr Episoden durchführt, **lernt der Agent, immer besser zu spielen**.\n",
    "\n",
    "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/MC-3p.jpg\" alt=\"Monte Carlo\"/>\n",
    "\n",
    "Wenn wir zum Beispiel eine Zustandswertfunktion mit Monte Carlo trainieren:\n",
    "\n",
    "- Wir initialisieren unsere Wertfunktion **so, dass sie für jeden Zustand den Wert 0 liefert**\n",
    "- Unsere Lernrate (lr) ist 0,1 und unsere Abzinsungsrate ist 1 (= keine Abzinsung)\n",
    "- Unsere Maus **erforscht die Umgebung und führt zufällige Aktionen aus**\n",
    "\n",
    "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/MC-4.jpg\" alt=\"Monte Carlo\"/>\n",
    "\n",
    "\n",
    "- Die Maus hat mehr als 10 Schritte gemacht, also endet die Episode .\n",
    "\n",
    "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/MC-4p.jpg\" alt=\"Monte Carlo\"/>\n",
    "\n",
    "\n",
    "\n",
    "- Wir haben eine Liste von Zustand, Aktion, Belohnungen, nächster_Zustand, **wir müssen die Rückgabe berechnen \\\\(G{t=0}\\)**\n",
    "\n",
    "\\\\(G_t = R_{t+1} + R_{t+2} + R_{t+3} ...\\\\\\) (der Einfachheit halber lassen wir die Belohnungen unberücksichtigt)\n",
    "\n",
    "\\\\(G_0 = R_{1} + R_{2} + R_{3}...\\\\\\)\n",
    "\n",
    "\\\\(G_0 = 1 + 0 + 0 + 0 + 0 + 0 + 1 + 1 + 0 + 0\\\\)\n",
    "\n",
    "\\\\(G_0 = 3\\\\)\n",
    "\n",
    "- Wir können nun das **neue** \\\\(V(S_0)\\\\) berechnen:\n",
    "\n",
    "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/MC-5.jpg\" alt=\"Monte Carlo\"/>\n",
    "\n",
    "\\\\(V(S_0) = V(S_0) + lr * [G_0 - V(S_0)]\\\\)\n",
    "\n",
    "\\\\(V(S_0) = 0 + 0,1 * [3 - 0]\\\\)\n",
    "\n",
    "\\\\(V(S_0) = 0,3\\\\)\n",
    "\n",
    "\n",
    "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/MC-5p.jpg\" alt=\"Monte Carlo\"/>\n",
    "\n",
    "\n",
    "## Temporal Difference Learning: Lernen bei jedem Schritt \n",
    "\n",
    "**Temporal Difference hingegen wartet nur auf eine Interaktion (einen Schritt) \\\\\\(S_{t+1}\\\\)**, um ein TD-Ziel zu bilden und \\\\(V(S_t)\\\\) unter Verwendung von \\\\\\(R_{t+1}\\) und \\\\\\( \\gamma * V(S_{t+1})\\\\\\) zu aktualisieren.\n",
    "\n",
    "Die Idee bei **TD ist, das \\\\(V(S_t)\\\\) bei jedem Schritt zu aktualisieren.\n",
    "\n",
    "Da wir aber nicht eine ganze Episode erlebt haben, verfügen wir nicht über \\\\(G_t\\\\) (erwartete Belohnung). Stattdessen **schätzen wir \\\\(G_t\\\\), indem wir \\\\(R_{t+1}\\) und den Rabattierten Wert des nächsten Zustands addieren.**\n",
    "\n",
    "Dies wird Bootstrapping genannt. Es wird so genannt, **weil TD seine Aktualisierung zum Teil auf eine bestehende Schätzung \\\\(V(S_{t+1})\\\\) und nicht auf eine vollständige Stichprobe \\\\(G_t\\\\) stützt.**\n",
    "\n",
    "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/TD-1.jpg\" alt=\"Temporal Difference\"/>\n",
    "\n",
    "\n",
    "Diese Methode wird TD(0) oder **einschrittige TD (Aktualisierung der Wertfunktion nach jedem einzelnen Schritt) genannt.**\n",
    "\n",
    "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/TD-1p.jpg\" alt=\"Temporal Difference\"/>\n",
    "\n",
    "Wenn wir das gleiche Beispiel nehmen,\n",
    "\n",
    "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/TD-2.jpg\" alt=\"Temporal Difference\"/>\n",
    "\n",
    "- Wir initialisieren unsere Wertfunktion so, dass sie für jeden Zustand den Wert 0 zurückgibt.\n",
    "- Unsere Lernrate (lr) ist 0,1, und unsere Abzinsungsrate ist 1 (keine Abzinsung).\n",
    "- Unsere Maus beginnt, die Umgebung zu erkunden und führt eine zufällige Aktion aus: **nach links gehen**\n",
    "- Sie erhält eine Belohnung \\\\(R_{t+1} = 1\\\\), da **sie ein Stück Käse isst**\n",
    "\n",
    "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/TD-2p.jpg\" alt=\"Temporal Difference\"/>\n",
    "\n",
    "\n",
    "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/TD-3.jpg\" alt=\"Temporal Difference\"/>\n",
    "\n",
    "Wir können nun \\\\(V(S_0)\\\\) aktualisieren:\n",
    "\n",
    "Neu \\\\(V(S_0) = V(S_0) + lr * [R_1 + \\gamma * V(S_1) - V(S_0)]\\\\)\n",
    "\n",
    "Neu \\\\(V(S_0) = 0 + 0,1 * [1 + 1 * 0-0]\\\\)\n",
    "\n",
    "Neu \\\\(V(S_0) = 0,1\\\\)\n",
    "\n",
    "Wir haben also gerade unsere Wertfunktion für den Zustand 0 aktualisiert.\n",
    "\n",
    "Jetzt fahren wir damit fort, mit unserer aktualisierten Wertfunktion mit dieser Umgebung zu interagieren.\n",
    "\n",
    "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/TD-3p.jpg\" alt=\"Temporal Difference\"/>\n",
    "\n",
    "  Um es zusammenzufassen:\n",
    "\n",
    "  - Mit *Monte Carlo* aktualisieren wir die Wertfunktion aus einer kompletten Episode, und so **verwenden wir die tatsächliche genaue Rabattierte Belohnung dieser Episode.**\n",
    "  - Mit *TD Learning* aktualisieren wir die Wertfunktion aus einem Schritt, und wir ersetzen \\\\(G_t\\\\), das wir nicht kennen, mit **einer geschätzten Belohnung, die TD-Ziel genannt wird.**\n",
    "\n",
    "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Summary.jpg\" alt=\"Zusammenfassung\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mid-way Recap\n",
    "Bevor wir uns mit Q-Learning beschäftigen, sollten wir das eben Gelernte noch einmal zusammenfassen.\n",
    "\n",
    "Wir haben zwei Arten von wertbasierten Funktionen:\n",
    "\n",
    "- Zustandswertfunktion: gibt den erwarteten Ertrag aus, wenn **der Agent in einem bestimmten Zustand startet und danach für immer gemäß der Strategie handelt**.\n",
    "- Aktionswertfunktion: gibt den erwarteten Ertrag aus, wenn **der Agent in einem bestimmten Zustand startet, in diesem Zustand eine bestimmte Aktion ausführt** und danach für immer entsprechend der Strategie handelt.\n",
    "- Bei wertbasierten Methoden wird die Strategie nicht erlernt, sondern **man definiert die Strategie von Hand** und lernt eine Wertfunktion. Wenn wir eine optimale Wertfunktion haben, haben wir **eine optimale Strategie**.\n",
    "\n",
    "Es gibt zwei Arten von Methoden zur Aktualisierung der Wertfunktion:\n",
    "\n",
    "- Bei der *Monte-Carlo-Methode* aktualisieren wir die Wertfunktion anhand einer vollständigen Episode, d. h. wir **verwenden die tatsächliche Rabattierte Belohnung dieser Episode**.\n",
    "- Mit *der TD-Learning-Methode* aktualisieren wir die Wertfunktion aus einem Schritt, wobei wir die unbekannte Belohnung durch **eine geschätzte Belohnung, die TD-Zielvorgabe, ersetzen.**\n",
    "\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/summary-learning-mtds.jpg\" alt=\"Zusammenfassung\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Einführung in das Q-Learning \n",
    "## Was ist Q-Learning?\n",
    "\n",
    "Q-Learning ist eine **off-policy wertbasierte Methode, die einen TD-Ansatz verwendet, um ihre Aktionswertfunktion zu trainieren:**\n",
    "\n",
    "- *Off-policy*: wir werden am Ende dieser Einheit darüber sprechen.\n",
    "- *Wertbasierte Methode*: Findet die optimale Policy indirekt durch das Training einer Wert- oder Aktionswertfunktion, die uns **den Wert jedes Zustands oder jedes Zustands-Aktionspaares* liefert.\n",
    "- *TD-Ansatz:* **Aktualisiert seine Aktions-Wert-Funktion bei jedem Schritt statt am Ende der Episode.**\n",
    "\n",
    "**Q-Learning ist der Algorithmus, mit dem wir unsere Q-Funktion** trainieren, eine **Aktionswertfunktion**, die den Wert eines bestimmten Zustands und einer bestimmten Aktion in diesem Zustand bestimmt.\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-function.jpg\" alt=\"Q-function\"/>\n",
    "  <figcaption>Given a state and action, our Q Function outputs a state-action value (also called Q-value)</figcaption>\n",
    "</figure>\n",
    "Das **Q kommt von \"der Qualität\" (dem Wert) dieser Handlung in diesem Zustand.\n",
    "\n",
    "Erinnern wir uns an den Unterschied zwischen Wert und Belohnung:\n",
    "\n",
    "- Der *Wert eines Zustands* oder eines *Zustands-Aktions-Paars* ist die erwartete kumulative Belohnung, die unser Agent erhält, wenn er in diesem Zustand (oder Zustands-Aktions-Paar) startet und dann entsprechend seiner Strategie handelt.\n",
    "- Die *Belohnung* ist die **Rückmeldung, die ich von der Umwelt erhalte**, nachdem ich eine Aktion in einem Zustand ausgeführt habe.\n",
    "\n",
    "Intern wird unsere Q-Funktion durch **eine Q-Tabelle kodiert, eine Tabelle, in der jede Zelle einem Wert für ein Zustands-Aktionspaar entspricht.** Stellen Sie sich diese Q-Tabelle als **den Speicher oder Spickzettel unserer Q-Funktion** vor.\n",
    "\n",
    "Gehen wir ein Beispiel für ein Labyrinth durch.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Maze-1.jpg\" alt=\"Labyrinth Beispiel\"/>\n",
    "\n",
    "Die Q-Tabelle wird initialisiert. Deshalb sind alle Werte = 0. Diese Tabelle **enthält für jeden Zustand und jede Aktion die entsprechenden Zustands-Aktions-Werte.** \n",
    "In diesem einfachen Beispiel ist der Zustand nur durch die Position der Maus definiert. Daher haben wir 2*3 Zeilen in unserer Q-Tabelle, eine Zeile für jede mögliche Position der Maus. In komplexeren Szenarien könnte der Zustand mehr Informationen als die Position des Akteurs enthalten.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Maze-2.jpg\" alt=\"Maze example\"/>\n",
    "\n",
    "Hier sehen wir, dass der **Zustandsaktionswert des Ausgangszustands und des Aufstiegs 0 ist:**\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Maze-3.jpg\" alt=\"Labyrinth-Beispiel\"/>\n",
    "\n",
    "Also: Die Q-Funktion verwendet eine Q-Tabelle, **die den Wert jedes Zustands-Aktions-Paares enthält.** Gegeben einen Zustand und eine Aktion, **sucht unsere Q-Funktion in ihrer Q-Tabelle, um den Wert auszugeben.**\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-function-2.jpg\" alt=\"Q-function\"/>\n",
    "</figure>\n",
    "\n",
    "Wenn wir rekapitulieren, ist *Q-Lernen* **der RL-Algorithmus, der:**\n",
    "\n",
    "- eine *Q-Funktion* (eine **Aktionswertfunktion**) trainiert, die intern eine **Q-Tabelle ist, die alle Werte des Zustands-Aktionspaares enthält.**\n",
    "- Wenn ein Zustand und eine Aktion gegeben sind, **sucht unsere Q-Funktion in ihrer Q-Tabelle nach dem entsprechenden Wert**.\n",
    "- Wenn das Training abgeschlossen ist, **haben wir eine optimale Q-Funktion, was bedeutet, dass wir eine optimale Q-Tabelle haben.**\n",
    "- Und wenn wir **eine optimale Q-Funktion** haben, haben wir **eine optimale Policy**, da wir **die beste Aktion für jeden Zustand kennen.**\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/link-value-policy.jpg\" alt=\"Link value policy\"/>\n",
    "\n",
    "\n",
    "Am Anfang ist **unsere Q-Tabelle nutzlos, da sie beliebige Werte für jedes Zustands-Aktions-Paar enthält** (meistens initialisieren wir die Q-Tabelle mit 0). Wenn der Agent **die Umgebung erkundet und wir die Q-Tabelle aktualisieren, wird sie uns eine immer bessere Annäherung** an die optimale Strategie liefern.\n",
    "\n",
    "<figure class=\"image table text-center m-0 w-full\">\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-1.jpg\" alt=\"Q-learning\"/>\n",
    "  <figcaption>We see here that with the training, our Q-table is better since, thanks to it, we can know the value of each state-action pair.</figcaption>\n",
    "</figure>\n",
    "\n",
    "Da wir nun wissen, was Q-Learning, Q-Funktionen und Q-Tabellen sind, **können wir uns nun näher mit dem Q-Learning-Algorithmus** beschäftigen.\n",
    "\n",
    "## Der Q-Learning-Algorithmus [[q-learning-algo]]\n",
    "\n",
    "Dies ist der Pseudocode des Q-Learning-Algorithmus; lassen Sie uns jeden Teil studieren und **an einem einfachen Beispiel sehen, wie er funktioniert, bevor wir ihn implementieren**. Lassen Sie sich davon nicht einschüchtern, es ist einfacher als es aussieht! Wir werden jeden Schritt durchgehen.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg\" alt=\"Q-learning\"/>\n",
    "\n",
    "### Schritt 1: Wir initialisieren die Q-Tabelle [[Schritt1]]\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-3.jpg\" alt=\"Q-learning\"/>\n",
    "\n",
    "\n",
    "Wir müssen die Q-Tabelle für jedes Zustands-Aktionspaar initialisieren. ** Meistens initialisieren wir mit Werten von 0.\n",
    "\n",
    "### Schritt 2: Wählen Sie eine Aktion mit Hilfe der Epsilon-Greedy-Strategie [[step2]]\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-4.jpg\" alt=\"Q-learning\"/>\n",
    "\n",
    "\n",
    "Die Epsilon-Greedy-Strategie ist eine Strategie, die den Kompromiss zwischen Erkundung und Ausbeutung behandelt.\n",
    "\n",
    "Die Idee ist, dass bei einem Anfangswert von ɛ = 1,0:\n",
    "\n",
    "- Mit einer Wahrscheinlichkeit von 1 - ɛ*: **Ausbeutung** (d. h. unser Agent wählt die Aktion mit dem höchsten Wert des Zustands-Aktionspaares).\n",
    "- Mit der Wahrscheinlichkeit ɛ: **wir machen Exploration** (wir versuchen eine zufällige Aktion).\n",
    "\n",
    "Zu Beginn des Trainings wird **die Wahrscheinlichkeit der Exploration sehr groß sein, da ɛ sehr hoch ist, also werden wir die meiste Zeit explorieren**. Aber wenn das Training weitergeht und folglich unsere **Q-Tabelle in ihren Schätzungen immer besser wird, verringern wir schrittweise den Epsilon-Wert**, da wir immer weniger Exploration und mehr Ausbeutung brauchen werden.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-5.jpg\" alt=\"Q-learning\"/>\n",
    "\n",
    "\n",
    "### Schritt 3: Führe Aktion At aus, erhalte Belohnung Rt+1 und nächsten Zustand St+1 [[Schritt3]]\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-6.jpg\" alt=\"Q-learning\"/>\n",
    "\n",
    "### Schritt 4: Aktualisiere Q(St, At) [[Schritt4]]\n",
    "\n",
    "Erinnern Sie sich daran, dass wir beim TD-Lernen unsere Policy oder Wertfunktion (je nach der von uns gewählten RL-Methode) **nach einem Schritt der Interaktion** aktualisieren.\n",
    "\n",
    "Um unser TD-Ziel zu erzeugen, **benutzen wir die unmittelbare Belohnung \\\\(R_{t+1}\\) plus den Rabattierten Wert des nächsten Zustands**, der berechnet wird, indem wir die Aktion finden, die die aktuelle Q-Funktion im nächsten Zustand maximiert. (Wir nennen das Bootstrap).\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-7.jpg\" alt=\"Q-learning\"/>\n",
    "\n",
    "Daher lautet unsere \\\\(Q(S_t, A_t)\\\\) **Aktualisierungsformel wie folgt aus: **\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-8.jpg\" alt=\"Q-learning\"/>\n",
    "\n",
    "\n",
    "Das heißt, um unser \\\\(Q(S_t, A_t)\\\\) zu aktualisieren:\n",
    "\n",
    "- Wir brauchen \\\\(S_t, A_t, R_{t+1}, S_{t+1}\\\\).\n",
    "- Um unseren Q-Wert bei einem bestimmten Zustands-Aktionspaar zu aktualisieren, verwenden wir das TD-Ziel.\n",
    "\n",
    "Wie bilden wir das TD-Ziel?\n",
    "1. Wir erhalten die Belohnung \\\\(R_{t+1}\\), nachdem wir die Aktion \\\\(A_t\\\\) ausgeführt haben.\n",
    "2. Um den **besten Wert des Zustands-Aktionspaares** für den nächsten Zustand zu erhalten, verwenden wir eine gierige Strategie, um die nächstbeste Aktion auszuwählen. Beachten Sie, dass es sich hierbei nicht um eine Epsilon-Greedy-Policy handelt, die immer die Aktion mit dem höchsten Zustands-Aktionswert wählt.\n",
    "\n",
    "Wenn die Aktualisierung dieses Q-Wertes abgeschlossen ist, beginnen wir in einem neuen Zustand und wählen unsere Aktion **wieder mit einer Epsilon-Greedy-Strategie aus.**\n",
    "\n",
    "**Aus diesem Grund sagen wir, dass Q-Learning ein Off-Policy-Algorithmus ist.**\n",
    "\n",
    "## Off-Policy vs. On-Policy [[off-vs-on]]\n",
    "\n",
    "Der Unterschied ist subtil:\n",
    "\n",
    "- *Off-policy*: Verwendung **einer anderen Policy für das Handeln (Inferenz) und Aktualisieren (Training).**\n",
    "\n",
    "Beim Q-Learning beispielsweise unterscheidet sich die Epsilon-Greedy-Policy (acting policy) von der Greedy-Policy, die **zur Auswahl des besten Aktionswerts für den nächsten Zustand verwendet wird, um unseren Q-Wert zu aktualisieren (updating policy).**\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/off-on-1.jpg\" alt=\"Off-on policy\"/>\n",
    "  <figcaption>Acting Policy</figcaption>\n",
    "</figure>\n",
    "\n",
    "Unterscheidet sich von der Richtlinie, die wir während des Trainingsteils verwenden:\n",
    "\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/off-on-2.jpg\" alt=\"Off-on policy\"/>\n",
    "  <figcaption>Updating policy</figcaption>\n",
    "</figure>\n",
    "\n",
    "- *On-Policy:* Verwendung derselben **Policy zum Handeln und Aktualisieren.\n",
    "\n",
    "Bei Sarsa, einem anderen wertbasierten Algorithmus, wählt beispielsweise **die Epsilon-Greedy-Policy das nächste State-Action-Paar aus, nicht eine Greedy-Policy.**\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/off-on-3.jpg\" alt=\"Off-on policy\"/>\n",
    "    <figcaption>Sarsa</figcaption>\n",
    "</figure>\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/off-on-4.jpg\" alt=\"Off-on policy\"/>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ein Q-Learning-Beispiel [[q-learning-example]]\n",
    "\n",
    "Um Q-Learning besser zu verstehen, wollen wir ein einfaches Beispiel nehmen:\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Maze-Example-2.jpg\" alt=\"Labyrinth-Beispiel\"/>\n",
    "\n",
    "- Du bist eine Maus in diesem kleinen Labyrinth. Du **begannst immer am gleichen Startpunkt**.\n",
    "- Das Ziel ist es, **den großen Käsehaufen in der rechten unteren Ecke** zu fressen und das Gift zu vermeiden. Denn wer mag schon keinen Käse?\n",
    "- Die Episode endet, wenn wir das Gift essen, **den großen Käsestapel essen** oder wenn wir mehr als fünf Schritte gehen.\n",
    "- Die Lernrate beträgt 0,1\n",
    "- Die Rabattierungsrate (Gamma) ist 0,99\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/q-ex-1.jpg\" alt=\"Labyrinth-Beispiel\"/>\n",
    "\n",
    "\n",
    "Die Belohnungsfunktion sieht folgendermaßen aus:\n",
    "\n",
    "- **+0:** Erreichen eines Zustands, in dem kein Käse liegt.\n",
    "- **+1:** Erreichen eines Zustands mit einem kleinen Käse darin.\n",
    "- **+10:** Erreichen des Zustands mit dem großen Käsehaufen.\n",
    "- **-10:** In den Staat mit dem Gift gehen und somit sterben.\n",
    "- **+0** Wenn wir mehr als fünf Schritte gehen.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/q-ex-2.jpg\" alt=\"Labyrinth-Beispiel\"/>\n",
    "\n",
    "Um unseren Agenten auf eine optimale Strategie zu trainieren (also eine Strategie, die nach rechts, rechts, unten geht), **werden wir den Q-Learning-Algorithmus** verwenden.\n",
    "\n",
    "## Schritt 1: Initialisieren der Q-Tabelle [[Schritt1]]\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Example-1.jpg\" alt=\"Maze-Beispiel\"/>\n",
    "\n",
    "Für den Moment ist **unsere Q-Tabelle nutzlos**; wir müssen **unsere Q-Funktion mit dem Q-Learning-Algorithmus trainieren**.\n",
    "\n",
    "Tun wir dies für 2 Trainingszeitschritte:\n",
    "\n",
    "Trainingszeitschritt 1:\n",
    "\n",
    "## Schritt 2: Wählen Sie eine Aktion mit der Epsilon Greedy Strategy [[step2]]\n",
    "\n",
    "Da Epsilon groß ist (= 1,0), wähle ich eine zufällige Aktion. In diesem Fall gehe ich nach rechts.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/q-ex-3.jpg\" alt=\"Maze-Beispiel\"/>\n",
    "\n",
    "\n",
    "## Schritt 3: Führe Aktion At aus, erhalte Rt+1 und St+1 [[Schritt3]]\n",
    "\n",
    "Wenn ich nach rechts gehe, bekomme ich einen kleinen Käse, also R_{t+1} = 1\\\\) und ich bin in einem neuen Zustand.\n",
    "\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/q-ex-4.jpg\" alt=\"Labyrinth-Beispiel\"/>\n",
    "\n",
    "\n",
    "## Schritt 4: Q(St, At) aktualisieren [[Schritt4]]\n",
    "\n",
    "Wir können nun \\\\(Q(S_t, A_t)\\\\) mit unserer Formel aktualisieren.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/q-ex-5.jpg\" alt=\"Maze-Beispiel\"/>\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Example-4.jpg\" alt=\"Labyrinth-Beispiel\"/>\n",
    "\n",
    "Trainingszeitschritt 2:\n",
    "\n",
    "## Schritt 2: Wähle eine Aktion mit der Epsilon Greedy Strategy [[step2-2]]\n",
    "\n",
    "**Ich nehme wieder eine zufällige Aktion, da epsilon=0,99 groß ist**. (Beachten Sie, dass wir epsilon ein wenig vermindern, da wir mit fortschreitendem Training immer weniger erforschen wollen).\n",
    "\n",
    "Ich habe die Aktion 'nach unten' gewählt. **Dies ist keine gute Aktion, da sie mich zum Gift führt**.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/q-ex-6.jpg\" alt=\"Labyrinth-Beispiel\"/>\n",
    "\n",
    "\n",
    "## Schritt 3: Führe Aktion At aus, erhalte Rt+1 und St+1 [[Schritt3-3]]\n",
    "\n",
    "Da ich Gift gegessen habe, bekomme ich **(R_{t+1} = -10}), und ich sterbe**.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/q-ex-7.jpg\" alt=\"Labyrinth-Beispiel\"/>\n",
    "\n",
    "## Schritt 4: Q(St, At) aktualisieren [[step4-4]]\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/q-ex-8.jpg\" alt=\"Labyrinth-Beispiel\"/>\n",
    "\n",
    "Weil wir tot sind, beginnen wir eine neue Episode. Aber was wir hier sehen, ist, dass **mit zwei Erkundungsschritten mein Agent schlauer geworden ist.**\n",
    "\n",
    "Wenn wir weiterhin die Umgebung erforschen und ausnutzen und die Q-Werte anhand des TD-Ziels aktualisieren, wird uns die **Q-Tabelle eine immer bessere Annäherung liefern. Am Ende des Trainings erhalten wir eine Schätzung der optimalen Q-Funktion.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning Rekapitulation\n",
    "\n",
    "\n",
    "*Q-Learning* **ist der RL-Algorithmus, der** :\n",
    "\n",
    "- Eine *Q-Funktion* trainiert, eine **Aktionswertfunktion**, die im internen Speicher durch eine *Q-Tabelle* **kodiert ist, die alle Werte des Zustands-Aktionspaares enthält**.\n",
    "\n",
    "- Angesichts eines Zustands und einer Aktion sucht unsere Q-Funktion **in ihrer Q-Tabelle nach dem entsprechenden Wert**.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-function-2.jpg\" alt=\"Q-Funktion\" width=\"100%\"/>\n",
    "\n",
    "- Wenn das Training abgeschlossen ist, **haben wir eine optimale Q-Funktion, oder, äquivalent, eine optimale Q-Tabelle**.\n",
    "\n",
    "- Und wenn wir **eine optimale Q-Funktion** haben, haben wir\n",
    "haben wir eine optimale Policy, da wir **für jeden Zustand die beste Aktion kennen, die zu ergreifen ist.**\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/link-value-policy.jpg\" alt=\"Link value policy\" width=\"100%\"/>\n",
    "\n",
    "Am Anfang ist unsere **Q-Tabelle jedoch nutzlos, da sie für jedes Zustands-Aktionspaar beliebige Werte angibt (meistens initialisieren wir die Q-Tabelle mit 0 Werten)**. Aber wenn wir die Umgebung erforschen und unsere Q-Tabelle aktualisieren, wird sie uns eine immer bessere Annäherung liefern.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/q-learning.jpeg\" alt=\"q-learning.jpeg\" width=\"100%\"/>\n",
    "\n",
    "Dies ist der Pseudocode für Q-Learning:\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg\" alt=\"Q-Learning\" width=\"100%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HEtx8Y8MqKfH"
   },
   "source": [
    "# Programmieren wir unseren ersten Reinforcement Learning Algorithmus 🚀."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kdxb1IhzTn0v"
   },
   "source": [
    "Um dieses Hands-On für den [Zertifizierungsprozess] (https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process) zu validieren, müssen Sie Ihr trainiertes Taximodell an den Hub senden und **ein Ergebnis von >= 4,5** erhalten.\n",
    "\n",
    "Um Ihr Ergebnis zu finden, gehen Sie zur [Bestenliste] (https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) und suchen Sie Ihr Modell, **das Ergebnis = mean_reward - std of reward**\n",
    "\n",
    "Weitere Informationen über den Zertifizierungsprozess finden Sie in diesem Abschnitt 👉 https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4gpxC1_kqUYe"
   },
   "source": [
    "## Abhängigkeiten installieren und einen virtuellen Bildschirm erstellen 🔽.\n",
    "\n",
    "Im Notebook müssen wir ein Wiederholungsvideo erstellen. Dazu benötigen wir mit Colab **einen virtuellen Bildschirm, um die Umgebung zu rendern** (und somit die Bilder aufzunehmen).\n",
    "\n",
    "Daher wird die folgende Zelle die Bibliotheken installieren und einen virtuellen Bildschirm erstellen und starten 🖥\n",
    "\n",
    "Wir werden mehrere Bibliotheken installieren:\n",
    "\n",
    "- `gymnasium`: Enthält die Umgebungen FrozenLake-v1 ⛄ und Taxi-v3 🚕.\n",
    "- pygame\": Wird für die FrozenLake-v1- und Taxi-v3-Benutzeroberfläche verwendet.\n",
    "- `numpy`: Wird für die Handhabung unserer Q-Tabelle verwendet.\n",
    "\n",
    "Der Hugging Face Hub 🤗 dient als zentraler Ort, an dem jeder Modelle und Datensätze teilen und erforschen kann. Er bietet Versionierung, Metriken, Visualisierungen und andere Funktionen, die eine einfache Zusammenarbeit mit anderen ermöglichen.\n",
    "\n",
    "Sie können alle verfügbaren Deep-RL-Modelle (sofern sie Q Learning verwenden) hier sehen 👉 https://huggingface.co/models?other=q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9XaULfDZDvrC"
   },
   "outputs": [],
   "source": [
    "!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n71uTX7qqzz2"
   },
   "outputs": [],
   "source": [
    "!sudo apt-get update\n",
    "!sudo apt-get install -y python3-opengl\n",
    "!apt install ffmpeg xvfb\n",
    "!pip3 install pyvirtualdisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K6XC13pTfFiD"
   },
   "source": [
    "Um sicherzustellen, dass die neu installierten Bibliotheken verwendet werden, **ist es manchmal erforderlich, die Laufzeit des Notebooks neu zu starten**. Die nächste Zelle wird die **Laufzeitumgebung zum Absturz bringen, so dass Sie eine neue Verbindung herstellen und den Code von hier aus ausführen müssen**. Dank dieses Tricks **können wir unseren virtuellen Bildschirm ausführen**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3kuZbWAkfHdg"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DaY1N4dBrabi"
   },
   "outputs": [],
   "source": [
    "# Virtual display\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "virtual_display = Display(visible=0, size=(1400, 900))\n",
    "virtual_display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W-7f-Swax_9x"
   },
   "source": [
    "## Importieren Sie die Pakete 📦\n",
    "\n",
    "Zusätzlich zu den installierten Bibliotheken verwenden wir auch:\n",
    "\n",
    "- `random`: Um Zufallszahlen zu erzeugen (die für die Epsilon-Greedy-Policy nützlich sind).\n",
    "- `imageio`: Zur Erzeugung eines Wiederholungsvideos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VcNvOAQlysBJ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import random\n",
    "import imageio\n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "import pickle5 as pickle\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xp4-bXKIy1mQ"
   },
   "source": [
    "Wir sind jetzt bereit, unseren Q-Learning-Algorithmus zu programmieren 🔥."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xya49aNJWVvv"
   },
   "source": [
    "# Teil 1: Gefrorener See ⛄ (nicht rutschige Version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NAvihuHdy9tw"
   },
   "source": [
    "## Erstellen und verstehen [FrozenLake-Umgebung ⛄]((https://gymnasium.farama.org/environments/toy_text/frozen_lake/)\n",
    "---\n",
    "\n",
    "💡 Eine gute Angewohnheit, wenn Sie anfangen, eine Umgebung zu benutzen, ist es, ihre Dokumentation zu überprüfen\n",
    "\n",
    "👉 https://gymnasium.farama.org/environments/toy_text/frozen_lake/\n",
    "\n",
    "---\n",
    "\n",
    "Wir werden unseren Q-Learning-Agenten darauf trainieren, **vom Startzustand (S) zum Zielzustand (G) zu navigieren, indem wir nur auf gefrorenen Kacheln (F) laufen und Löcher (H)** vermeiden.\n",
    "\n",
    "Wir können zwei Größen von Umgebungen haben:\n",
    "\n",
    "- `map_name=\"4x4\"`: eine 4x4-Gitterversion\n",
    "- `map_name=\"8x8\"`: eine 8x8-Gitterversion\n",
    "\n",
    "\n",
    "Die Umgebung hat zwei Modi:\n",
    "\n",
    "- `is_slippery=False`: Der Agent bewegt sich immer **in die beabsichtigte Richtung**, da der gefrorene See nicht rutschig ist (deterministisch).\n",
    "- is_slippery=True`: Der Agent **bewegt sich aufgrund der glatten Beschaffenheit des gefrorenen Sees nicht immer in die beabsichtigte Richtung** (stochastisch)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UaW_LHfS0PY2"
   },
   "source": [
    "Für den Moment halten wir es einfach mit der 4x4 Karte und nicht rutschig.\n",
    "Wir fügen einen Parameter namens `render_mode` hinzu, der angibt, wie die Umgebung visualisiert werden soll. In unserem Fall, weil wir **am Ende ein Video der Umgebung aufnehmen wollen, müssen wir render_mode auf rgb_array** setzen.\n",
    "\n",
    "Wie [in der Dokumentation erklärt](https://gymnasium.farama.org/api/env/#gymnasium.Env.render) \"rgb_array\": Gibt ein einzelnes Bild zurück, das den aktuellen Zustand der Umgebung darstellt. Ein Frame ist ein np.ndarray mit der Form (x, y, 3), das RGB-Werte für ein x-mal-y-Pixelbild darstellt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IzJnb8O3y8up"
   },
   "outputs": [],
   "source": [
    "# Create the FrozenLake-v1 environment using 4x4 map and non-slippery version and render_mode=\"rgb_array\"\n",
    "env = gym.make() # TODO use the correct parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ji_UrI5l2zzn"
   },
   "source": [
    "### Lösung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jNxUbPMP0akP"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False, render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KASNViqL4tZn"
   },
   "source": [
    "Sie können Ihr eigenes benutzerdefiniertes Raster wie dieses erstellen:\n",
    "\n",
    "```python\n",
    "desc=[\"SFFF\", \"FHFH\", \"FFFH\", \"HFFG\"]\n",
    "gym.make('FrozenLake-v1', desc=desc, is_slippery=True)\n",
    "```\n",
    "\n",
    "aber wir werden vorerst die Standardumgebung verwenden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXbTfdeJ1Xi9"
   },
   "source": [
    "### Mal sehen, wie die Umwelt aussieht:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZNPG0g_UGCfh"
   },
   "outputs": [],
   "source": [
    "# We create our environment with gym.make(\"<name_of_the_environment>\")- `is_slippery=False`: The agent always moves in the intended direction due to the non-slippery nature of the frozen lake (deterministic).\n",
    "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
    "print(\"Observation Space\", env.observation_space)\n",
    "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2MXc15qFE0M9"
   },
   "source": [
    "Mit \"Observation Space Shape Discrete(16)\" sehen wir, dass die Beobachtung eine ganze Zahl ist, die die **aktuelle Position des Agenten als current_row * ncols + current_col darstellt (wobei sowohl row als auch col bei 0 beginnen)**.\n",
    "\n",
    "Zum Beispiel kann die Zielposition in der 4x4-Karte wie folgt berechnet werden: 3 * 4 + 3 = 15. Die Anzahl der möglichen Beobachtungen ist abhängig von der Größe der Karte. **Die 4x4-Karte hat zum Beispiel 16 mögliche Beobachtungen**.\n",
    "\n",
    "\n",
    "So sieht zum Beispiel der Zustand = 0 aus:\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/frozenlake.png\" alt=\"FrozenLake\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "We5WqOBGLoSm"
   },
   "outputs": [],
   "source": [
    "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
    "print(\"Action Space Shape\", env.action_space.n)\n",
    "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MyxXwkI2Magx"
   },
   "source": [
    "Der Aktionsraum (die Menge der möglichen Aktionen, die der Agent ausführen kann) ist diskret mit 4 verfügbaren Aktionen 🎮:\n",
    "- 0: LINKS GEHEN\n",
    "- 1: ABWÄRTS GEHEN\n",
    "- 2: RECHTS GEHEN\n",
    "- 3: NACH OBEN GEHEN\n",
    "\n",
    "Belohnungsfunktion 💰:\n",
    "- Erreichen des Ziels: +1\n",
    "- Loch erreichen: 0\n",
    "- Gefrorenes Ziel erreichen: 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1pFhWblk3Awr"
   },
   "source": [
    "## Erstellen und Initialisieren der Q-Tabelle 🗄️\n",
    "\n",
    "(👀 Schritt 1 des Pseudocodes)\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg\" alt=\"Q-Learning\" width=\"100%\"/>\n",
    "\n",
    "\n",
    "Es ist an der Zeit, unsere Q-Tabelle zu initialisieren! Um zu wissen, wie viele Zeilen (Zustände) und Spalten (Aktionen) wir verwenden sollen, müssen wir den Aktions- und Beobachtungsraum kennen. Wir kennen ihre Werte bereits von früher, aber wir wollen sie programmatisch erhalten, damit unser Algorithmus für verschiedene Umgebungen verallgemeinert werden kann. Gym bietet uns eine Möglichkeit, dies zu tun: `env.action_space.n` und `env.observation_space.n`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y3ZCdluj3k0l"
   },
   "outputs": [],
   "source": [
    "state_space =\n",
    "print(\"There are \", state_space, \" possible states\")\n",
    "\n",
    "action_space =\n",
    "print(\"There are \", action_space, \" possible actions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rCddoOXM3UQH"
   },
   "outputs": [],
   "source": [
    "# Let's create our Qtable of size (state_space, action_space) and initialized each values at 0 using np.zeros. np.zeros needs a tuple (a,b)\n",
    "def initialize_q_table(state_space, action_space):\n",
    "  Qtable =\n",
    "  return Qtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9YfvrqRt3jdR"
   },
   "outputs": [],
   "source": [
    "Qtable_frozenlake = initialize_q_table(state_space, action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67OdoKL63eDD"
   },
   "source": [
    "### Lösung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HuTKv3th3ohG"
   },
   "outputs": [],
   "source": [
    "state_space = env.observation_space.n\n",
    "print(\"There are \", state_space, \" possible states\")\n",
    "\n",
    "action_space = env.action_space.n\n",
    "print(\"There are \", action_space, \" possible actions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lnrb_nX33fJo"
   },
   "outputs": [],
   "source": [
    "# Let's create our Qtable of size (state_space, action_space) and initialized each values at 0 using np.zeros\n",
    "def initialize_q_table(state_space, action_space):\n",
    "  Qtable = np.zeros((state_space, action_space))\n",
    "  return Qtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y0WlgkVO3Jf9"
   },
   "outputs": [],
   "source": [
    "Qtable_frozenlake = initialize_q_table(state_space, action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Atll4Z774gri"
   },
   "source": [
    "## Definieren Sie die gierige Policy 🤖.\n",
    "\n",
    "Denken Sie daran, dass wir zwei Strategien haben, da Q-Learning ein **off-policy** Algorithmus ist. Das bedeutet, dass wir eine **unterschiedliche Strategie für das Handeln und die Aktualisierung der Wertfunktion** verwenden.\n",
    "\n",
    "- Epsilon-Greedy-Strategie (handelnde Strategie)\n",
    "- Greedy-Policy (AktualisierungsPolicy)\n",
    "\n",
    "Die Greedy-Policy wird auch die endgültige Policy sein, die wir haben, wenn der Q-Learning-Agent das Training abgeschlossen hat. Die Greedy-Policy wird verwendet, um eine Aktion anhand der Q-Tabelle auszuwählen.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/off-on-4.jpg\" alt=\"Q-Learning\" width=\"100%\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E3SCLmLX5bWG"
   },
   "outputs": [],
   "source": [
    "def greedy_policy(Qtable, state):\n",
    "  # Exploitation: take the action with the highest state, action value\n",
    "  action =\n",
    "\n",
    "  return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B2_-8b8z5k54"
   },
   "source": [
    "#### Lösung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "se2OzWGW5kYJ"
   },
   "outputs": [],
   "source": [
    "def greedy_policy(Qtable, state):\n",
    "  # Exploitation: take the action with the highest state, action value\n",
    "  action = np.argmax(Qtable[state][:])\n",
    "\n",
    "  return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "flILKhBU3yZ7"
   },
   "source": [
    "## Definieren Sie die Epsilon-Greedy-Policy 🤖.\n",
    "\n",
    "Epsilon-Greedy ist die Trainingsstrategie, die den Kompromiss zwischen Erkundung und Ausbeutung behandelt.\n",
    "\n",
    "Die Idee mit Epsilon-Greedy:\n",
    "\n",
    "- Mit *Wahrscheinlichkeit 1 - ɛ* : **wir machen Exploitation** (d.h. unser Agent wählt die Aktion mit dem höchsten Wert des Zustands-Aktionspaares).\n",
    "\n",
    "- Mit *Wahrscheinlichkeit ɛ*: **Exploration** (wir versuchen eine zufällige Aktion).\n",
    "\n",
    "Mit fortschreitendem Training wird der Epsilon-Wert schrittweise **verringert, da wir immer weniger Exploration und mehr Exploitation benötigen**.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-4.jpg\" alt=\"Q-Learning\" width=\"100%\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Bj7x3in3_Pq"
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(Qtable, state, epsilon):\n",
    "  # Randomly generate a number between 0 and 1\n",
    "  random_num =\n",
    "  # if random_num > greater than epsilon --> exploitation\n",
    "  if random_num > epsilon:\n",
    "    # Take the action with the highest value given a state\n",
    "    # np.argmax can be useful here\n",
    "    action =\n",
    "  # else --> exploration\n",
    "  else:\n",
    "    action = # Take a random action\n",
    "\n",
    "  return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8R5ej1fS4P2V"
   },
   "source": [
    "#### Lösung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cYxHuckr4LiG"
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(Qtable, state, epsilon):\n",
    "  # Randomly generate a number between 0 and 1\n",
    "  random_num = random.uniform(0,1)\n",
    "  # if random_num > greater than epsilon --> exploitation\n",
    "  if random_num > epsilon:\n",
    "    # Take the action with the highest value given a state\n",
    "    # np.argmax can be useful here\n",
    "    action = greedy_policy(Qtable, state)\n",
    "  # else --> exploration\n",
    "  else:\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "  return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hW80DealcRtu"
   },
   "source": [
    "## Definieren Sie die Hyperparameter ⚙️\n",
    "\n",
    "Die Hyperparameter, die sich auf die Exploration beziehen, gehören zu den wichtigsten Parametern.\n",
    "\n",
    "- Wir müssen sicherstellen, dass unser Agent **genug vom Zustandsraum erforscht**, um eine gute Wertannäherung zu lernen. Um dies zu erreichen, müssen wir einen progressiven Verfall von epsilon haben.\n",
    "- Wenn man epsilon zu schnell verringert (zu hohe decay_rate), **geht man das Risiko ein, dass der Agent feststeckt**, da er den Zustandsraum nicht ausreichend erforscht hat und daher das Problem nicht lösen kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y1tWn0tycWZ1"
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "n_training_episodes = 10000  # Total training episodes\n",
    "learning_rate = 0.7          # Learning rate\n",
    "\n",
    "# Evaluation parameters\n",
    "n_eval_episodes = 100        # Total number of test episodes\n",
    "\n",
    "# Environment parameters\n",
    "env_id = \"FrozenLake-v1\"     # Name of the environment\n",
    "max_steps = 99               # Max steps per episode\n",
    "gamma = 0.95                 # Discounting rate\n",
    "eval_seed = []               # The evaluation seed of the environment\n",
    "\n",
    "# Exploration parameters\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.05            # Minimum exploration probability\n",
    "decay_rate = 0.0005            # Exponential decay rate for exploration prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cDb7Tdx8atfL"
   },
   "source": [
    "## Erstellen Sie die Trainingsschleifenmethode\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg\" alt=\"Q-Learning\" width=\"100%\"/>\n",
    "\n",
    "Die Trainingsschleife sieht folgendermaßen aus:\n",
    "\n",
    "```\n",
    "Für eine Episode in der Gesamtheit der Trainingsepisoden:\n",
    "\n",
    "Epsilon reduzieren (da wir immer weniger Erkundung brauchen)\n",
    "Die Umgebung zurücksetzen\n",
    "\n",
    "  Für Schritt in maximalen Zeitschritten:    \n",
    "    Wählen Sie die Aktion At mit epsilon gierig Policy\n",
    "    Ausführen der Aktion (a) und Beobachten des Ergebniszustands (s') und der Belohnung (r)\n",
    "    Aktualisierung des Q-Wertes Q(s,a) unter Verwendung der Bellman-Gleichung Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "    Wenn fertig, beende die Episode\n",
    "    Unser nächster Zustand ist der neue Zustand\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "paOynXy3aoJW"
   },
   "outputs": [],
   "source": [
    "def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n",
    "  for episode in tqdm(range(n_training_episodes)):\n",
    "    # Reduce epsilon (because we need less and less exploration)\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
    "    # Reset the environment\n",
    "    state, info = env.reset()\n",
    "    step = 0\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "\n",
    "    # repeat\n",
    "    for step in range(max_steps):\n",
    "      # Choose the action At using epsilon greedy policy\n",
    "      action =\n",
    "\n",
    "      # Take action At and observe Rt+1 and St+1\n",
    "      # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "      new_state, reward, terminated, truncated, info =\n",
    "\n",
    "      # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "      Qtable[state][action] =\n",
    "\n",
    "      # If terminated or truncated finish the episode\n",
    "      if terminated or truncated:\n",
    "        break\n",
    "\n",
    "      # Our next state is the new state\n",
    "      state = new_state\n",
    "  return Qtable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pnpk2ePoem3r"
   },
   "source": [
    "#### Lösung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IyZaYbUAeolw"
   },
   "outputs": [],
   "source": [
    "def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n",
    "  for episode in tqdm(range(n_training_episodes)):\n",
    "    # Reduce epsilon (because we need less and less exploration)\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
    "    # Reset the environment\n",
    "    state, info = env.reset()\n",
    "    step = 0\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "\n",
    "    # repeat\n",
    "    for step in range(max_steps):\n",
    "      # Choose the action At using epsilon greedy policy\n",
    "      action = epsilon_greedy_policy(Qtable, state, epsilon)\n",
    "\n",
    "      # Take action At and observe Rt+1 and St+1\n",
    "      # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "      new_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "      # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "      Qtable[state][action] = Qtable[state][action] + learning_rate * (reward + gamma * np.max(Qtable[new_state]) - Qtable[state][action])\n",
    "\n",
    "      # If terminated or truncated finish the episode\n",
    "      if terminated or truncated:\n",
    "        break\n",
    "\n",
    "      # Our next state is the new state\n",
    "      state = new_state\n",
    "  return Qtable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WLwKQ4tUdhGI"
   },
   "source": [
    "## Trainieren Sie den Q-Learning-Agenten 🏃."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DPBxfjJdTCOH"
   },
   "outputs": [],
   "source": [
    "Qtable_frozenlake = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_frozenlake)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yVeEhUCrc30L"
   },
   "source": [
    "## Mal sehen, wie unsere Q-Learning-Tabelle jetzt aussieht 👀."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nmfchsTITw4q"
   },
   "outputs": [],
   "source": [
    "Qtable_frozenlake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pUrWkxsHccXD"
   },
   "source": [
    "## Die Bewertungsmethode 📝\n",
    "\n",
    "- Wir haben die Auswertungsmethode definiert, mit der wir unseren Q-Learning-Agenten testen werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jNl0_JO2cbkm"
   },
   "outputs": [],
   "source": [
    "def evaluate_agent(env, max_steps, n_eval_episodes, Q, seed):\n",
    "  \"\"\"\n",
    "  Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n",
    "  :param env: The evaluation environment\n",
    "  :param max_steps: Maximum number of steps per episode\n",
    "  :param n_eval_episodes: Number of episode to evaluate the agent\n",
    "  :param Q: The Q-table\n",
    "  :param seed: The evaluation seed array (for taxi-v3)\n",
    "  \"\"\"\n",
    "  episode_rewards = []\n",
    "  for episode in tqdm(range(n_eval_episodes)):\n",
    "    if seed:\n",
    "      state, info = env.reset(seed=seed[episode])\n",
    "    else:\n",
    "      state, info = env.reset()\n",
    "    step = 0\n",
    "    truncated = False\n",
    "    terminated = False\n",
    "    total_rewards_ep = 0\n",
    "\n",
    "    for step in range(max_steps):\n",
    "      # Take the action (index) that have the maximum expected future reward given that state\n",
    "      action = greedy_policy(Q, state)\n",
    "      new_state, reward, terminated, truncated, info = env.step(action)\n",
    "      total_rewards_ep += reward\n",
    "\n",
    "      if terminated or truncated:\n",
    "        break\n",
    "      state = new_state\n",
    "    episode_rewards.append(total_rewards_ep)\n",
    "  mean_reward = np.mean(episode_rewards)\n",
    "  std_reward = np.std(episode_rewards)\n",
    "\n",
    "  return mean_reward, std_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0jJqjaoAnxUo"
   },
   "source": [
    "## Bewerten Sie unseren Q-Learning-Agenten 📈\n",
    "\n",
    "- Normalerweise sollte man eine mittlere Belohnung von 1,0 haben.\n",
    "- Die **Umgebung ist relativ einfach**, da der Zustandsraum sehr klein ist (16). Man kann versuchen, sie durch die schlüpfrige Version zu ersetzen (https://gymnasium.farama.org/environments/toy_text/frozen_lake/), was Stochastizität einführt und die Umgebung komplexer macht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fAgB7s0HEFMm"
   },
   "outputs": [],
   "source": [
    "# Evaluate our Agent\n",
    "mean_reward, std_reward = evaluate_agent(env, max_steps, n_eval_episodes, Qtable_frozenlake, eval_seed)\n",
    "print(f\"Mean_reward={mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yxaP3bPdg1DV"
   },
   "source": [
    "## Veröffentliche unser trainiertes Modell auf dem Hub 🔥\n",
    "\n",
    "Nun, da wir nach dem Training gute Ergebnisse gesehen haben, **können wir unser trainiertes Modell mit einer Zeile Code auf dem Hub 🤗 veröffentlichen**.\n",
    "\n",
    "Hier ist ein Beispiel für eine Model Card:\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/modelcard.png\" alt=\"Modellkarte\" width=\"100%\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kv0k1JQjpMq3"
   },
   "source": [
    "Unter der Haube verwendet der Hub Git-basierte Repositories (keine Sorge, wenn Sie nicht wissen, was Git ist), was bedeutet, dass Sie das Modell mit neuen Versionen aktualisieren können, wenn Sie experimentieren und Ihren Agenten verbessern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QZ5LrR-joIHD"
   },
   "source": [
    "#### Dieser Code darf nicht verändert werden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jex3i9lZ8ksX"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, snapshot_download\n",
    "from huggingface_hub.repocard import metadata_eval_result, metadata_save\n",
    "\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qo57HBn3W74O"
   },
   "outputs": [],
   "source": [
    "def record_video(env, Qtable, out_directory, fps=1):\n",
    "  \"\"\"\n",
    "  Generate a replay video of the agent\n",
    "  :param env\n",
    "  :param Qtable: Qtable of our agent\n",
    "  :param out_directory\n",
    "  :param fps: how many frame per seconds (with taxi-v3 and frozenlake-v1 we use 1)\n",
    "  \"\"\"\n",
    "  images = []\n",
    "  terminated = False\n",
    "  truncated = False\n",
    "  state, info = env.reset(seed=random.randint(0,500))\n",
    "  img = env.render()\n",
    "  images.append(img)\n",
    "  while not terminated or truncated:\n",
    "    # Take the action (index) that have the maximum expected future reward given that state\n",
    "    action = np.argmax(Qtable[state][:])\n",
    "    state, reward, terminated, truncated, info = env.step(action) # We directly put next_state = state for recording logic\n",
    "    img = env.render()\n",
    "    images.append(img)\n",
    "  imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4mdUTKkGnUd"
   },
   "outputs": [],
   "source": [
    "def push_to_hub(\n",
    "    repo_id, model, env, video_fps=1, local_repo_path=\"hub\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate, Generate a video and Upload a model to Hugging Face Hub.\n",
    "    This method does the complete pipeline:\n",
    "    - It evaluates the model\n",
    "    - It generates the model card\n",
    "    - It generates a replay video of the agent\n",
    "    - It pushes everything to the Hub\n",
    "\n",
    "    :param repo_id: repo_id: id of the model repository from the Hugging Face Hub\n",
    "    :param env\n",
    "    :param video_fps: how many frame per seconds to record our video replay\n",
    "    (with taxi-v3 and frozenlake-v1 we use 1)\n",
    "    :param local_repo_path: where the local repository is\n",
    "    \"\"\"\n",
    "    _, repo_name = repo_id.split(\"/\")\n",
    "\n",
    "    eval_env = env\n",
    "    api = HfApi()\n",
    "\n",
    "    # Step 1: Create the repo\n",
    "    repo_url = api.create_repo(\n",
    "        repo_id=repo_id,\n",
    "        exist_ok=True,\n",
    "    )\n",
    "\n",
    "    # Step 2: Download files\n",
    "    repo_local_path = Path(snapshot_download(repo_id=repo_id))\n",
    "\n",
    "    # Step 3: Save the model\n",
    "    if env.spec.kwargs.get(\"map_name\"):\n",
    "        model[\"map_name\"] = env.spec.kwargs.get(\"map_name\")\n",
    "        if env.spec.kwargs.get(\"is_slippery\", \"\") == False:\n",
    "            model[\"slippery\"] = False\n",
    "\n",
    "    # Pickle the model\n",
    "    with open((repo_local_path) / \"q-learning.pkl\", \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "    # Step 4: Evaluate the model and build JSON with evaluation metrics\n",
    "    mean_reward, std_reward = evaluate_agent(\n",
    "        eval_env, model[\"max_steps\"], model[\"n_eval_episodes\"], model[\"qtable\"], model[\"eval_seed\"]\n",
    "    )\n",
    "\n",
    "    evaluate_data = {\n",
    "        \"env_id\": model[\"env_id\"],\n",
    "        \"mean_reward\": mean_reward,\n",
    "        \"n_eval_episodes\": model[\"n_eval_episodes\"],\n",
    "        \"eval_datetime\": datetime.datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "    # Write a JSON file called \"results.json\" that will contain the\n",
    "    # evaluation results\n",
    "    with open(repo_local_path / \"results.json\", \"w\") as outfile:\n",
    "        json.dump(evaluate_data, outfile)\n",
    "\n",
    "    # Step 5: Create the model card\n",
    "    env_name = model[\"env_id\"]\n",
    "    if env.spec.kwargs.get(\"map_name\"):\n",
    "        env_name += \"-\" + env.spec.kwargs.get(\"map_name\")\n",
    "\n",
    "    if env.spec.kwargs.get(\"is_slippery\", \"\") == False:\n",
    "        env_name += \"-\" + \"no_slippery\"\n",
    "\n",
    "    metadata = {}\n",
    "    metadata[\"tags\"] = [env_name, \"q-learning\", \"reinforcement-learning\", \"custom-implementation\"]\n",
    "\n",
    "    # Add metrics\n",
    "    eval = metadata_eval_result(\n",
    "        model_pretty_name=repo_name,\n",
    "        task_pretty_name=\"reinforcement-learning\",\n",
    "        task_id=\"reinforcement-learning\",\n",
    "        metrics_pretty_name=\"mean_reward\",\n",
    "        metrics_id=\"mean_reward\",\n",
    "        metrics_value=f\"{mean_reward:.2f} +/- {std_reward:.2f}\",\n",
    "        dataset_pretty_name=env_name,\n",
    "        dataset_id=env_name,\n",
    "    )\n",
    "\n",
    "    # Merges both dictionaries\n",
    "    metadata = {**metadata, **eval}\n",
    "\n",
    "    model_card = f\"\"\"\n",
    "  # **Q-Learning** Agent playing1 **{env_id}**\n",
    "  This is a trained model of a **Q-Learning** agent playing **{env_id}** .\n",
    "\n",
    "  ## Usage\n",
    "\n",
    "  ```python\n",
    "\n",
    "  model = load_from_hub(repo_id=\"{repo_id}\", filename=\"q-learning.pkl\")\n",
    "\n",
    "  # Don't forget to check if you need to add additional attributes (is_slippery=False etc)\n",
    "  env = gym.make(model[\"env_id\"])\n",
    "  ```\n",
    "  \"\"\"\n",
    "\n",
    "    evaluate_agent(env, model[\"max_steps\"], model[\"n_eval_episodes\"], model[\"qtable\"], model[\"eval_seed\"])\n",
    "\n",
    "    readme_path = repo_local_path / \"README.md\"\n",
    "    readme = \"\"\n",
    "    print(readme_path.exists())\n",
    "    if readme_path.exists():\n",
    "        with readme_path.open(\"r\", encoding=\"utf8\") as f:\n",
    "            readme = f.read()\n",
    "    else:\n",
    "        readme = model_card\n",
    "\n",
    "    with readme_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(readme)\n",
    "\n",
    "    # Save our metrics to Readme metadata\n",
    "    metadata_save(readme_path, metadata)\n",
    "\n",
    "    # Step 6: Record a video\n",
    "    video_path = repo_local_path / \"replay.mp4\"\n",
    "    record_video(env, model[\"qtable\"], video_path, video_fps)\n",
    "\n",
    "    # Step 7. Push everything to the Hub\n",
    "    api.upload_folder(\n",
    "        repo_id=repo_id,\n",
    "        folder_path=repo_local_path,\n",
    "        path_in_repo=\".\",\n",
    "    )\n",
    "\n",
    "    print(\"Your model is pushed to the Hub. You can view your model here: \", repo_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81J6cet_ogSS"
   },
   "source": [
    "### .\n",
    "\n",
    "Mit \"push_to_hub\" **werten Sie aus, zeichnen ein Replay auf, generieren eine Modellkarte Ihres Agenten und schieben sie an den Hub**.\n",
    "\n",
    "This way:\n",
    "- Sie können **unsere Arbeit vorführen** 🔥.\n",
    "- Sie können **Ihren Agenten beim Spielen visualisieren** 👀\n",
    "- Du kannst **einen Agenten mit der Community teilen, den andere nutzen können** 💾\n",
    "- Sie können **auf eine Bestenliste zugreifen 🏆 um zu sehen, wie gut Ihr Agent im Vergleich zu Ihren Klassenkameraden abschneidet** 👉 https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cWnFC0iZooTw"
   },
   "source": [
    "Um Ihr Modell mit der Gemeinschaft teilen zu können, sind drei weitere Schritte erforderlich:\n",
    "\n",
    "1️⃣ (Falls noch nicht geschehen) Erstellen Sie ein Konto für HF ➡ https://huggingface.co/join\n",
    "\n",
    "2️⃣ Melde dich an und speichere dann dein Authentifizierungs-Token von der Hugging Face Website.\n",
    "- Erstellen Sie ein neues Token (https://huggingface.co/settings/tokens) **mit Schreibrolle**\n",
    "\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/create-token.jpg\" alt=\"HF-Token erstellen\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QB5nIcxR8paT"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GyWc1x3-o3xG"
   },
   "source": [
    "Wenn Sie kein Google Colab oder ein Jupyter Notebook verwenden möchten, müssen Sie stattdessen diesen Befehl verwenden: `huggingface-cli login` (oder `login`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gc5AfUeFo3xH"
   },
   "source": [
    "3️⃣ Wir sind jetzt bereit, unseren trainierten Agenten mit der Funktion \"push_to_hub()\" an den 🤗 Hub 🔥 zu senden.\n",
    "\n",
    "- Erstellen wir **das Modellwörterbuch, das die Hyperparameter und die Q_table** enthält."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FiMqxqVHg0I4"
   },
   "outputs": [],
   "source": [
    "model = {\n",
    "    \"env_id\": env_id,\n",
    "    \"max_steps\": max_steps,\n",
    "    \"n_training_episodes\": n_training_episodes,\n",
    "    \"n_eval_episodes\": n_eval_episodes,\n",
    "    \"eval_seed\": eval_seed,\n",
    "\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"gamma\": gamma,\n",
    "\n",
    "    \"max_epsilon\": max_epsilon,\n",
    "    \"min_epsilon\": min_epsilon,\n",
    "    \"decay_rate\": decay_rate,\n",
    "\n",
    "    \"qtable\": Qtable_frozenlake\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9kld-AEso3xH"
   },
   "source": [
    "Füllen wir die Funktion \"push_to_hub\":\n",
    "\n",
    "- `repo_id`: der Name des Hugging Face Hub Repository, das erstellt/aktualisiert werden soll `\n",
    "(repo_id = {Benutzername}/{repo_name})`\n",
    "💡 Eine gute `repo_id` ist `{username}/q-{env_id}`\n",
    "- model\": unser Modellwörterbuch mit den Hyperparametern und der Q-Tabelle.\n",
    "- `env`: die Umgebung.\n",
    "- `commit_message`: die Nachricht der Übergabe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5sBo2umnXpPd"
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RpOTtSt83kPZ"
   },
   "outputs": [],
   "source": [
    "username = \"\" # FILL THIS\n",
    "repo_name = \"q-FrozenLake-v1-4x4-noSlippery\"\n",
    "push_to_hub(\n",
    "    repo_id=f\"{username}/{repo_name}\",\n",
    "    model=model,\n",
    "    env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E2875IGsprzq"
   },
   "source": [
    "Herzlichen Glückwunsch 🥳 Sie haben soeben Ihren ersten Reinforcement Learning Agent von Grund auf implementiert, trainiert und hochgeladen.\n",
    "FrozenLake-v1 no_slippery ist eine sehr einfache Umgebung, versuchen wir eine schwierigere 🔥."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18lN8Bz7yvLt"
   },
   "source": [
    "# Teil 2: Taxi-v3 🚖\n",
    "\n",
    "## Erstellen und verstehen [Taxi-v3 🚕](https://gymnasium.farama.org/environments/toy_text/taxi/)\n",
    "---\n",
    "\n",
    "💡 Eine gute Angewohnheit, wenn du anfängst, eine Umgebung zu benutzen, ist es, ihre Dokumentation zu überprüfen.\n",
    "\n",
    "👉 https://gymnasium.farama.org/environments/toy_text/taxi/\n",
    "\n",
    "---\n",
    "\n",
    "In `Taxi-v3` 🚕 gibt es vier bestimmte Orte in der Gitterwelt, die mit R(ed), G(reen), Y(ellow) und B(lue) bezeichnet werden.\n",
    "\n",
    "Zu Beginn der Episode **startet das Taxi an einem zufälligen Platz** und der Fahrgast befindet sich an einem zufälligen Ort. Das Taxi fährt zum Standort des Fahrgastes, **nimmt den Fahrgast auf**, fährt zum Zielort des Fahrgastes (einem anderen der vier angegebenen Orte) und setzt den Fahrgast dann **ab**. Sobald der Fahrgast abgesetzt ist, endet die Episode.\n",
    "\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/taxi.png\" alt=\"Taxi\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gL0wpeO8gpej"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\", render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gBOaXgtsrmtT"
   },
   "source": [
    "Es gibt **500 diskrete Zustände, da es 25 Taxipositionen, 5 mögliche Standorte des Fahrgastes** (einschließlich des Falles, in dem sich der Fahrgast im Taxi befindet) und **4 Zielstandorte** gibt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_TPNaGSZrgqA"
   },
   "outputs": [],
   "source": [
    "state_space = env.observation_space.n\n",
    "print(\"There are \", state_space, \" possible states\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CdeeZuokrhit"
   },
   "outputs": [],
   "source": [
    "action_space = env.action_space.n\n",
    "print(\"There are \", action_space, \" possible actions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R1r50Advrh5Q"
   },
   "source": [
    "Der Aktionsraum (die Menge der möglichen Aktionen, die der Agent ausführen kann) ist diskret mit **6 verfügbaren Aktionen 🎮**:\n",
    "\n",
    "- 0: nach Süden gehen\n",
    "- 1: nach Norden ziehen\n",
    "- 2: nach Osten ziehen\n",
    "- 3: nach Westen fahren\n",
    "- 4: Fahrgast aufnehmen\n",
    "- 5: Fahrgast absetzen\n",
    "\n",
    "Belohnungsfunktion 💰:\n",
    "\n",
    "-1 pro Schritt, wenn keine andere Belohnung ausgelöst wird.\n",
    "- +20 Fahrgast abliefern.\n",
    "-10 Unerlaubtes Ausführen der Aktionen \"Abholen\" und \"Absetzen\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "US3yDXnEtY9I"
   },
   "outputs": [],
   "source": [
    "# Create our Q table with state_size rows and action_size columns (500x6)\n",
    "Qtable_taxi = initialize_q_table(state_space, action_space)\n",
    "print(Qtable_taxi)\n",
    "print(\"Q-table shape: \", Qtable_taxi .shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gUMKPH0_LJyH"
   },
   "source": [
    "## Definieren Sie die Hyperparameter ⚙️\n",
    "\n",
    "⚠ VERÄNDERN SIE NICHT EVAL_SEED: Das eval_seed-Array **ermöglicht es uns, Ihren Agenten mit denselben Taxistartpositionen für jeden Klassenkameraden zu bewerten**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AB6n__hhg7YS"
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "n_training_episodes = 25000   # Total training episodes\n",
    "learning_rate = 0.7           # Learning rate\n",
    "\n",
    "# Evaluation parameters\n",
    "n_eval_episodes = 100        # Total number of test episodes\n",
    "\n",
    "# DO NOT MODIFY EVAL_SEED\n",
    "eval_seed = [16,54,165,177,191,191,120,80,149,178,48,38,6,125,174,73,50,172,100,148,146,6,25,40,68,148,49,167,9,97,164,176,61,7,54,55,\n",
    " 161,131,184,51,170,12,120,113,95,126,51,98,36,135,54,82,45,95,89,59,95,124,9,113,58,85,51,134,121,169,105,21,30,11,50,65,12,43,82,145,152,97,106,55,31,85,38,\n",
    " 112,102,168,123,97,21,83,158,26,80,63,5,81,32,11,28,148] # Evaluation seed, this ensures that all classmates agents are trained on the same taxi starting position\n",
    "                                                          # Each seed has a specific starting state\n",
    "\n",
    "# Environment parameters\n",
    "env_id = \"Taxi-v3\"           # Name of the environment\n",
    "max_steps = 99               # Max steps per episode\n",
    "gamma = 0.95                 # Discounting rate\n",
    "\n",
    "# Exploration parameters\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.05           # Minimum exploration probability\n",
    "decay_rate = 0.005            # Exponential decay rate for exploration prob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1TMORo1VLTsX"
   },
   "source": [
    "## Trainieren Sie unseren Q-Learning-Agenten 🏃."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WwP3Y2z2eS-K"
   },
   "outputs": [],
   "source": [
    "Qtable_taxi = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_taxi)\n",
    "Qtable_taxi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wPdu0SueLVl2"
   },
   "source": [
    "## Erstellen Sie ein Modellwörterbuch 💾 und veröffentlichen Sie unser trainiertes Modell auf dem Hub 🔥.\n",
    "\n",
    "- Wir erstellen ein Modell-Wörterbuch, das alle Trainings-Hyperparameter für die Reproduzierbarkeit und die Q-Tabelle enthalten wird.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0a1FpE_3hNYr"
   },
   "outputs": [],
   "source": [
    "model = {\n",
    "    \"env_id\": env_id,\n",
    "    \"max_steps\": max_steps,\n",
    "    \"n_training_episodes\": n_training_episodes,\n",
    "    \"n_eval_episodes\": n_eval_episodes,\n",
    "    \"eval_seed\": eval_seed,\n",
    "\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"gamma\": gamma,\n",
    "\n",
    "    \"max_epsilon\": max_epsilon,\n",
    "    \"min_epsilon\": min_epsilon,\n",
    "    \"decay_rate\": decay_rate,\n",
    "\n",
    "    \"qtable\": Qtable_taxi\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dhQtiQozhOn1"
   },
   "outputs": [],
   "source": [
    "username = \"\" # FILL THIS\n",
    "repo_name = \"\" # FILL THIS\n",
    "push_to_hub(\n",
    "    repo_id=f\"{username}/{repo_name}\",\n",
    "    model=model,\n",
    "    env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZgSdjgbIpRti"
   },
   "source": [
    "Jetzt, wo es auf dem Hub ist, kannst du die Ergebnisse deines Taxi-v3 mit deinen Klassenkameraden in der Bestenliste vergleichen 🏆 👉 https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard\n",
    "\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/taxi-leaderboard.png\" alt=\"Taxi-Rangliste\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bzgIO70c0bu2"
   },
   "source": [
    "# Teil 3: Laden vom Hub 🔽.\n",
    "\n",
    "Das Erstaunliche an Hugging Face Hub 🤗 ist, dass du ganz einfach leistungsstarke Modelle aus der Community laden kannst.\n",
    "\n",
    "Das Laden eines gespeicherten Modells aus dem Hub ist wirklich einfach:\n",
    "\n",
    "1. Du gehst auf https://huggingface.co/models?other=q-learning, um die Liste aller gespeicherten q-learning-Modelle zu sehen.\n",
    "2. Sie wählen eines aus und kopieren seine repo_id\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/copy-id.png\" alt=\"ID kopieren\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTth6thRoC6X"
   },
   "source": [
    "3. Dann müssen wir nur `load_from_hub` mit verwenden:\n",
    "- Die Repo_id\n",
    "- Der Dateiname: das gespeicherte Modell innerhalb der Repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EtrfoTaBoNrd"
   },
   "source": [
    "#### Dieser Code darf nicht verändert werden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eo8qEzNtCaVI"
   },
   "outputs": [],
   "source": [
    "from urllib.error import HTTPError\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "\n",
    "def load_from_hub(repo_id: str, filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Download a model from Hugging Face Hub.\n",
    "    :param repo_id: id of the model repository from the Hugging Face Hub\n",
    "    :param filename: name of the model zip file from the repository\n",
    "    \"\"\"\n",
    "    # Get the model from the Hub, download and cache the model on your local disk\n",
    "    pickle_model = hf_hub_download(\n",
    "        repo_id=repo_id,\n",
    "        filename=filename\n",
    "    )\n",
    "\n",
    "    with open(pickle_model, 'rb') as f:\n",
    "      downloaded_model_file = pickle.load(f)\n",
    "\n",
    "    return downloaded_model_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b_sM2gNioPZH"
   },
   "source": [
    "### ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JUm9lz2gCQcU"
   },
   "outputs": [],
   "source": [
    "model = load_from_hub(repo_id=\"ThomasSimonini/q-Taxi-v3\", filename=\"q-learning.pkl\") # Try to use another model\n",
    "\n",
    "print(model)\n",
    "env = gym.make(model[\"env_id\"])\n",
    "\n",
    "evaluate_agent(env, model[\"max_steps\"], model[\"n_eval_episodes\"], model[\"qtable\"], model[\"eval_seed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O7pL8rg1MulN"
   },
   "outputs": [],
   "source": [
    "model = load_from_hub(repo_id=\"ThomasSimonini/q-FrozenLake-v1-no-slippery\", filename=\"q-learning.pkl\") # Try to use another model\n",
    "\n",
    "env = gym.make(model[\"env_id\"], is_slippery=False)\n",
    "\n",
    "evaluate_agent(env, model[\"max_steps\"], model[\"n_eval_episodes\"], model[\"qtable\"], model[\"eval_seed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQAwLnYFPk-s"
   },
   "source": [
    "## Einige zusätzliche Herausforderungen 🏆\n",
    "\n",
    "Die beste Art zu lernen **ist, Dinge selbst auszuprobieren**! Wie Sie gesehen haben, ist der derzeitige Agent nicht besonders gut. Als ersten Vorschlag können Sie für mehr Schritte trainieren. Bei 1.000.000 Schritten haben wir tolle Ergebnisse gesehen!\n",
    "\n",
    "In der [Rangliste] (https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) findest du deine Agenten. Können Sie sich an die Spitze setzen?\n",
    "\n",
    "Hier sind einige Ideen, um in der Rangliste aufzusteigen:\n",
    "\n",
    "* Trainiere mehr Schritte\n",
    "* Probiere verschiedene Hyperparameter aus, indem du dir ansiehst, was deine Klassenkameraden gemacht haben.\n",
    "* Pushe dein neu trainiertes Modell** auf dem Hub 🔥.\n",
    "\n",
    "Sind Ihnen das Laufen auf Eis und das Fahren von Taxis zu langweilig? Versuche, die **Umgebung** zu verändern, warum nicht die rutschige Version von FrozenLake-v1 benutzen? Schau dir an, wie sie funktionieren [mit Hilfe der Turnhallendokumentation] (https://gymnasium.farama.org/) und hab Spaß 🎉."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-fW-EU5WejJ"
   },
   "source": [
    "_____________________________________________________________________\n",
    "Herzlichen Glückwunsch 🥳, Sie haben soeben Ihren ersten Reinforcement Learning Agent implementiert, trainiert und hochgeladen.\n",
    "\n",
    "Das Verständnis von Q-Learning ist ein **wichtiger Schritt zum Verständnis wertbasierter Methoden**.\n",
    "\n",
    "In der nächsten Einheit mit Deep Q-Learning werden wir sehen, dass das Erstellen und Aktualisieren einer Q-Tabelle zwar eine gute Strategie war - **aber nicht skalierbar ist.**\n",
    "\n",
    "Stellen Sie sich zum Beispiel vor, Sie erstellen einen Agenten, der lernt, Doom zu spielen.\n",
    "\n",
    "<img src=\"https://vizdoom.cs.put.edu.pl/user/pages/01.tutorial/basic.png\" alt=\"Doom\"/>\n",
    "\n",
    "Doom ist eine große Umgebung mit einem riesigen Zustandsraum (Millionen von verschiedenen Zuständen). Das Erstellen und Aktualisieren einer Q-Tabelle für diese Umgebung wäre nicht effizient.\n",
    "\n",
    "Deshalb werden wir uns in der nächsten Einheit mit Deep Q-Learning beschäftigen, einem Algorithmus, **bei dem wir ein neuronales Netz verwenden, das bei einem bestimmten Zustand die verschiedenen Q-Werte für jede Aktion annähert**.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/atari-envs.gif\" alt=\"Umgebungen\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BjLhT70TEZIn"
   },
   "source": [
    "Wir sehen uns in Referat 3! 🔥\n",
    "\n",
    "## Lernt weiter, bleibt fantastisch 🤗"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "67OdoKL63eDD",
    "B2_-8b8z5k54",
    "8R5ej1fS4P2V",
    "Pnpk2ePoem3r"
   ],
   "include_colab_link": true,
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
