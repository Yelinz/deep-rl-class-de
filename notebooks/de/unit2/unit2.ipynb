{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njb_ProuHiOe"
   },
   "source": [
    "# Einheit 2: Q-Learning mit FrozenLake-v1 ‚õÑ und Taxi-v3 üöï\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/thumbnail.jpg\" alt=\"Unit 2 Thumbnail\">\n",
    "\n",
    "In diesem Notizbuch **programmieren Sie Ihren ersten Reinforcement Learning-Agenten von Grund auf**, um FrozenLake ‚ùÑÔ∏è mit Q-Learning zu spielen, ihn mit der Community zu teilen und mit verschiedenen Konfigurationen zu experimentieren.\n",
    "\n",
    "‚¨áÔ∏è Hier ist ein Beispiel daf√ºr, was **Sie in nur wenigen Minuten erreichen werden.** ‚¨áÔ∏è\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vRU_vXBrl1Jx"
   },
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/envs.gif\" alt=\"Umgebungen\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DPTBOv9HYLZ2"
   },
   "source": [
    "### üéÆ Umgebungen:\n",
    "\n",
    "- [FrozenLake-v1](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)\n",
    "- [Taxi-v3](https://gymnasium.farama.org/environments/toy_text/taxi/)\n",
    "\n",
    "### üìö RL-Library:\n",
    "\n",
    "- Python und NumPy\n",
    "- [Gymnasium](https://gymnasium.farama.org/)\n",
    "\n",
    "Wir sind st√§ndig bem√ºht, unsere Tutorials zu verbessern. **Wenn Sie also Probleme in diesem Notizbuch finden**, √∂ffnen Sie bitte [einen Fehler im GitHub Repo](https://github.com/huggingface/deep-rl-class/issues)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4i6tjI2tHQ8j"
   },
   "source": [
    "## Ziele dieses Notizbuchs üèÜ\n",
    "\n",
    "Am Ende des Notizbuchs werden Sie:\n",
    "\n",
    "- In der Lage sein, **Gymnasium**, die Umgebungsbibliothek, zu benutzen.\n",
    "- In der Lage sein, einen Q-Learning-Agenten von Grund auf zu programmieren.\n",
    "- In der Lage sein, **Ihren trainierten Agenten und den Code auf den Hub** mit einer sch√∂nen Videowiedergabe und einer Bewertung zu pushen üî•.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "viNzVbVaYvY3"
   },
   "source": [
    "## Dieses Notizbuch stammt aus dem Kurs Deep Reinforcement Learning\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/deep-rl-course-illustration.jpg\" alt=\"Deep RL Course illustration\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6p5HnEefISCB"
   },
   "source": [
    "In diesem kostenlosen Kurs lernen Sie:\n",
    "\n",
    "- üìñ Deep Reinforcement Learning in **Theorie und Praxis** studieren.\n",
    "- üßë‚Äçüíª Lernen Sie, **ber√ºhmte Deep RL-Bibliotheken** wie Stable Baselines3, RL Baselines3 Zoo, CleanRL und Sample Factory 2.0 zu verwenden.\n",
    "- ü§ñ Trainieren Sie **Agenten in einzigartigen Umgebungen**.\n",
    "\n",
    "Und mehr, siehe üìö den Lehrplan üëâ https://simoninithomas.github.io/deep-rl-course\n",
    "\n",
    "Vergessen Sie nicht, sich **<a href=\"http://eepurl.com/ic5ZUD\">f√ºr den Kurs anzumelden</a>** (wir sammeln Ihre E-Mail, um Ihnen **die Links zu senden, wenn die einzelnen Einheiten ver√∂ffentlicht werden, und Sie √ºber die Herausforderungen und Aktualisierungen zu informieren).**\n",
    "\n",
    "\n",
    "Der beste Weg, um in Kontakt zu bleiben, ist, unserem Discord-Server beizutreten, um sich mit der Community und mit uns auszutauschen üëâüèª https://discord.gg/ydHrjt3WP5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Einf√ºhrung in das Q-Learning\n",
    "\n",
    "In der ersten Einheit dieses Kurses haben wir etwas √ºber Reinforcement Learning (RL), den RL-Prozess und die verschiedenen Methoden zur L√∂sung eines RL-Problems gelernt. Wir haben auch **unsere ersten Agenten trainiert und sie in den Hugging Face Hub hochgeladen.**\n",
    "\n",
    "In dieser Einheit werden wir **eine der Methoden des Reinforcement Learning vertiefen: wertbasierte Methoden** und unseren ersten RL-Algorithmus untersuchen: **Q-Learning.**\n",
    "\n",
    "Wir werden auch **unseren ersten RL-Agenten von Grund auf** implementieren, einen Q-Learning-Agenten, und ihn in zwei Umgebungen trainieren:\n",
    "\n",
    "1. Frozen-Lake-v1 (rutschfeste Version): Hier muss unser Agent **vom Startzustand (S) zum Zielzustand (G)** gelangen, indem er nur auf gefrorenen Kacheln (F) l√§uft und L√∂cher (H) vermeidet.\n",
    "2. Ein autonomes Taxi: Unser Agent muss **lernen, sich in einer Stadt zurechtzufinden**, um **seine Fahrg√§ste von Punkt A nach Punkt B zu bef√∂rdern**.\n",
    "\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/envs.gif\" alt=\"Umgebungen\"/>\n",
    "\n",
    "Konkret werden wir:\n",
    "\n",
    "- Lernen Sie **wertbasierte Methoden** kennen.\n",
    "- Lernen Sie die **Unterschiede zwischen Monte Carlo und Temporal Difference Learning** kennen.\n",
    "- Studium und Implementierung **unseres ersten RL-Algorithmus**: Q-Lernen.\n",
    "\n",
    "Diese Einheit ist **grundlegend, wenn Sie in der Lage sein wollen, an Deep Q-Learning** zu arbeiten: der erste Deep RL-Algorithmus, der Atari-Spiele spielte und bei einigen von ihnen das menschliche Niveau schlug (Breakout, Space Invaders, etc.).\n",
    "\n",
    "Also lasst uns anfangen! üöÄ\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Was ist RL? Eine kurze Zusammenfassung\n",
    "\n",
    "In RL bauen wir einen Agenten, der **kluge Entscheidungen** treffen kann. Zum Beispiel einen Agenten, der **lernt, ein Videospiel zu spielen** oder einen Handelsagenten, der **lernt, seinen Gewinn zu maximieren**, indem er entscheidet, **welche Aktien er kaufen und wann er verkaufen soll**.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/rl-process.jpg\" alt=\"RL-Prozess\"/>\n",
    "\n",
    "\n",
    "Um intelligente Entscheidungen zu treffen, lernt unser Agent von der Umwelt, indem er **durch Versuch und Irrtum** mit ihr interagiert und (positive oder negative) Belohnungen **als einzigartiges Feedback** erh√§lt.\n",
    "\n",
    "Sein Ziel **ist es, seine erwartete kumulative Belohnung** zu maximieren (aufgrund der Belohnungshypothese).\n",
    "\n",
    "**Der Entscheidungsprozess des Agenten wird als Policy œÄ bezeichnet:** In einem gegebenen Zustand gibt eine Policy eine Aktion oder eine Wahrscheinlichkeitsverteilung √ºber Aktionen aus. Das hei√üt, bei einer Beobachtung der Umgebung gibt eine Strategie eine Aktion (oder mehrere Wahrscheinlichkeiten f√ºr jede Aktion) vor, die der Agent ausf√ºhren sollte.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/policy.jpg\" alt=\"Policy\"/>\n",
    "\n",
    "**Unser Ziel ist es, eine optimale Strategie œÄ* ** zu finden, d. h. eine Strategie, die zur besten erwarteten kumulativen Belohnung f√ºhrt.\n",
    "\n",
    "Und um diese optimale Policy zu finden (und damit das RL-Problem zu l√∂sen), gibt es **zwei Haupttypen von RL-Methoden**:\n",
    "\n",
    "- *Policybasierte Methoden*: **Direktes Trainieren der Strategie**, um zu lernen, welche Aktion bei einem bestimmten Zustand zu ergreifen ist.\n",
    "- *Wertbasierte Methoden*: **Trainieren eine Wertfunktion**, um zu lernen, **welcher Zustand wertvoller ist**, und verwenden diese Wertfunktion **um die Aktion zu ergreifen, die zu diesem Zustand f√ºhrt**.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/two-approaches.jpg\" alt=\"Zwei RL-Ans√§tze\"/>\n",
    "\n",
    "Und in dieser Einheit **werden wir tiefer in die wertbasierten Methoden eintauchen**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Die Bellman-Gleichung: Vereinfachung der Wertberechnung\n",
    "\n",
    "Die Bellman-Gleichung **vereinfacht unsere Zustandswert- oder Zustands-Aktionswert-Berechnung**.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/bellman.jpg\" alt=\"Bellman equation\"/>\n",
    "\n",
    "Mit dem, was wir bisher gelernt haben, wissen wir, dass wir, wenn wir \\\\(V(S_t)\\\\) (den Wert eines Zustands) berechnen, die Belohnung ab diesem Zustand berechnen und dann die Strategie f√ºr immer weiter verfolgen m√ºssen. **(Die Policy, die wir im folgenden Beispiel definiert haben, ist eine Greedy-Policy; zur Vereinfachung lassen wir die Belohnung unber√ºcksichtigt).\n",
    "\n",
    "Um also \\\\(V(S_t)\\\\) zu berechnen, m√ºssen wir die Summe der erwarteten Belohnungen berechnen. Daraus folgt:\n",
    "<figure>\n",
    "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/bellman2.jpg\" alt=\"Bellman equation\"/>\n",
    "  <figcaption>To calculate the value of State 1: the sum of rewards¬†if the agent started in that state¬†and then followed the¬†greedy policy (taking actions that leads to the best states values) for all the time steps.</figcaption>\n",
    "</figure>\n",
    "Zur Berechnung von \\\\(V(S_{t+1})\\\\) m√ºssen wir dann die Belohnung ab diesem Zustand \\\\(S_{t+1}\\) berechnen.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/bellman3.jpg\" alt=\"Bellman equation\"/>\n",
    "  <figcaption>To calculate the value of State 2: the sum of rewards¬†<b>if the agent started in that state</b>,¬†and then followed the¬†<b>policy for all the time steps.</b></figcaption>\n",
    "</figure>\n",
    "\n",
    "Sie haben vielleicht bemerkt, dass wir die Berechnung des Wertes der verschiedenen Zust√§nde wiederholen, was m√ºhsam sein kann, wenn Sie dies f√ºr jeden Zustandswert oder Zustandsaktionswert tun m√ºssen.\n",
    "\n",
    "Anstatt die erwartete Belohnung f√ºr jeden Zustand oder jedes Zustands-Aktionspaar zu berechnen, **k√∂nnen wir die Bellman-Gleichung verwenden** (Hinweis: Wenn Sie wissen, was Dynamische Programmierung ist, ist dies sehr √§hnlich! wenn Sie nicht wissen, was es ist, keine Sorge!)\n",
    "\n",
    "Die Bellman-Gleichung ist eine rekursive Gleichung, die wie folgt funktioniert: Anstatt f√ºr jeden Zustand von vorne zu beginnen und die Belohnung zu berechnen, k√∂nnen wir den Wert eines jeden Zustands als:\n",
    "\n",
    "**Die unmittelbare Belohnung \\\\(R_{t+1}\\) + der Rabattierte Wert des darauf folgenden Zustands ( \\\\(gamma * V(S_{t+1}) \\\\) ) .**\n",
    "\n",
    "<figure>\n",
    "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/bellman4.jpg\" alt=\"Bellman equation\"/>\n",
    "</figure>\n",
    "\n",
    "Wenn wir zu unserem Beispiel zur√ºckkehren, k√∂nnen wir sagen, dass der Wert von Zustand 1 gleich der erwarteten kumulativen Belohnung ist, wenn wir bei diesem Zustand beginnen.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/bellman2.jpg\" alt=\"Bellman equation\"/>\n",
    "\n",
    "\n",
    "Um den Wert von Zustand 1 zu berechnen: die Summe der Belohnungen, **wenn der Agent in diesem Zustand 1** beginnen und dann die **Policy f√ºr alle Zeitschritte verfolgen w√ºrde.**\n",
    "\n",
    "Dies ist √§quivalent zu \\\\(V(S_{t})\\\\) = Sofortige Belohnung \\\\(R_{t+1}\\\\) + Rabattierter Wert des n√§chsten Zustands \\\\(\\gamma * V(S_{t+1})\\\\\\)\n",
    "\n",
    "\n",
    "<figure>\n",
    "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/bellman6.jpg\" alt=\"Bellman equation\"/>\n",
    "  <figcaption>For simplification, here we don‚Äôt discount so gamma = 1.</figcaption>\n",
    "</figure>\n",
    "\n",
    "Der Einfachheit halber wird hier nicht Rabattiert, also ist gamma = 1.\n",
    "Sie werden jedoch im Abschnitt Q-Learning dieser Einheit ein Beispiel mit gamma = 0,99 untersuchen.\n",
    "\n",
    "- Der Wert von \\\\(V(S_{t+1}) \\\\) = unmittelbare Belohnung \\\\(R_{t+2}\\) + Rabattierter Wert des n√§chsten Zustands ( \\\\\\(gamma * V(S_{t+2})\\\\\\) ).\n",
    "- Und so weiter.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Zusammenfassend l√§sst sich sagen, dass die Idee der Bellman-Gleichung darin besteht, dass wir, anstatt jeden Wert als Summe der erwarteten Belohnung zu berechnen, **was ein langwieriger Prozess ist**, den Wert als **die Summe der unmittelbaren Belohnung + des Rabattierten Wertes des folgenden Zustands berechnen.**\n",
    "\n",
    "Bevor wir zum n√§chsten Abschnitt √ºbergehen, denken Sie √ºber die Rolle von Gamma in der Bellman-Gleichung nach. Was passiert, wenn der Wert von gamma sehr niedrig ist (z. B. 0,1 oder sogar 0)? Was passiert, wenn der Wert 1 ist? Was passiert, wenn der Wert sehr hoch ist, z. B. eine Million?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-mo_6rXIjRi"
   },
   "source": [
    "## Voraussetzungen üèóÔ∏è\n",
    "\n",
    "Bevor Sie sich mit dem Notebook besch√§ftigen, m√ºssen Sie:\n",
    "\n",
    "üî≤ üìö **Studieren Sie [Q-Learning by reading Unit 2](https://huggingface.co/deep-rl-course/unit2/introduction)** ü§ó"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2ONOODsyrMU"
   },
   "source": [
    "## Eine kleine Zusammenfassung von Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V68VveLacfxJ"
   },
   "source": [
    "*Q-Learning* **ist der RL-Algorithmus, der**:\n",
    "\n",
    "- Eine *Q-Funktion* trainiert, eine **Aktionswertfunktion**, die im internen Speicher durch eine *Q-Tabelle* **kodiert wird, die alle Werte der Zustands-Aktionspaare enth√§lt**.\n",
    "\n",
    "- Wenn ein Zustand und eine Aktion gegeben sind, sucht unsere Q-Funktion **in der Q-Tabelle nach dem entsprechenden Wert**.\n",
    "    \n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-function-2.jpg\" alt=\"Q-Funktion\" width=\"100%\"/>\n",
    "\n",
    "- Wenn das Training abgeschlossen ist, **haben wir eine optimale Q-Funktion, also eine optimale Q-Tabelle.**\n",
    "    \n",
    "- Und wenn wir **eine optimale Q-Funktion** haben, haben wir\n",
    "haben wir eine optimale Policy, da wir **f√ºr jeden Zustand die beste Aktion kennen, die zu ergreifen ist.**\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/link-value-policy.jpg\" alt=\"Link value policy\" width=\"100%\"/>\n",
    "\n",
    "\n",
    "Aber am Anfang ist unsere **Q-Tabelle nutzlos, da sie f√ºr jedes Zustands-Aktionspaar einen beliebigen Wert angibt (meistens initialisieren wir die Q-Tabelle mit 0 Werten)**. Aber wenn wir die Umgebung erkunden und unsere Q-Tabelle aktualisieren, wird sie uns immer bessere Ann√§herungen liefern\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/q-learning.jpeg\" alt=\"q-learning.jpeg\" width=\"100%\"/>\n",
    "\n",
    "Dies ist der Pseudocode f√ºr das Q-Learning:\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg\" alt=\"Q-Learning\" width=\"100%\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo vs. Temporales Differenzlernen\n",
    "\n",
    "Der letzte Punkt, den wir besprechen m√ºssen, bevor wir uns dem Q-Learning zuwenden, sind die beiden Lernstrategien.\n",
    "\n",
    "Denken Sie daran, dass ein RL-Agent **durch die Interaktion mit seiner Umgebung lernt.** Die Idee ist, dass **der Agent aufgrund der Erfahrung und der erhaltenen Belohnung seine Wertfunktion oder Strategie aktualisiert.**\n",
    "\n",
    "Monte Carlo und Temporal Difference Learning sind zwei verschiedene **Strategien, wie wir unsere Wertfunktion oder unsere Policy-Funktion trainieren k√∂nnen.** Beide **nutzen Erfahrung, um das RL-Problem zu l√∂sen.**\n",
    "\n",
    "Auf der einen Seite verwendet Monte Carlo **eine ganze Episode an Erfahrung, bevor es lernt.** Auf der anderen Seite verwendet Temporal Difference **nur einen Schritt ( \\\\(S_t, A_t, R_{t+1}, S_{t+1}\\) ) zum Lernen.**\n",
    "\n",
    "Wir werden beide Methoden anhand eines Beispiels f√ºr eine wertbasierte Methode erl√§utern.\n",
    "\n",
    "## Monte Carlo: Lernen am Ende der Episode\n",
    "\n",
    "Monte Carlo wartet bis zum Ende der Episode, berechnet \\\\(G_t\\\\) (R√ºckgabe) und verwendet es als **Ziel f√ºr die Aktualisierung von \\\\(V(S_t)\\\\).**\n",
    "\n",
    "Es ist also eine **vollst√§ndige Episode der Interaktion erforderlich, bevor wir unsere Wertfunktion aktualisieren k√∂nnen.**\n",
    "\n",
    "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/monte-carlo-approach.jpg\" alt=\"Monte Carlo\"/>\n",
    "\n",
    "\n",
    "Wenn wir ein Beispiel nehmen:\n",
    "\n",
    "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/MC-2.jpg\" alt=\"Monte Carlo\"/>\n",
    "\n",
    "\n",
    "- Wir beginnen die Episode immer **am gleichen Startpunkt.**\n",
    "- **Der Agent unternimmt Aktionen unter Verwendung der Policy**. Zum Beispiel mit einer Epsilon Greedy Strategy, einer Strategie, die zwischen Exploration (zuf√§llige Aktionen) und Ausbeutung wechselt.\n",
    "- Wir erhalten **die Belohnung und den n√§chsten Zustand**.\n",
    "- Wir beenden die Episode, wenn die Katze die Maus frisst oder wenn sich die Maus > 10 Schritte bewegt.\n",
    "\n",
    "- Am Ende der Episode haben wir **eine Liste von Zustands-, Aktions-, Belohnungs- und Folgezustands-Tupeln**\n",
    "Zum Beispiel [[Zustandskachel 3 unten, Nach links gehen, +1, Zustandskachel 2 unten], [Zustandskachel 2 unten, Nach links gehen, +0, Zustandskachel 1 unten]...]\n",
    "\n",
    "- **Der Agent wird die Gesamtbelohnungen \\\\(G_t\\\\)** addieren (um zu sehen, wie gut er abgeschnitten hat).\n",
    "- Dann **aktualisiert er \\\\(V(s_t)\\\\) anhand der Formel**\n",
    "\n",
    "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/MC-3.jpg\" alt=\"Monte Carlo\"/>\n",
    "\n",
    "- Dann **starten Sie ein neues Spiel mit diesem neuen Wissen**\n",
    "\n",
    "Indem man mehr und mehr Episoden durchf√ºhrt, **lernt der Agent, immer besser zu spielen**.\n",
    "\n",
    "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/MC-3p.jpg\" alt=\"Monte Carlo\"/>\n",
    "\n",
    "Wenn wir zum Beispiel eine Zustandswertfunktion mit Monte Carlo trainieren:\n",
    "\n",
    "- Wir initialisieren unsere Wertfunktion **so, dass sie f√ºr jeden Zustand den Wert 0 liefert**\n",
    "- Unsere Lernrate (lr) ist 0,1 und unsere Abzinsungsrate ist 1 (= keine Abzinsung)\n",
    "- Unsere Maus **erforscht die Umgebung und f√ºhrt zuf√§llige Aktionen aus**\n",
    "\n",
    "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/MC-4.jpg\" alt=\"Monte Carlo\"/>\n",
    "\n",
    "\n",
    "- Die Maus hat mehr als 10 Schritte gemacht, also endet die Episode .\n",
    "\n",
    "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/MC-4p.jpg\" alt=\"Monte Carlo\"/>\n",
    "\n",
    "\n",
    "\n",
    "- Wir haben eine Liste von Zustand, Aktion, Belohnungen, n√§chster_Zustand, **wir m√ºssen die R√ºckgabe berechnen \\\\(G{t=0}\\)**\n",
    "\n",
    "\\\\(G_t = R_{t+1} + R_{t+2} + R_{t+3} ...\\\\\\) (der Einfachheit halber lassen wir die Belohnungen unber√ºcksichtigt)\n",
    "\n",
    "\\\\(G_0 = R_{1} + R_{2} + R_{3}...\\\\\\)\n",
    "\n",
    "\\\\(G_0 = 1 + 0 + 0 + 0 + 0 + 0 + 1 + 1 + 0 + 0\\\\)\n",
    "\n",
    "\\\\(G_0 = 3\\\\)\n",
    "\n",
    "- Wir k√∂nnen nun das **neue** \\\\(V(S_0)\\\\) berechnen:\n",
    "\n",
    "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/MC-5.jpg\" alt=\"Monte Carlo\"/>\n",
    "\n",
    "\\\\(V(S_0) = V(S_0) + lr * [G_0 - V(S_0)]\\\\)\n",
    "\n",
    "\\\\(V(S_0) = 0 + 0,1 * [3 - 0]\\\\)\n",
    "\n",
    "\\\\(V(S_0) = 0,3\\\\)\n",
    "\n",
    "\n",
    "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/MC-5p.jpg\" alt=\"Monte Carlo\"/>\n",
    "\n",
    "\n",
    "## Temporal Difference Learning: Lernen bei jedem Schritt \n",
    "\n",
    "**Temporal Difference hingegen wartet nur auf eine Interaktion (einen Schritt) \\\\\\(S_{t+1}\\\\)**, um ein TD-Ziel zu bilden und \\\\(V(S_t)\\\\) unter Verwendung von \\\\\\(R_{t+1}\\) und \\\\\\( \\gamma * V(S_{t+1})\\\\\\) zu aktualisieren.\n",
    "\n",
    "Die Idee bei **TD ist, das \\\\(V(S_t)\\\\) bei jedem Schritt zu aktualisieren.\n",
    "\n",
    "Da wir aber nicht eine ganze Episode erlebt haben, verf√ºgen wir nicht √ºber \\\\(G_t\\\\) (erwartete Belohnung). Stattdessen **sch√§tzen wir \\\\(G_t\\\\), indem wir \\\\(R_{t+1}\\) und den Rabattierten Wert des n√§chsten Zustands addieren.**\n",
    "\n",
    "Dies wird Bootstrapping genannt. Es wird so genannt, **weil TD seine Aktualisierung zum Teil auf eine bestehende Sch√§tzung \\\\(V(S_{t+1})\\\\) und nicht auf eine vollst√§ndige Stichprobe \\\\(G_t\\\\) st√ºtzt.**\n",
    "\n",
    "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/TD-1.jpg\" alt=\"Temporal Difference\"/>\n",
    "\n",
    "\n",
    "Diese Methode wird TD(0) oder **einschrittige TD (Aktualisierung der Wertfunktion nach jedem einzelnen Schritt) genannt.**\n",
    "\n",
    "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/TD-1p.jpg\" alt=\"Temporal Difference\"/>\n",
    "\n",
    "Wenn wir das gleiche Beispiel nehmen,\n",
    "\n",
    "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/TD-2.jpg\" alt=\"Temporal Difference\"/>\n",
    "\n",
    "- Wir initialisieren unsere Wertfunktion so, dass sie f√ºr jeden Zustand den Wert 0 zur√ºckgibt.\n",
    "- Unsere Lernrate (lr) ist 0,1, und unsere Abzinsungsrate ist 1 (keine Abzinsung).\n",
    "- Unsere Maus beginnt, die Umgebung zu erkunden und f√ºhrt eine zuf√§llige Aktion aus: **nach links gehen**\n",
    "- Sie erh√§lt eine Belohnung \\\\(R_{t+1} = 1\\\\), da **sie ein St√ºck K√§se isst**\n",
    "\n",
    "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/TD-2p.jpg\" alt=\"Temporal Difference\"/>\n",
    "\n",
    "\n",
    "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/TD-3.jpg\" alt=\"Temporal Difference\"/>\n",
    "\n",
    "Wir k√∂nnen nun \\\\(V(S_0)\\\\) aktualisieren:\n",
    "\n",
    "Neu \\\\(V(S_0) = V(S_0) + lr * [R_1 + \\gamma * V(S_1) - V(S_0)]\\\\)\n",
    "\n",
    "Neu \\\\(V(S_0) = 0 + 0,1 * [1 + 1 * 0-0]\\\\)\n",
    "\n",
    "Neu \\\\(V(S_0) = 0,1\\\\)\n",
    "\n",
    "Wir haben also gerade unsere Wertfunktion f√ºr den Zustand 0 aktualisiert.\n",
    "\n",
    "Jetzt fahren wir damit fort, mit unserer aktualisierten Wertfunktion mit dieser Umgebung zu interagieren.\n",
    "\n",
    "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/TD-3p.jpg\" alt=\"Temporal Difference\"/>\n",
    "\n",
    "  Um es zusammenzufassen:\n",
    "\n",
    "  - Mit *Monte Carlo* aktualisieren wir die Wertfunktion aus einer kompletten Episode, und so **verwenden wir die tats√§chliche genaue Rabattierte Belohnung dieser Episode.**\n",
    "  - Mit *TD Learning* aktualisieren wir die Wertfunktion aus einem Schritt, und wir ersetzen \\\\(G_t\\\\), das wir nicht kennen, mit **einer gesch√§tzten Belohnung, die TD-Ziel genannt wird.**\n",
    "\n",
    "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Summary.jpg\" alt=\"Zusammenfassung\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mid-way Recap\n",
    "Bevor wir uns mit Q-Learning besch√§ftigen, sollten wir das eben Gelernte noch einmal zusammenfassen.\n",
    "\n",
    "Wir haben zwei Arten von wertbasierten Funktionen:\n",
    "\n",
    "- Zustandswertfunktion: gibt den erwarteten Ertrag aus, wenn **der Agent in einem bestimmten Zustand startet und danach f√ºr immer gem√§√ü der Strategie handelt**.\n",
    "- Aktionswertfunktion: gibt den erwarteten Ertrag aus, wenn **der Agent in einem bestimmten Zustand startet, in diesem Zustand eine bestimmte Aktion ausf√ºhrt** und danach f√ºr immer entsprechend der Strategie handelt.\n",
    "- Bei wertbasierten Methoden wird die Strategie nicht erlernt, sondern **man definiert die Strategie von Hand** und lernt eine Wertfunktion. Wenn wir eine optimale Wertfunktion haben, haben wir **eine optimale Strategie**.\n",
    "\n",
    "Es gibt zwei Arten von Methoden zur Aktualisierung der Wertfunktion:\n",
    "\n",
    "- Bei der *Monte-Carlo-Methode* aktualisieren wir die Wertfunktion anhand einer vollst√§ndigen Episode, d. h. wir **verwenden die tats√§chliche Rabattierte Belohnung dieser Episode**.\n",
    "- Mit *der TD-Learning-Methode* aktualisieren wir die Wertfunktion aus einem Schritt, wobei wir die unbekannte Belohnung durch **eine gesch√§tzte Belohnung, die TD-Zielvorgabe, ersetzen.**\n",
    "\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/summary-learning-mtds.jpg\" alt=\"Zusammenfassung\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Einf√ºhrung in das Q-Learning \n",
    "## Was ist Q-Learning?\n",
    "\n",
    "Q-Learning ist eine **off-policy wertbasierte Methode, die einen TD-Ansatz verwendet, um ihre Aktionswertfunktion zu trainieren:**\n",
    "\n",
    "- *Off-policy*: wir werden am Ende dieser Einheit dar√ºber sprechen.\n",
    "- *Wertbasierte Methode*: Findet die optimale Policy indirekt durch das Training einer Wert- oder Aktionswertfunktion, die uns **den Wert jedes Zustands oder jedes Zustands-Aktionspaares* liefert.\n",
    "- *TD-Ansatz:* **Aktualisiert seine Aktions-Wert-Funktion bei jedem Schritt statt am Ende der Episode.**\n",
    "\n",
    "**Q-Learning ist der Algorithmus, mit dem wir unsere Q-Funktion** trainieren, eine **Aktionswertfunktion**, die den Wert eines bestimmten Zustands und einer bestimmten Aktion in diesem Zustand bestimmt.\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-function.jpg\" alt=\"Q-function\"/>\n",
    "  <figcaption>Given a state and action, our Q Function outputs a state-action value (also called Q-value)</figcaption>\n",
    "</figure>\n",
    "Das **Q kommt von \"der Qualit√§t\" (dem Wert) dieser Handlung in diesem Zustand.\n",
    "\n",
    "Erinnern wir uns an den Unterschied zwischen Wert und Belohnung:\n",
    "\n",
    "- Der *Wert eines Zustands* oder eines *Zustands-Aktions-Paars* ist die erwartete kumulative Belohnung, die unser Agent erh√§lt, wenn er in diesem Zustand (oder Zustands-Aktions-Paar) startet und dann entsprechend seiner Strategie handelt.\n",
    "- Die *Belohnung* ist die **R√ºckmeldung, die ich von der Umwelt erhalte**, nachdem ich eine Aktion in einem Zustand ausgef√ºhrt habe.\n",
    "\n",
    "Intern wird unsere Q-Funktion durch **eine Q-Tabelle kodiert, eine Tabelle, in der jede Zelle einem Wert f√ºr ein Zustands-Aktionspaar entspricht.** Stellen Sie sich diese Q-Tabelle als **den Speicher oder Spickzettel unserer Q-Funktion** vor.\n",
    "\n",
    "Gehen wir ein Beispiel f√ºr ein Labyrinth durch.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Maze-1.jpg\" alt=\"Labyrinth Beispiel\"/>\n",
    "\n",
    "Die Q-Tabelle wird initialisiert. Deshalb sind alle Werte = 0. Diese Tabelle **enth√§lt f√ºr jeden Zustand und jede Aktion die entsprechenden Zustands-Aktions-Werte.** \n",
    "In diesem einfachen Beispiel ist der Zustand nur durch die Position der Maus definiert. Daher haben wir 2*3 Zeilen in unserer Q-Tabelle, eine Zeile f√ºr jede m√∂gliche Position der Maus. In komplexeren Szenarien k√∂nnte der Zustand mehr Informationen als die Position des Akteurs enthalten.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Maze-2.jpg\" alt=\"Maze example\"/>\n",
    "\n",
    "Hier sehen wir, dass der **Zustandsaktionswert des Ausgangszustands und des Aufstiegs 0 ist:**\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Maze-3.jpg\" alt=\"Labyrinth-Beispiel\"/>\n",
    "\n",
    "Also: Die Q-Funktion verwendet eine Q-Tabelle, **die den Wert jedes Zustands-Aktions-Paares enth√§lt.** Gegeben einen Zustand und eine Aktion, **sucht unsere Q-Funktion in ihrer Q-Tabelle, um den Wert auszugeben.**\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-function-2.jpg\" alt=\"Q-function\"/>\n",
    "</figure>\n",
    "\n",
    "Wenn wir rekapitulieren, ist *Q-Lernen* **der RL-Algorithmus, der:**\n",
    "\n",
    "- eine *Q-Funktion* (eine **Aktionswertfunktion**) trainiert, die intern eine **Q-Tabelle ist, die alle Werte des Zustands-Aktionspaares enth√§lt.**\n",
    "- Wenn ein Zustand und eine Aktion gegeben sind, **sucht unsere Q-Funktion in ihrer Q-Tabelle nach dem entsprechenden Wert**.\n",
    "- Wenn das Training abgeschlossen ist, **haben wir eine optimale Q-Funktion, was bedeutet, dass wir eine optimale Q-Tabelle haben.**\n",
    "- Und wenn wir **eine optimale Q-Funktion** haben, haben wir **eine optimale Policy**, da wir **die beste Aktion f√ºr jeden Zustand kennen.**\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/link-value-policy.jpg\" alt=\"Link value policy\"/>\n",
    "\n",
    "\n",
    "Am Anfang ist **unsere Q-Tabelle nutzlos, da sie beliebige Werte f√ºr jedes Zustands-Aktions-Paar enth√§lt** (meistens initialisieren wir die Q-Tabelle mit 0). Wenn der Agent **die Umgebung erkundet und wir die Q-Tabelle aktualisieren, wird sie uns eine immer bessere Ann√§herung** an die optimale Strategie liefern.\n",
    "\n",
    "<figure class=\"image table text-center m-0 w-full\">\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-1.jpg\" alt=\"Q-learning\"/>\n",
    "  <figcaption>We see here that with the training, our Q-table is better since, thanks to it, we can know the value of each state-action pair.</figcaption>\n",
    "</figure>\n",
    "\n",
    "Da wir nun wissen, was Q-Learning, Q-Funktionen und Q-Tabellen sind, **k√∂nnen wir uns nun n√§her mit dem Q-Learning-Algorithmus** besch√§ftigen.\n",
    "\n",
    "## Der Q-Learning-Algorithmus [[q-learning-algo]]\n",
    "\n",
    "Dies ist der Pseudocode des Q-Learning-Algorithmus; lassen Sie uns jeden Teil studieren und **an einem einfachen Beispiel sehen, wie er funktioniert, bevor wir ihn implementieren**. Lassen Sie sich davon nicht einsch√ºchtern, es ist einfacher als es aussieht! Wir werden jeden Schritt durchgehen.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg\" alt=\"Q-learning\"/>\n",
    "\n",
    "### Schritt 1: Wir initialisieren die Q-Tabelle [[Schritt1]]\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-3.jpg\" alt=\"Q-learning\"/>\n",
    "\n",
    "\n",
    "Wir m√ºssen die Q-Tabelle f√ºr jedes Zustands-Aktionspaar initialisieren. ** Meistens initialisieren wir mit Werten von 0.\n",
    "\n",
    "### Schritt 2: W√§hlen Sie eine Aktion mit Hilfe der Epsilon-Greedy-Strategie [[step2]]\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-4.jpg\" alt=\"Q-learning\"/>\n",
    "\n",
    "\n",
    "Die Epsilon-Greedy-Strategie ist eine Strategie, die den Kompromiss zwischen Erkundung und Ausbeutung behandelt.\n",
    "\n",
    "Die Idee ist, dass bei einem Anfangswert von …õ = 1,0:\n",
    "\n",
    "- Mit einer Wahrscheinlichkeit von 1 - …õ*: **Ausbeutung** (d. h. unser Agent w√§hlt die Aktion mit dem h√∂chsten Wert des Zustands-Aktionspaares).\n",
    "- Mit der Wahrscheinlichkeit …õ: **wir machen Exploration** (wir versuchen eine zuf√§llige Aktion).\n",
    "\n",
    "Zu Beginn des Trainings wird **die Wahrscheinlichkeit der Exploration sehr gro√ü sein, da …õ sehr hoch ist, also werden wir die meiste Zeit explorieren**. Aber wenn das Training weitergeht und folglich unsere **Q-Tabelle in ihren Sch√§tzungen immer besser wird, verringern wir schrittweise den Epsilon-Wert**, da wir immer weniger Exploration und mehr Ausbeutung brauchen werden.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-5.jpg\" alt=\"Q-learning\"/>\n",
    "\n",
    "\n",
    "### Schritt 3: F√ºhre Aktion At aus, erhalte Belohnung Rt+1 und n√§chsten Zustand St+1 [[Schritt3]]\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-6.jpg\" alt=\"Q-learning\"/>\n",
    "\n",
    "### Schritt 4: Aktualisiere Q(St, At) [[Schritt4]]\n",
    "\n",
    "Erinnern Sie sich daran, dass wir beim TD-Lernen unsere Policy oder Wertfunktion (je nach der von uns gew√§hlten RL-Methode) **nach einem Schritt der Interaktion** aktualisieren.\n",
    "\n",
    "Um unser TD-Ziel zu erzeugen, **benutzen wir die unmittelbare Belohnung \\\\(R_{t+1}\\) plus den Rabattierten Wert des n√§chsten Zustands**, der berechnet wird, indem wir die Aktion finden, die die aktuelle Q-Funktion im n√§chsten Zustand maximiert. (Wir nennen das Bootstrap).\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-7.jpg\" alt=\"Q-learning\"/>\n",
    "\n",
    "Daher lautet unsere \\\\(Q(S_t, A_t)\\\\) **Aktualisierungsformel wie folgt aus: **\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-8.jpg\" alt=\"Q-learning\"/>\n",
    "\n",
    "\n",
    "Das hei√üt, um unser \\\\(Q(S_t, A_t)\\\\) zu aktualisieren:\n",
    "\n",
    "- Wir brauchen \\\\(S_t, A_t, R_{t+1}, S_{t+1}\\\\).\n",
    "- Um unseren Q-Wert bei einem bestimmten Zustands-Aktionspaar zu aktualisieren, verwenden wir das TD-Ziel.\n",
    "\n",
    "Wie bilden wir das TD-Ziel?\n",
    "1. Wir erhalten die Belohnung \\\\(R_{t+1}\\), nachdem wir die Aktion \\\\(A_t\\\\) ausgef√ºhrt haben.\n",
    "2. Um den **besten Wert des Zustands-Aktionspaares** f√ºr den n√§chsten Zustand zu erhalten, verwenden wir eine gierige Strategie, um die n√§chstbeste Aktion auszuw√§hlen. Beachten Sie, dass es sich hierbei nicht um eine Epsilon-Greedy-Policy handelt, die immer die Aktion mit dem h√∂chsten Zustands-Aktionswert w√§hlt.\n",
    "\n",
    "Wenn die Aktualisierung dieses Q-Wertes abgeschlossen ist, beginnen wir in einem neuen Zustand und w√§hlen unsere Aktion **wieder mit einer Epsilon-Greedy-Strategie aus.**\n",
    "\n",
    "**Aus diesem Grund sagen wir, dass Q-Learning ein Off-Policy-Algorithmus ist.**\n",
    "\n",
    "## Off-Policy vs. On-Policy [[off-vs-on]]\n",
    "\n",
    "Der Unterschied ist subtil:\n",
    "\n",
    "- *Off-policy*: Verwendung **einer anderen Policy f√ºr das Handeln (Inferenz) und Aktualisieren (Training).**\n",
    "\n",
    "Beim Q-Learning beispielsweise unterscheidet sich die Epsilon-Greedy-Policy (acting policy) von der Greedy-Policy, die **zur Auswahl des besten Aktionswerts f√ºr den n√§chsten Zustand verwendet wird, um unseren Q-Wert zu aktualisieren (updating policy).**\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/off-on-1.jpg\" alt=\"Off-on policy\"/>\n",
    "  <figcaption>Acting Policy</figcaption>\n",
    "</figure>\n",
    "\n",
    "Unterscheidet sich von der Richtlinie, die wir w√§hrend des Trainingsteils verwenden:\n",
    "\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/off-on-2.jpg\" alt=\"Off-on policy\"/>\n",
    "  <figcaption>Updating policy</figcaption>\n",
    "</figure>\n",
    "\n",
    "- *On-Policy:* Verwendung derselben **Policy zum Handeln und Aktualisieren.\n",
    "\n",
    "Bei Sarsa, einem anderen wertbasierten Algorithmus, w√§hlt beispielsweise **die Epsilon-Greedy-Policy das n√§chste State-Action-Paar aus, nicht eine Greedy-Policy.**\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/off-on-3.jpg\" alt=\"Off-on policy\"/>\n",
    "    <figcaption>Sarsa</figcaption>\n",
    "</figure>\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/off-on-4.jpg\" alt=\"Off-on policy\"/>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ein Q-Learning-Beispiel [[q-learning-example]]\n",
    "\n",
    "Um Q-Learning besser zu verstehen, wollen wir ein einfaches Beispiel nehmen:\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Maze-Example-2.jpg\" alt=\"Labyrinth-Beispiel\"/>\n",
    "\n",
    "- Du bist eine Maus in diesem kleinen Labyrinth. Du **begannst immer am gleichen Startpunkt**.\n",
    "- Das Ziel ist es, **den gro√üen K√§sehaufen in der rechten unteren Ecke** zu fressen und das Gift zu vermeiden. Denn wer mag schon keinen K√§se?\n",
    "- Die Episode endet, wenn wir das Gift essen, **den gro√üen K√§sestapel essen** oder wenn wir mehr als f√ºnf Schritte gehen.\n",
    "- Die Lernrate betr√§gt 0,1\n",
    "- Die Rabattierungsrate (Gamma) ist 0,99\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/q-ex-1.jpg\" alt=\"Labyrinth-Beispiel\"/>\n",
    "\n",
    "\n",
    "Die Belohnungsfunktion sieht folgenderma√üen aus:\n",
    "\n",
    "- **+0:** Erreichen eines Zustands, in dem kein K√§se liegt.\n",
    "- **+1:** Erreichen eines Zustands mit einem kleinen K√§se darin.\n",
    "- **+10:** Erreichen des Zustands mit dem gro√üen K√§sehaufen.\n",
    "- **-10:** In den Staat mit dem Gift gehen und somit sterben.\n",
    "- **+0** Wenn wir mehr als f√ºnf Schritte gehen.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/q-ex-2.jpg\" alt=\"Labyrinth-Beispiel\"/>\n",
    "\n",
    "Um unseren Agenten auf eine optimale Strategie zu trainieren (also eine Strategie, die nach rechts, rechts, unten geht), **werden wir den Q-Learning-Algorithmus** verwenden.\n",
    "\n",
    "## Schritt 1: Initialisieren der Q-Tabelle [[Schritt1]]\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Example-1.jpg\" alt=\"Maze-Beispiel\"/>\n",
    "\n",
    "F√ºr den Moment ist **unsere Q-Tabelle nutzlos**; wir m√ºssen **unsere Q-Funktion mit dem Q-Learning-Algorithmus trainieren**.\n",
    "\n",
    "Tun wir dies f√ºr 2 Trainingszeitschritte:\n",
    "\n",
    "Trainingszeitschritt 1:\n",
    "\n",
    "## Schritt 2: W√§hlen Sie eine Aktion mit der Epsilon Greedy Strategy [[step2]]\n",
    "\n",
    "Da Epsilon gro√ü ist (= 1,0), w√§hle ich eine zuf√§llige Aktion. In diesem Fall gehe ich nach rechts.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/q-ex-3.jpg\" alt=\"Maze-Beispiel\"/>\n",
    "\n",
    "\n",
    "## Schritt 3: F√ºhre Aktion At aus, erhalte Rt+1 und St+1 [[Schritt3]]\n",
    "\n",
    "Wenn ich nach rechts gehe, bekomme ich einen kleinen K√§se, also R_{t+1} = 1\\\\) und ich bin in einem neuen Zustand.\n",
    "\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/q-ex-4.jpg\" alt=\"Labyrinth-Beispiel\"/>\n",
    "\n",
    "\n",
    "## Schritt 4: Q(St, At) aktualisieren [[Schritt4]]\n",
    "\n",
    "Wir k√∂nnen nun \\\\(Q(S_t, A_t)\\\\) mit unserer Formel aktualisieren.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/q-ex-5.jpg\" alt=\"Maze-Beispiel\"/>\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Example-4.jpg\" alt=\"Labyrinth-Beispiel\"/>\n",
    "\n",
    "Trainingszeitschritt 2:\n",
    "\n",
    "## Schritt 2: W√§hle eine Aktion mit der Epsilon Greedy Strategy [[step2-2]]\n",
    "\n",
    "**Ich nehme wieder eine zuf√§llige Aktion, da epsilon=0,99 gro√ü ist**. (Beachten Sie, dass wir epsilon ein wenig vermindern, da wir mit fortschreitendem Training immer weniger erforschen wollen).\n",
    "\n",
    "Ich habe die Aktion 'nach unten' gew√§hlt. **Dies ist keine gute Aktion, da sie mich zum Gift f√ºhrt**.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/q-ex-6.jpg\" alt=\"Labyrinth-Beispiel\"/>\n",
    "\n",
    "\n",
    "## Schritt 3: F√ºhre Aktion At aus, erhalte Rt+1 und St+1 [[Schritt3-3]]\n",
    "\n",
    "Da ich Gift gegessen habe, bekomme ich **(R_{t+1} = -10}), und ich sterbe**.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/q-ex-7.jpg\" alt=\"Labyrinth-Beispiel\"/>\n",
    "\n",
    "## Schritt 4: Q(St, At) aktualisieren [[step4-4]]\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/q-ex-8.jpg\" alt=\"Labyrinth-Beispiel\"/>\n",
    "\n",
    "Weil wir tot sind, beginnen wir eine neue Episode. Aber was wir hier sehen, ist, dass **mit zwei Erkundungsschritten mein Agent schlauer geworden ist.**\n",
    "\n",
    "Wenn wir weiterhin die Umgebung erforschen und ausnutzen und die Q-Werte anhand des TD-Ziels aktualisieren, wird uns die **Q-Tabelle eine immer bessere Ann√§herung liefern. Am Ende des Trainings erhalten wir eine Sch√§tzung der optimalen Q-Funktion.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning Rekapitulation\n",
    "\n",
    "\n",
    "*Q-Learning* **ist der RL-Algorithmus, der** :\n",
    "\n",
    "- Eine *Q-Funktion* trainiert, eine **Aktionswertfunktion**, die im internen Speicher durch eine *Q-Tabelle* **kodiert ist, die alle Werte des Zustands-Aktionspaares enth√§lt**.\n",
    "\n",
    "- Angesichts eines Zustands und einer Aktion sucht unsere Q-Funktion **in ihrer Q-Tabelle nach dem entsprechenden Wert**.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-function-2.jpg\" alt=\"Q-Funktion\" width=\"100%\"/>\n",
    "\n",
    "- Wenn das Training abgeschlossen ist, **haben wir eine optimale Q-Funktion, oder, √§quivalent, eine optimale Q-Tabelle**.\n",
    "\n",
    "- Und wenn wir **eine optimale Q-Funktion** haben, haben wir\n",
    "haben wir eine optimale Policy, da wir **f√ºr jeden Zustand die beste Aktion kennen, die zu ergreifen ist.**\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/link-value-policy.jpg\" alt=\"Link value policy\" width=\"100%\"/>\n",
    "\n",
    "Am Anfang ist unsere **Q-Tabelle jedoch nutzlos, da sie f√ºr jedes Zustands-Aktionspaar beliebige Werte angibt (meistens initialisieren wir die Q-Tabelle mit 0 Werten)**. Aber wenn wir die Umgebung erforschen und unsere Q-Tabelle aktualisieren, wird sie uns eine immer bessere Ann√§herung liefern.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/q-learning.jpeg\" alt=\"q-learning.jpeg\" width=\"100%\"/>\n",
    "\n",
    "Dies ist der Pseudocode f√ºr Q-Learning:\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg\" alt=\"Q-Learning\" width=\"100%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HEtx8Y8MqKfH"
   },
   "source": [
    "# Programmieren wir unseren ersten Reinforcement Learning Algorithmus üöÄ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kdxb1IhzTn0v"
   },
   "source": [
    "Um dieses Hands-On f√ºr den [Zertifizierungsprozess] (https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process) zu validieren, m√ºssen Sie Ihr trainiertes Taximodell an den Hub senden und **ein Ergebnis von >= 4,5** erhalten.\n",
    "\n",
    "Um Ihr Ergebnis zu finden, gehen Sie zur [Bestenliste] (https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) und suchen Sie Ihr Modell, **das Ergebnis = mean_reward - std of reward**\n",
    "\n",
    "Weitere Informationen √ºber den Zertifizierungsprozess finden Sie in diesem Abschnitt üëâ https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4gpxC1_kqUYe"
   },
   "source": [
    "## Abh√§ngigkeiten installieren und einen virtuellen Bildschirm erstellen üîΩ.\n",
    "\n",
    "Im Notebook m√ºssen wir ein Wiederholungsvideo erstellen. Dazu ben√∂tigen wir mit Colab **einen virtuellen Bildschirm, um die Umgebung zu rendern** (und somit die Bilder aufzunehmen).\n",
    "\n",
    "Daher wird die folgende Zelle die Bibliotheken installieren und einen virtuellen Bildschirm erstellen und starten üñ•\n",
    "\n",
    "Wir werden mehrere Bibliotheken installieren:\n",
    "\n",
    "- `gymnasium`: Enth√§lt die Umgebungen FrozenLake-v1 ‚õÑ und Taxi-v3 üöï.\n",
    "- pygame\": Wird f√ºr die FrozenLake-v1- und Taxi-v3-Benutzeroberfl√§che verwendet.\n",
    "- `numpy`: Wird f√ºr die Handhabung unserer Q-Tabelle verwendet.\n",
    "\n",
    "Der Hugging Face Hub ü§ó dient als zentraler Ort, an dem jeder Modelle und Datens√§tze teilen und erforschen kann. Er bietet Versionierung, Metriken, Visualisierungen und andere Funktionen, die eine einfache Zusammenarbeit mit anderen erm√∂glichen.\n",
    "\n",
    "Sie k√∂nnen alle verf√ºgbaren Deep-RL-Modelle (sofern sie Q Learning verwenden) hier sehen üëâ https://huggingface.co/models?other=q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9XaULfDZDvrC"
   },
   "outputs": [],
   "source": [
    "!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n71uTX7qqzz2"
   },
   "outputs": [],
   "source": [
    "!sudo apt-get update\n",
    "!sudo apt-get install -y python3-opengl\n",
    "!apt install ffmpeg xvfb\n",
    "!pip3 install pyvirtualdisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K6XC13pTfFiD"
   },
   "source": [
    "Um sicherzustellen, dass die neu installierten Bibliotheken verwendet werden, **ist es manchmal erforderlich, die Laufzeit des Notebooks neu zu starten**. Die n√§chste Zelle wird die **Laufzeitumgebung zum Absturz bringen, so dass Sie eine neue Verbindung herstellen und den Code von hier aus ausf√ºhren m√ºssen**. Dank dieses Tricks **k√∂nnen wir unseren virtuellen Bildschirm ausf√ºhren**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3kuZbWAkfHdg"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DaY1N4dBrabi"
   },
   "outputs": [],
   "source": [
    "# Virtual display\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "virtual_display = Display(visible=0, size=(1400, 900))\n",
    "virtual_display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W-7f-Swax_9x"
   },
   "source": [
    "## Importieren Sie die Pakete üì¶\n",
    "\n",
    "Zus√§tzlich zu den installierten Bibliotheken verwenden wir auch:\n",
    "\n",
    "- `random`: Um Zufallszahlen zu erzeugen (die f√ºr die Epsilon-Greedy-Policy n√ºtzlich sind).\n",
    "- `imageio`: Zur Erzeugung eines Wiederholungsvideos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VcNvOAQlysBJ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import random\n",
    "import imageio\n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "import pickle5 as pickle\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xp4-bXKIy1mQ"
   },
   "source": [
    "Wir sind jetzt bereit, unseren Q-Learning-Algorithmus zu programmieren üî•."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xya49aNJWVvv"
   },
   "source": [
    "# Teil 1: Gefrorener See ‚õÑ (nicht rutschige Version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NAvihuHdy9tw"
   },
   "source": [
    "## Erstellen und verstehen [FrozenLake-Umgebung ‚õÑ]((https://gymnasium.farama.org/environments/toy_text/frozen_lake/)\n",
    "---\n",
    "\n",
    "üí° Eine gute Angewohnheit, wenn Sie anfangen, eine Umgebung zu benutzen, ist es, ihre Dokumentation zu √ºberpr√ºfen\n",
    "\n",
    "üëâ https://gymnasium.farama.org/environments/toy_text/frozen_lake/\n",
    "\n",
    "---\n",
    "\n",
    "Wir werden unseren Q-Learning-Agenten darauf trainieren, **vom Startzustand (S) zum Zielzustand (G) zu navigieren, indem wir nur auf gefrorenen Kacheln (F) laufen und L√∂cher (H)** vermeiden.\n",
    "\n",
    "Wir k√∂nnen zwei Gr√∂√üen von Umgebungen haben:\n",
    "\n",
    "- `map_name=\"4x4\"`: eine 4x4-Gitterversion\n",
    "- `map_name=\"8x8\"`: eine 8x8-Gitterversion\n",
    "\n",
    "\n",
    "Die Umgebung hat zwei Modi:\n",
    "\n",
    "- `is_slippery=False`: Der Agent bewegt sich immer **in die beabsichtigte Richtung**, da der gefrorene See nicht rutschig ist (deterministisch).\n",
    "- is_slippery=True`: Der Agent **bewegt sich aufgrund der glatten Beschaffenheit des gefrorenen Sees nicht immer in die beabsichtigte Richtung** (stochastisch)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UaW_LHfS0PY2"
   },
   "source": [
    "F√ºr den Moment halten wir es einfach mit der 4x4 Karte und nicht rutschig.\n",
    "Wir f√ºgen einen Parameter namens `render_mode` hinzu, der angibt, wie die Umgebung visualisiert werden soll. In unserem Fall, weil wir **am Ende ein Video der Umgebung aufnehmen wollen, m√ºssen wir render_mode auf rgb_array** setzen.\n",
    "\n",
    "Wie [in der Dokumentation erkl√§rt](https://gymnasium.farama.org/api/env/#gymnasium.Env.render) \"rgb_array\": Gibt ein einzelnes Bild zur√ºck, das den aktuellen Zustand der Umgebung darstellt. Ein Frame ist ein np.ndarray mit der Form (x, y, 3), das RGB-Werte f√ºr ein x-mal-y-Pixelbild darstellt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IzJnb8O3y8up"
   },
   "outputs": [],
   "source": [
    "# Create the FrozenLake-v1 environment using 4x4 map and non-slippery version and render_mode=\"rgb_array\"\n",
    "env = gym.make() # TODO use the correct parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ji_UrI5l2zzn"
   },
   "source": [
    "### L√∂sung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jNxUbPMP0akP"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False, render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KASNViqL4tZn"
   },
   "source": [
    "Sie k√∂nnen Ihr eigenes benutzerdefiniertes Raster wie dieses erstellen:\n",
    "\n",
    "```python\n",
    "desc=[\"SFFF\", \"FHFH\", \"FFFH\", \"HFFG\"]\n",
    "gym.make('FrozenLake-v1', desc=desc, is_slippery=True)\n",
    "```\n",
    "\n",
    "aber wir werden vorerst die Standardumgebung verwenden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXbTfdeJ1Xi9"
   },
   "source": [
    "### Mal sehen, wie die Umwelt aussieht:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZNPG0g_UGCfh"
   },
   "outputs": [],
   "source": [
    "# We create our environment with gym.make(\"<name_of_the_environment>\")- `is_slippery=False`: The agent always moves in the intended direction due to the non-slippery nature of the frozen lake (deterministic).\n",
    "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
    "print(\"Observation Space\", env.observation_space)\n",
    "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2MXc15qFE0M9"
   },
   "source": [
    "Mit \"Observation Space Shape Discrete(16)\" sehen wir, dass die Beobachtung eine ganze Zahl ist, die die **aktuelle Position des Agenten als current_row * ncols + current_col darstellt (wobei sowohl row als auch col bei 0 beginnen)**.\n",
    "\n",
    "Zum Beispiel kann die Zielposition in der 4x4-Karte wie folgt berechnet werden: 3 * 4 + 3 = 15. Die Anzahl der m√∂glichen Beobachtungen ist abh√§ngig von der Gr√∂√üe der Karte. **Die 4x4-Karte hat zum Beispiel 16 m√∂gliche Beobachtungen**.\n",
    "\n",
    "\n",
    "So sieht zum Beispiel der Zustand = 0 aus:\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/frozenlake.png\" alt=\"FrozenLake\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "We5WqOBGLoSm"
   },
   "outputs": [],
   "source": [
    "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
    "print(\"Action Space Shape\", env.action_space.n)\n",
    "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MyxXwkI2Magx"
   },
   "source": [
    "Der Aktionsraum (die Menge der m√∂glichen Aktionen, die der Agent ausf√ºhren kann) ist diskret mit 4 verf√ºgbaren Aktionen üéÆ:\n",
    "- 0: LINKS GEHEN\n",
    "- 1: ABW√ÑRTS GEHEN\n",
    "- 2: RECHTS GEHEN\n",
    "- 3: NACH OBEN GEHEN\n",
    "\n",
    "Belohnungsfunktion üí∞:\n",
    "- Erreichen des Ziels: +1\n",
    "- Loch erreichen: 0\n",
    "- Gefrorenes Ziel erreichen: 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1pFhWblk3Awr"
   },
   "source": [
    "## Erstellen und Initialisieren der Q-Tabelle üóÑÔ∏è\n",
    "\n",
    "(üëÄ Schritt 1 des Pseudocodes)\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg\" alt=\"Q-Learning\" width=\"100%\"/>\n",
    "\n",
    "\n",
    "Es ist an der Zeit, unsere Q-Tabelle zu initialisieren! Um zu wissen, wie viele Zeilen (Zust√§nde) und Spalten (Aktionen) wir verwenden sollen, m√ºssen wir den Aktions- und Beobachtungsraum kennen. Wir kennen ihre Werte bereits von fr√ºher, aber wir wollen sie programmatisch erhalten, damit unser Algorithmus f√ºr verschiedene Umgebungen verallgemeinert werden kann. Gym bietet uns eine M√∂glichkeit, dies zu tun: `env.action_space.n` und `env.observation_space.n`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y3ZCdluj3k0l"
   },
   "outputs": [],
   "source": [
    "state_space =\n",
    "print(\"There are \", state_space, \" possible states\")\n",
    "\n",
    "action_space =\n",
    "print(\"There are \", action_space, \" possible actions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rCddoOXM3UQH"
   },
   "outputs": [],
   "source": [
    "# Let's create our Qtable of size (state_space, action_space) and initialized each values at 0 using np.zeros. np.zeros needs a tuple (a,b)\n",
    "def initialize_q_table(state_space, action_space):\n",
    "  Qtable =\n",
    "  return Qtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9YfvrqRt3jdR"
   },
   "outputs": [],
   "source": [
    "Qtable_frozenlake = initialize_q_table(state_space, action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67OdoKL63eDD"
   },
   "source": [
    "### L√∂sung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HuTKv3th3ohG"
   },
   "outputs": [],
   "source": [
    "state_space = env.observation_space.n\n",
    "print(\"There are \", state_space, \" possible states\")\n",
    "\n",
    "action_space = env.action_space.n\n",
    "print(\"There are \", action_space, \" possible actions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lnrb_nX33fJo"
   },
   "outputs": [],
   "source": [
    "# Let's create our Qtable of size (state_space, action_space) and initialized each values at 0 using np.zeros\n",
    "def initialize_q_table(state_space, action_space):\n",
    "  Qtable = np.zeros((state_space, action_space))\n",
    "  return Qtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y0WlgkVO3Jf9"
   },
   "outputs": [],
   "source": [
    "Qtable_frozenlake = initialize_q_table(state_space, action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Atll4Z774gri"
   },
   "source": [
    "## Definieren Sie die gierige Policy ü§ñ.\n",
    "\n",
    "Denken Sie daran, dass wir zwei Strategien haben, da Q-Learning ein **off-policy** Algorithmus ist. Das bedeutet, dass wir eine **unterschiedliche Strategie f√ºr das Handeln und die Aktualisierung der Wertfunktion** verwenden.\n",
    "\n",
    "- Epsilon-Greedy-Strategie (handelnde Strategie)\n",
    "- Greedy-Policy (AktualisierungsPolicy)\n",
    "\n",
    "Die Greedy-Policy wird auch die endg√ºltige Policy sein, die wir haben, wenn der Q-Learning-Agent das Training abgeschlossen hat. Die Greedy-Policy wird verwendet, um eine Aktion anhand der Q-Tabelle auszuw√§hlen.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/off-on-4.jpg\" alt=\"Q-Learning\" width=\"100%\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E3SCLmLX5bWG"
   },
   "outputs": [],
   "source": [
    "def greedy_policy(Qtable, state):\n",
    "  # Exploitation: take the action with the highest state, action value\n",
    "  action =\n",
    "\n",
    "  return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B2_-8b8z5k54"
   },
   "source": [
    "#### L√∂sung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "se2OzWGW5kYJ"
   },
   "outputs": [],
   "source": [
    "def greedy_policy(Qtable, state):\n",
    "  # Exploitation: take the action with the highest state, action value\n",
    "  action = np.argmax(Qtable[state][:])\n",
    "\n",
    "  return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "flILKhBU3yZ7"
   },
   "source": [
    "## Definieren Sie die Epsilon-Greedy-Policy ü§ñ.\n",
    "\n",
    "Epsilon-Greedy ist die Trainingsstrategie, die den Kompromiss zwischen Erkundung und Ausbeutung behandelt.\n",
    "\n",
    "Die Idee mit Epsilon-Greedy:\n",
    "\n",
    "- Mit *Wahrscheinlichkeit 1 - …õ* : **wir machen Exploitation** (d.h. unser Agent w√§hlt die Aktion mit dem h√∂chsten Wert des Zustands-Aktionspaares).\n",
    "\n",
    "- Mit *Wahrscheinlichkeit …õ*: **Exploration** (wir versuchen eine zuf√§llige Aktion).\n",
    "\n",
    "Mit fortschreitendem Training wird der Epsilon-Wert schrittweise **verringert, da wir immer weniger Exploration und mehr Exploitation ben√∂tigen**.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-4.jpg\" alt=\"Q-Learning\" width=\"100%\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Bj7x3in3_Pq"
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(Qtable, state, epsilon):\n",
    "  # Randomly generate a number between 0 and 1\n",
    "  random_num =\n",
    "  # if random_num > greater than epsilon --> exploitation\n",
    "  if random_num > epsilon:\n",
    "    # Take the action with the highest value given a state\n",
    "    # np.argmax can be useful here\n",
    "    action =\n",
    "  # else --> exploration\n",
    "  else:\n",
    "    action = # Take a random action\n",
    "\n",
    "  return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8R5ej1fS4P2V"
   },
   "source": [
    "#### L√∂sung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cYxHuckr4LiG"
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(Qtable, state, epsilon):\n",
    "  # Randomly generate a number between 0 and 1\n",
    "  random_num = random.uniform(0,1)\n",
    "  # if random_num > greater than epsilon --> exploitation\n",
    "  if random_num > epsilon:\n",
    "    # Take the action with the highest value given a state\n",
    "    # np.argmax can be useful here\n",
    "    action = greedy_policy(Qtable, state)\n",
    "  # else --> exploration\n",
    "  else:\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "  return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hW80DealcRtu"
   },
   "source": [
    "## Definieren Sie die Hyperparameter ‚öôÔ∏è\n",
    "\n",
    "Die Hyperparameter, die sich auf die Exploration beziehen, geh√∂ren zu den wichtigsten Parametern.\n",
    "\n",
    "- Wir m√ºssen sicherstellen, dass unser Agent **genug vom Zustandsraum erforscht**, um eine gute Wertann√§herung zu lernen. Um dies zu erreichen, m√ºssen wir einen progressiven Verfall von epsilon haben.\n",
    "- Wenn man epsilon zu schnell verringert (zu hohe decay_rate), **geht man das Risiko ein, dass der Agent feststeckt**, da er den Zustandsraum nicht ausreichend erforscht hat und daher das Problem nicht l√∂sen kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y1tWn0tycWZ1"
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "n_training_episodes = 10000  # Total training episodes\n",
    "learning_rate = 0.7          # Learning rate\n",
    "\n",
    "# Evaluation parameters\n",
    "n_eval_episodes = 100        # Total number of test episodes\n",
    "\n",
    "# Environment parameters\n",
    "env_id = \"FrozenLake-v1\"     # Name of the environment\n",
    "max_steps = 99               # Max steps per episode\n",
    "gamma = 0.95                 # Discounting rate\n",
    "eval_seed = []               # The evaluation seed of the environment\n",
    "\n",
    "# Exploration parameters\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.05            # Minimum exploration probability\n",
    "decay_rate = 0.0005            # Exponential decay rate for exploration prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cDb7Tdx8atfL"
   },
   "source": [
    "## Erstellen Sie die Trainingsschleifenmethode\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg\" alt=\"Q-Learning\" width=\"100%\"/>\n",
    "\n",
    "Die Trainingsschleife sieht folgenderma√üen aus:\n",
    "\n",
    "```\n",
    "F√ºr eine Episode in der Gesamtheit der Trainingsepisoden:\n",
    "\n",
    "Epsilon reduzieren (da wir immer weniger Erkundung brauchen)\n",
    "Die Umgebung zur√ºcksetzen\n",
    "\n",
    "  F√ºr Schritt in maximalen Zeitschritten:    \n",
    "    W√§hlen Sie die Aktion At mit epsilon gierig Policy\n",
    "    Ausf√ºhren der Aktion (a) und Beobachten des Ergebniszustands (s') und der Belohnung (r)\n",
    "    Aktualisierung des Q-Wertes Q(s,a) unter Verwendung der Bellman-Gleichung Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "    Wenn fertig, beende die Episode\n",
    "    Unser n√§chster Zustand ist der neue Zustand\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "paOynXy3aoJW"
   },
   "outputs": [],
   "source": [
    "def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n",
    "  for episode in tqdm(range(n_training_episodes)):\n",
    "    # Reduce epsilon (because we need less and less exploration)\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
    "    # Reset the environment\n",
    "    state, info = env.reset()\n",
    "    step = 0\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "\n",
    "    # repeat\n",
    "    for step in range(max_steps):\n",
    "      # Choose the action At using epsilon greedy policy\n",
    "      action =\n",
    "\n",
    "      # Take action At and observe Rt+1 and St+1\n",
    "      # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "      new_state, reward, terminated, truncated, info =\n",
    "\n",
    "      # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "      Qtable[state][action] =\n",
    "\n",
    "      # If terminated or truncated finish the episode\n",
    "      if terminated or truncated:\n",
    "        break\n",
    "\n",
    "      # Our next state is the new state\n",
    "      state = new_state\n",
    "  return Qtable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pnpk2ePoem3r"
   },
   "source": [
    "#### L√∂sung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IyZaYbUAeolw"
   },
   "outputs": [],
   "source": [
    "def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n",
    "  for episode in tqdm(range(n_training_episodes)):\n",
    "    # Reduce epsilon (because we need less and less exploration)\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
    "    # Reset the environment\n",
    "    state, info = env.reset()\n",
    "    step = 0\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "\n",
    "    # repeat\n",
    "    for step in range(max_steps):\n",
    "      # Choose the action At using epsilon greedy policy\n",
    "      action = epsilon_greedy_policy(Qtable, state, epsilon)\n",
    "\n",
    "      # Take action At and observe Rt+1 and St+1\n",
    "      # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "      new_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "      # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "      Qtable[state][action] = Qtable[state][action] + learning_rate * (reward + gamma * np.max(Qtable[new_state]) - Qtable[state][action])\n",
    "\n",
    "      # If terminated or truncated finish the episode\n",
    "      if terminated or truncated:\n",
    "        break\n",
    "\n",
    "      # Our next state is the new state\n",
    "      state = new_state\n",
    "  return Qtable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WLwKQ4tUdhGI"
   },
   "source": [
    "## Trainieren Sie den Q-Learning-Agenten üèÉ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DPBxfjJdTCOH"
   },
   "outputs": [],
   "source": [
    "Qtable_frozenlake = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_frozenlake)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yVeEhUCrc30L"
   },
   "source": [
    "## Mal sehen, wie unsere Q-Learning-Tabelle jetzt aussieht üëÄ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nmfchsTITw4q"
   },
   "outputs": [],
   "source": [
    "Qtable_frozenlake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pUrWkxsHccXD"
   },
   "source": [
    "## Die Bewertungsmethode üìù\n",
    "\n",
    "- Wir haben die Auswertungsmethode definiert, mit der wir unseren Q-Learning-Agenten testen werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jNl0_JO2cbkm"
   },
   "outputs": [],
   "source": [
    "def evaluate_agent(env, max_steps, n_eval_episodes, Q, seed):\n",
    "  \"\"\"\n",
    "  Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n",
    "  :param env: The evaluation environment\n",
    "  :param max_steps: Maximum number of steps per episode\n",
    "  :param n_eval_episodes: Number of episode to evaluate the agent\n",
    "  :param Q: The Q-table\n",
    "  :param seed: The evaluation seed array (for taxi-v3)\n",
    "  \"\"\"\n",
    "  episode_rewards = []\n",
    "  for episode in tqdm(range(n_eval_episodes)):\n",
    "    if seed:\n",
    "      state, info = env.reset(seed=seed[episode])\n",
    "    else:\n",
    "      state, info = env.reset()\n",
    "    step = 0\n",
    "    truncated = False\n",
    "    terminated = False\n",
    "    total_rewards_ep = 0\n",
    "\n",
    "    for step in range(max_steps):\n",
    "      # Take the action (index) that have the maximum expected future reward given that state\n",
    "      action = greedy_policy(Q, state)\n",
    "      new_state, reward, terminated, truncated, info = env.step(action)\n",
    "      total_rewards_ep += reward\n",
    "\n",
    "      if terminated or truncated:\n",
    "        break\n",
    "      state = new_state\n",
    "    episode_rewards.append(total_rewards_ep)\n",
    "  mean_reward = np.mean(episode_rewards)\n",
    "  std_reward = np.std(episode_rewards)\n",
    "\n",
    "  return mean_reward, std_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0jJqjaoAnxUo"
   },
   "source": [
    "## Bewerten Sie unseren Q-Learning-Agenten üìà\n",
    "\n",
    "- Normalerweise sollte man eine mittlere Belohnung von 1,0 haben.\n",
    "- Die **Umgebung ist relativ einfach**, da der Zustandsraum sehr klein ist (16). Man kann versuchen, sie durch die schl√ºpfrige Version zu ersetzen (https://gymnasium.farama.org/environments/toy_text/frozen_lake/), was Stochastizit√§t einf√ºhrt und die Umgebung komplexer macht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fAgB7s0HEFMm"
   },
   "outputs": [],
   "source": [
    "# Evaluate our Agent\n",
    "mean_reward, std_reward = evaluate_agent(env, max_steps, n_eval_episodes, Qtable_frozenlake, eval_seed)\n",
    "print(f\"Mean_reward={mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yxaP3bPdg1DV"
   },
   "source": [
    "## Ver√∂ffentliche unser trainiertes Modell auf dem Hub üî•\n",
    "\n",
    "Nun, da wir nach dem Training gute Ergebnisse gesehen haben, **k√∂nnen wir unser trainiertes Modell mit einer Zeile Code auf dem Hub ü§ó ver√∂ffentlichen**.\n",
    "\n",
    "Hier ist ein Beispiel f√ºr eine Model Card:\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/modelcard.png\" alt=\"Modellkarte\" width=\"100%\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kv0k1JQjpMq3"
   },
   "source": [
    "Unter der Haube verwendet der Hub Git-basierte Repositories (keine Sorge, wenn Sie nicht wissen, was Git ist), was bedeutet, dass Sie das Modell mit neuen Versionen aktualisieren k√∂nnen, wenn Sie experimentieren und Ihren Agenten verbessern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QZ5LrR-joIHD"
   },
   "source": [
    "#### Dieser Code darf nicht ver√§ndert werden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jex3i9lZ8ksX"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, snapshot_download\n",
    "from huggingface_hub.repocard import metadata_eval_result, metadata_save\n",
    "\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qo57HBn3W74O"
   },
   "outputs": [],
   "source": [
    "def record_video(env, Qtable, out_directory, fps=1):\n",
    "  \"\"\"\n",
    "  Generate a replay video of the agent\n",
    "  :param env\n",
    "  :param Qtable: Qtable of our agent\n",
    "  :param out_directory\n",
    "  :param fps: how many frame per seconds (with taxi-v3 and frozenlake-v1 we use 1)\n",
    "  \"\"\"\n",
    "  images = []\n",
    "  terminated = False\n",
    "  truncated = False\n",
    "  state, info = env.reset(seed=random.randint(0,500))\n",
    "  img = env.render()\n",
    "  images.append(img)\n",
    "  while not terminated or truncated:\n",
    "    # Take the action (index) that have the maximum expected future reward given that state\n",
    "    action = np.argmax(Qtable[state][:])\n",
    "    state, reward, terminated, truncated, info = env.step(action) # We directly put next_state = state for recording logic\n",
    "    img = env.render()\n",
    "    images.append(img)\n",
    "  imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4mdUTKkGnUd"
   },
   "outputs": [],
   "source": [
    "def push_to_hub(\n",
    "    repo_id, model, env, video_fps=1, local_repo_path=\"hub\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate, Generate a video and Upload a model to Hugging Face Hub.\n",
    "    This method does the complete pipeline:\n",
    "    - It evaluates the model\n",
    "    - It generates the model card\n",
    "    - It generates a replay video of the agent\n",
    "    - It pushes everything to the Hub\n",
    "\n",
    "    :param repo_id: repo_id: id of the model repository from the Hugging Face Hub\n",
    "    :param env\n",
    "    :param video_fps: how many frame per seconds to record our video replay\n",
    "    (with taxi-v3 and frozenlake-v1 we use 1)\n",
    "    :param local_repo_path: where the local repository is\n",
    "    \"\"\"\n",
    "    _, repo_name = repo_id.split(\"/\")\n",
    "\n",
    "    eval_env = env\n",
    "    api = HfApi()\n",
    "\n",
    "    # Step 1: Create the repo\n",
    "    repo_url = api.create_repo(\n",
    "        repo_id=repo_id,\n",
    "        exist_ok=True,\n",
    "    )\n",
    "\n",
    "    # Step 2: Download files\n",
    "    repo_local_path = Path(snapshot_download(repo_id=repo_id))\n",
    "\n",
    "    # Step 3: Save the model\n",
    "    if env.spec.kwargs.get(\"map_name\"):\n",
    "        model[\"map_name\"] = env.spec.kwargs.get(\"map_name\")\n",
    "        if env.spec.kwargs.get(\"is_slippery\", \"\") == False:\n",
    "            model[\"slippery\"] = False\n",
    "\n",
    "    # Pickle the model\n",
    "    with open((repo_local_path) / \"q-learning.pkl\", \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "    # Step 4: Evaluate the model and build JSON with evaluation metrics\n",
    "    mean_reward, std_reward = evaluate_agent(\n",
    "        eval_env, model[\"max_steps\"], model[\"n_eval_episodes\"], model[\"qtable\"], model[\"eval_seed\"]\n",
    "    )\n",
    "\n",
    "    evaluate_data = {\n",
    "        \"env_id\": model[\"env_id\"],\n",
    "        \"mean_reward\": mean_reward,\n",
    "        \"n_eval_episodes\": model[\"n_eval_episodes\"],\n",
    "        \"eval_datetime\": datetime.datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "    # Write a JSON file called \"results.json\" that will contain the\n",
    "    # evaluation results\n",
    "    with open(repo_local_path / \"results.json\", \"w\") as outfile:\n",
    "        json.dump(evaluate_data, outfile)\n",
    "\n",
    "    # Step 5: Create the model card\n",
    "    env_name = model[\"env_id\"]\n",
    "    if env.spec.kwargs.get(\"map_name\"):\n",
    "        env_name += \"-\" + env.spec.kwargs.get(\"map_name\")\n",
    "\n",
    "    if env.spec.kwargs.get(\"is_slippery\", \"\") == False:\n",
    "        env_name += \"-\" + \"no_slippery\"\n",
    "\n",
    "    metadata = {}\n",
    "    metadata[\"tags\"] = [env_name, \"q-learning\", \"reinforcement-learning\", \"custom-implementation\"]\n",
    "\n",
    "    # Add metrics\n",
    "    eval = metadata_eval_result(\n",
    "        model_pretty_name=repo_name,\n",
    "        task_pretty_name=\"reinforcement-learning\",\n",
    "        task_id=\"reinforcement-learning\",\n",
    "        metrics_pretty_name=\"mean_reward\",\n",
    "        metrics_id=\"mean_reward\",\n",
    "        metrics_value=f\"{mean_reward:.2f} +/- {std_reward:.2f}\",\n",
    "        dataset_pretty_name=env_name,\n",
    "        dataset_id=env_name,\n",
    "    )\n",
    "\n",
    "    # Merges both dictionaries\n",
    "    metadata = {**metadata, **eval}\n",
    "\n",
    "    model_card = f\"\"\"\n",
    "  # **Q-Learning** Agent playing1 **{env_id}**\n",
    "  This is a trained model of a **Q-Learning** agent playing **{env_id}** .\n",
    "\n",
    "  ## Usage\n",
    "\n",
    "  ```python\n",
    "\n",
    "  model = load_from_hub(repo_id=\"{repo_id}\", filename=\"q-learning.pkl\")\n",
    "\n",
    "  # Don't forget to check if you need to add additional attributes (is_slippery=False etc)\n",
    "  env = gym.make(model[\"env_id\"])\n",
    "  ```\n",
    "  \"\"\"\n",
    "\n",
    "    evaluate_agent(env, model[\"max_steps\"], model[\"n_eval_episodes\"], model[\"qtable\"], model[\"eval_seed\"])\n",
    "\n",
    "    readme_path = repo_local_path / \"README.md\"\n",
    "    readme = \"\"\n",
    "    print(readme_path.exists())\n",
    "    if readme_path.exists():\n",
    "        with readme_path.open(\"r\", encoding=\"utf8\") as f:\n",
    "            readme = f.read()\n",
    "    else:\n",
    "        readme = model_card\n",
    "\n",
    "    with readme_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(readme)\n",
    "\n",
    "    # Save our metrics to Readme metadata\n",
    "    metadata_save(readme_path, metadata)\n",
    "\n",
    "    # Step 6: Record a video\n",
    "    video_path = repo_local_path / \"replay.mp4\"\n",
    "    record_video(env, model[\"qtable\"], video_path, video_fps)\n",
    "\n",
    "    # Step 7. Push everything to the Hub\n",
    "    api.upload_folder(\n",
    "        repo_id=repo_id,\n",
    "        folder_path=repo_local_path,\n",
    "        path_in_repo=\".\",\n",
    "    )\n",
    "\n",
    "    print(\"Your model is pushed to the Hub. You can view your model here: \", repo_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81J6cet_ogSS"
   },
   "source": [
    "### .\n",
    "\n",
    "Mit \"push_to_hub\" **werten Sie aus, zeichnen ein Replay auf, generieren eine Modellkarte Ihres Agenten und schieben sie an den Hub**.\n",
    "\n",
    "This way:\n",
    "- Sie k√∂nnen **unsere Arbeit vorf√ºhren** üî•.\n",
    "- Sie k√∂nnen **Ihren Agenten beim Spielen visualisieren** üëÄ\n",
    "- Du kannst **einen Agenten mit der Community teilen, den andere nutzen k√∂nnen** üíæ\n",
    "- Sie k√∂nnen **auf eine Bestenliste zugreifen üèÜ um zu sehen, wie gut Ihr Agent im Vergleich zu Ihren Klassenkameraden abschneidet** üëâ https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cWnFC0iZooTw"
   },
   "source": [
    "Um Ihr Modell mit der Gemeinschaft teilen zu k√∂nnen, sind drei weitere Schritte erforderlich:\n",
    "\n",
    "1Ô∏è‚É£ (Falls noch nicht geschehen) Erstellen Sie ein Konto f√ºr HF ‚û° https://huggingface.co/join\n",
    "\n",
    "2Ô∏è‚É£ Melde dich an und speichere dann dein Authentifizierungs-Token von der Hugging Face Website.\n",
    "- Erstellen Sie ein neues Token (https://huggingface.co/settings/tokens) **mit Schreibrolle**\n",
    "\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/create-token.jpg\" alt=\"HF-Token erstellen\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QB5nIcxR8paT"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GyWc1x3-o3xG"
   },
   "source": [
    "Wenn Sie kein Google Colab oder ein Jupyter Notebook verwenden m√∂chten, m√ºssen Sie stattdessen diesen Befehl verwenden: `huggingface-cli login` (oder `login`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gc5AfUeFo3xH"
   },
   "source": [
    "3Ô∏è‚É£ Wir sind jetzt bereit, unseren trainierten Agenten mit der Funktion \"push_to_hub()\" an den ü§ó Hub üî• zu senden.\n",
    "\n",
    "- Erstellen wir **das Modellw√∂rterbuch, das die Hyperparameter und die Q_table** enth√§lt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FiMqxqVHg0I4"
   },
   "outputs": [],
   "source": [
    "model = {\n",
    "    \"env_id\": env_id,\n",
    "    \"max_steps\": max_steps,\n",
    "    \"n_training_episodes\": n_training_episodes,\n",
    "    \"n_eval_episodes\": n_eval_episodes,\n",
    "    \"eval_seed\": eval_seed,\n",
    "\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"gamma\": gamma,\n",
    "\n",
    "    \"max_epsilon\": max_epsilon,\n",
    "    \"min_epsilon\": min_epsilon,\n",
    "    \"decay_rate\": decay_rate,\n",
    "\n",
    "    \"qtable\": Qtable_frozenlake\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9kld-AEso3xH"
   },
   "source": [
    "F√ºllen wir die Funktion \"push_to_hub\":\n",
    "\n",
    "- `repo_id`: der Name des Hugging Face Hub Repository, das erstellt/aktualisiert werden soll `\n",
    "(repo_id = {Benutzername}/{repo_name})`\n",
    "üí° Eine gute `repo_id` ist `{username}/q-{env_id}`\n",
    "- model\": unser Modellw√∂rterbuch mit den Hyperparametern und der Q-Tabelle.\n",
    "- `env`: die Umgebung.\n",
    "- `commit_message`: die Nachricht der √úbergabe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5sBo2umnXpPd"
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RpOTtSt83kPZ"
   },
   "outputs": [],
   "source": [
    "username = \"\" # FILL THIS\n",
    "repo_name = \"q-FrozenLake-v1-4x4-noSlippery\"\n",
    "push_to_hub(\n",
    "    repo_id=f\"{username}/{repo_name}\",\n",
    "    model=model,\n",
    "    env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E2875IGsprzq"
   },
   "source": [
    "Herzlichen Gl√ºckwunsch ü•≥ Sie haben soeben Ihren ersten Reinforcement Learning Agent von Grund auf implementiert, trainiert und hochgeladen.\n",
    "FrozenLake-v1 no_slippery ist eine sehr einfache Umgebung, versuchen wir eine schwierigere üî•."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18lN8Bz7yvLt"
   },
   "source": [
    "# Teil 2: Taxi-v3 üöñ\n",
    "\n",
    "## Erstellen und verstehen [Taxi-v3 üöï](https://gymnasium.farama.org/environments/toy_text/taxi/)\n",
    "---\n",
    "\n",
    "üí° Eine gute Angewohnheit, wenn du anf√§ngst, eine Umgebung zu benutzen, ist es, ihre Dokumentation zu √ºberpr√ºfen.\n",
    "\n",
    "üëâ https://gymnasium.farama.org/environments/toy_text/taxi/\n",
    "\n",
    "---\n",
    "\n",
    "In `Taxi-v3` üöï gibt es vier bestimmte Orte in der Gitterwelt, die mit R(ed), G(reen), Y(ellow) und B(lue) bezeichnet werden.\n",
    "\n",
    "Zu Beginn der Episode **startet das Taxi an einem zuf√§lligen Platz** und der Fahrgast befindet sich an einem zuf√§lligen Ort. Das Taxi f√§hrt zum Standort des Fahrgastes, **nimmt den Fahrgast auf**, f√§hrt zum Zielort des Fahrgastes (einem anderen der vier angegebenen Orte) und setzt den Fahrgast dann **ab**. Sobald der Fahrgast abgesetzt ist, endet die Episode.\n",
    "\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/taxi.png\" alt=\"Taxi\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gL0wpeO8gpej"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\", render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gBOaXgtsrmtT"
   },
   "source": [
    "Es gibt **500 diskrete Zust√§nde, da es 25 Taxipositionen, 5 m√∂gliche Standorte des Fahrgastes** (einschlie√ülich des Falles, in dem sich der Fahrgast im Taxi befindet) und **4 Zielstandorte** gibt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_TPNaGSZrgqA"
   },
   "outputs": [],
   "source": [
    "state_space = env.observation_space.n\n",
    "print(\"There are \", state_space, \" possible states\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CdeeZuokrhit"
   },
   "outputs": [],
   "source": [
    "action_space = env.action_space.n\n",
    "print(\"There are \", action_space, \" possible actions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R1r50Advrh5Q"
   },
   "source": [
    "Der Aktionsraum (die Menge der m√∂glichen Aktionen, die der Agent ausf√ºhren kann) ist diskret mit **6 verf√ºgbaren Aktionen üéÆ**:\n",
    "\n",
    "- 0: nach S√ºden gehen\n",
    "- 1: nach Norden ziehen\n",
    "- 2: nach Osten ziehen\n",
    "- 3: nach Westen fahren\n",
    "- 4: Fahrgast aufnehmen\n",
    "- 5: Fahrgast absetzen\n",
    "\n",
    "Belohnungsfunktion üí∞:\n",
    "\n",
    "-1 pro Schritt, wenn keine andere Belohnung ausgel√∂st wird.\n",
    "- +20 Fahrgast abliefern.\n",
    "-10 Unerlaubtes Ausf√ºhren der Aktionen \"Abholen\" und \"Absetzen\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "US3yDXnEtY9I"
   },
   "outputs": [],
   "source": [
    "# Create our Q table with state_size rows and action_size columns (500x6)\n",
    "Qtable_taxi = initialize_q_table(state_space, action_space)\n",
    "print(Qtable_taxi)\n",
    "print(\"Q-table shape: \", Qtable_taxi .shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gUMKPH0_LJyH"
   },
   "source": [
    "## Definieren Sie die Hyperparameter ‚öôÔ∏è\n",
    "\n",
    "‚ö† VER√ÑNDERN SIE NICHT EVAL_SEED: Das eval_seed-Array **erm√∂glicht es uns, Ihren Agenten mit denselben Taxistartpositionen f√ºr jeden Klassenkameraden zu bewerten**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AB6n__hhg7YS"
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "n_training_episodes = 25000   # Total training episodes\n",
    "learning_rate = 0.7           # Learning rate\n",
    "\n",
    "# Evaluation parameters\n",
    "n_eval_episodes = 100        # Total number of test episodes\n",
    "\n",
    "# DO NOT MODIFY EVAL_SEED\n",
    "eval_seed = [16,54,165,177,191,191,120,80,149,178,48,38,6,125,174,73,50,172,100,148,146,6,25,40,68,148,49,167,9,97,164,176,61,7,54,55,\n",
    " 161,131,184,51,170,12,120,113,95,126,51,98,36,135,54,82,45,95,89,59,95,124,9,113,58,85,51,134,121,169,105,21,30,11,50,65,12,43,82,145,152,97,106,55,31,85,38,\n",
    " 112,102,168,123,97,21,83,158,26,80,63,5,81,32,11,28,148] # Evaluation seed, this ensures that all classmates agents are trained on the same taxi starting position\n",
    "                                                          # Each seed has a specific starting state\n",
    "\n",
    "# Environment parameters\n",
    "env_id = \"Taxi-v3\"           # Name of the environment\n",
    "max_steps = 99               # Max steps per episode\n",
    "gamma = 0.95                 # Discounting rate\n",
    "\n",
    "# Exploration parameters\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.05           # Minimum exploration probability\n",
    "decay_rate = 0.005            # Exponential decay rate for exploration prob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1TMORo1VLTsX"
   },
   "source": [
    "## Trainieren Sie unseren Q-Learning-Agenten üèÉ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WwP3Y2z2eS-K"
   },
   "outputs": [],
   "source": [
    "Qtable_taxi = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_taxi)\n",
    "Qtable_taxi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wPdu0SueLVl2"
   },
   "source": [
    "## Erstellen Sie ein Modellw√∂rterbuch üíæ und ver√∂ffentlichen Sie unser trainiertes Modell auf dem Hub üî•.\n",
    "\n",
    "- Wir erstellen ein Modell-W√∂rterbuch, das alle Trainings-Hyperparameter f√ºr die Reproduzierbarkeit und die Q-Tabelle enthalten wird.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0a1FpE_3hNYr"
   },
   "outputs": [],
   "source": [
    "model = {\n",
    "    \"env_id\": env_id,\n",
    "    \"max_steps\": max_steps,\n",
    "    \"n_training_episodes\": n_training_episodes,\n",
    "    \"n_eval_episodes\": n_eval_episodes,\n",
    "    \"eval_seed\": eval_seed,\n",
    "\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"gamma\": gamma,\n",
    "\n",
    "    \"max_epsilon\": max_epsilon,\n",
    "    \"min_epsilon\": min_epsilon,\n",
    "    \"decay_rate\": decay_rate,\n",
    "\n",
    "    \"qtable\": Qtable_taxi\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dhQtiQozhOn1"
   },
   "outputs": [],
   "source": [
    "username = \"\" # FILL THIS\n",
    "repo_name = \"\" # FILL THIS\n",
    "push_to_hub(\n",
    "    repo_id=f\"{username}/{repo_name}\",\n",
    "    model=model,\n",
    "    env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZgSdjgbIpRti"
   },
   "source": [
    "Jetzt, wo es auf dem Hub ist, kannst du die Ergebnisse deines Taxi-v3 mit deinen Klassenkameraden in der Bestenliste vergleichen üèÜ üëâ https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard\n",
    "\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/taxi-leaderboard.png\" alt=\"Taxi-Rangliste\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bzgIO70c0bu2"
   },
   "source": [
    "# Teil 3: Laden vom Hub üîΩ.\n",
    "\n",
    "Das Erstaunliche an Hugging Face Hub ü§ó ist, dass du ganz einfach leistungsstarke Modelle aus der Community laden kannst.\n",
    "\n",
    "Das Laden eines gespeicherten Modells aus dem Hub ist wirklich einfach:\n",
    "\n",
    "1. Du gehst auf https://huggingface.co/models?other=q-learning, um die Liste aller gespeicherten q-learning-Modelle zu sehen.\n",
    "2. Sie w√§hlen eines aus und kopieren seine repo_id\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/copy-id.png\" alt=\"ID kopieren\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTth6thRoC6X"
   },
   "source": [
    "3. Dann m√ºssen wir nur `load_from_hub` mit verwenden:\n",
    "- Die Repo_id\n",
    "- Der Dateiname: das gespeicherte Modell innerhalb der Repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EtrfoTaBoNrd"
   },
   "source": [
    "#### Dieser Code darf nicht ver√§ndert werden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eo8qEzNtCaVI"
   },
   "outputs": [],
   "source": [
    "from urllib.error import HTTPError\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "\n",
    "def load_from_hub(repo_id: str, filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Download a model from Hugging Face Hub.\n",
    "    :param repo_id: id of the model repository from the Hugging Face Hub\n",
    "    :param filename: name of the model zip file from the repository\n",
    "    \"\"\"\n",
    "    # Get the model from the Hub, download and cache the model on your local disk\n",
    "    pickle_model = hf_hub_download(\n",
    "        repo_id=repo_id,\n",
    "        filename=filename\n",
    "    )\n",
    "\n",
    "    with open(pickle_model, 'rb') as f:\n",
    "      downloaded_model_file = pickle.load(f)\n",
    "\n",
    "    return downloaded_model_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b_sM2gNioPZH"
   },
   "source": [
    "### ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JUm9lz2gCQcU"
   },
   "outputs": [],
   "source": [
    "model = load_from_hub(repo_id=\"ThomasSimonini/q-Taxi-v3\", filename=\"q-learning.pkl\") # Try to use another model\n",
    "\n",
    "print(model)\n",
    "env = gym.make(model[\"env_id\"])\n",
    "\n",
    "evaluate_agent(env, model[\"max_steps\"], model[\"n_eval_episodes\"], model[\"qtable\"], model[\"eval_seed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O7pL8rg1MulN"
   },
   "outputs": [],
   "source": [
    "model = load_from_hub(repo_id=\"ThomasSimonini/q-FrozenLake-v1-no-slippery\", filename=\"q-learning.pkl\") # Try to use another model\n",
    "\n",
    "env = gym.make(model[\"env_id\"], is_slippery=False)\n",
    "\n",
    "evaluate_agent(env, model[\"max_steps\"], model[\"n_eval_episodes\"], model[\"qtable\"], model[\"eval_seed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQAwLnYFPk-s"
   },
   "source": [
    "## Einige zus√§tzliche Herausforderungen üèÜ\n",
    "\n",
    "Die beste Art zu lernen **ist, Dinge selbst auszuprobieren**! Wie Sie gesehen haben, ist der derzeitige Agent nicht besonders gut. Als ersten Vorschlag k√∂nnen Sie f√ºr mehr Schritte trainieren. Bei 1.000.000 Schritten haben wir tolle Ergebnisse gesehen!\n",
    "\n",
    "In der [Rangliste] (https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) findest du deine Agenten. K√∂nnen Sie sich an die Spitze setzen?\n",
    "\n",
    "Hier sind einige Ideen, um in der Rangliste aufzusteigen:\n",
    "\n",
    "* Trainiere mehr Schritte\n",
    "* Probiere verschiedene Hyperparameter aus, indem du dir ansiehst, was deine Klassenkameraden gemacht haben.\n",
    "* Pushe dein neu trainiertes Modell** auf dem Hub üî•.\n",
    "\n",
    "Sind Ihnen das Laufen auf Eis und das Fahren von Taxis zu langweilig? Versuche, die **Umgebung** zu ver√§ndern, warum nicht die rutschige Version von FrozenLake-v1 benutzen? Schau dir an, wie sie funktionieren [mit Hilfe der Turnhallendokumentation] (https://gymnasium.farama.org/) und hab Spa√ü üéâ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-fW-EU5WejJ"
   },
   "source": [
    "_____________________________________________________________________\n",
    "Herzlichen Gl√ºckwunsch ü•≥, Sie haben soeben Ihren ersten Reinforcement Learning Agent implementiert, trainiert und hochgeladen.\n",
    "\n",
    "Das Verst√§ndnis von Q-Learning ist ein **wichtiger Schritt zum Verst√§ndnis wertbasierter Methoden**.\n",
    "\n",
    "In der n√§chsten Einheit mit Deep Q-Learning werden wir sehen, dass das Erstellen und Aktualisieren einer Q-Tabelle zwar eine gute Strategie war - **aber nicht skalierbar ist.**\n",
    "\n",
    "Stellen Sie sich zum Beispiel vor, Sie erstellen einen Agenten, der lernt, Doom zu spielen.\n",
    "\n",
    "<img src=\"https://vizdoom.cs.put.edu.pl/user/pages/01.tutorial/basic.png\" alt=\"Doom\"/>\n",
    "\n",
    "Doom ist eine gro√üe Umgebung mit einem riesigen Zustandsraum (Millionen von verschiedenen Zust√§nden). Das Erstellen und Aktualisieren einer Q-Tabelle f√ºr diese Umgebung w√§re nicht effizient.\n",
    "\n",
    "Deshalb werden wir uns in der n√§chsten Einheit mit Deep Q-Learning besch√§ftigen, einem Algorithmus, **bei dem wir ein neuronales Netz verwenden, das bei einem bestimmten Zustand die verschiedenen Q-Werte f√ºr jede Aktion ann√§hert**.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/atari-envs.gif\" alt=\"Umgebungen\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BjLhT70TEZIn"
   },
   "source": [
    "Wir sehen uns in Referat 3! üî•\n",
    "\n",
    "## Lernt weiter, bleibt fantastisch ü§ó"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "67OdoKL63eDD",
    "B2_-8b8z5k54",
    "8R5ej1fS4P2V",
    "Pnpk2ePoem3r"
   ],
   "include_colab_link": true,
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
