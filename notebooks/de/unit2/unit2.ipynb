{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/notebooks/unit2/unit2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"In Colab öffnen\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njb_ProuHiOe"
   },
   "source": [
    "# Einheit 2: Q-Learning mit FrozenLake-v1 ⛄ und Taxi-v3 🚕\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/thumbnail.jpg\" alt=\"Unit 2 Thumbnail\">\n",
    "\n",
    "In diesem Notizbuch **programmieren Sie Ihren ersten Reinforcement Learning-Agenten von Grund auf**, um FrozenLake ❄️ mit Q-Learning zu spielen, ihn mit der Community zu teilen und mit verschiedenen Konfigurationen zu experimentieren.\n",
    "\n",
    "⬇️ Hier ist ein Beispiel dafür, was **Sie in nur wenigen Minuten erreichen werden.** ⬇️\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vRU_vXBrl1Jx"
   },
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/envs.gif\" alt=\"Umgebungen\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DPTBOv9HYLZ2"
   },
   "source": [
    "###🎮 Umgebungen:\n",
    "\n",
    "- [FrozenLake-v1](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)\n",
    "- [Taxi-v3](https://gymnasium.farama.org/environments/toy_text/taxi/)\n",
    "\n",
    "###📚 RL-Library:\n",
    "\n",
    "- Python und NumPy\n",
    "- [Gymnasium](https://gymnasium.farama.org/)\n",
    "\n",
    "Wir sind ständig bemüht, unsere Tutorials zu verbessern. **Wenn Sie also Probleme in diesem Notizbuch finden**, öffnen Sie bitte [einen Fehler im GitHub Repo](https://github.com/huggingface/deep-rl-class/issues)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4i6tjI2tHQ8j"
   },
   "source": [
    "## Ziele dieses Notizbuchs 🏆\n",
    "\n",
    "Am Ende des Notizbuchs werden Sie:\n",
    "\n",
    "- In der Lage sein, **Gymnasium**, die Umgebungsbibliothek, zu benutzen.\n",
    "- In der Lage sein, einen Q-Learning-Agenten von Grund auf zu programmieren.\n",
    "- In der Lage sein, **Ihren trainierten Agenten und den Code auf den Hub** mit einer schönen Videowiedergabe und einer Bewertung zu pushen 🔥.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "viNzVbVaYvY3"
   },
   "source": [
    "## Dieses Notizbuch stammt aus dem Kurs Deep Reinforcement Learning\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/deep-rl-course-illustration.jpg\" alt=\"Deep RL Course illustration\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6p5HnEefISCB"
   },
   "source": [
    "In diesem kostenlosen Kurs lernen Sie:\n",
    "\n",
    "- 📖 Deep Reinforcement Learning in **Theorie und Praxis** studieren.\n",
    "- 🧑‍💻 Lernen Sie, **berühmte Deep RL-Bibliotheken** wie Stable Baselines3, RL Baselines3 Zoo, CleanRL und Sample Factory 2.0 zu verwenden.\n",
    "- 🤖 Trainieren Sie **Agenten in einzigartigen Umgebungen**.\n",
    "\n",
    "Und mehr, siehe 📚 den Lehrplan 👉 https://simoninithomas.github.io/deep-rl-course\n",
    "\n",
    "Vergessen Sie nicht, sich **<a href=\"http://eepurl.com/ic5ZUD\">für den Kurs anzumelden</a>** (wir sammeln Ihre E-Mail, um Ihnen **die Links zu senden, wenn die einzelnen Einheiten veröffentlicht werden, und Sie über die Herausforderungen und Aktualisierungen zu informieren).**\n",
    "\n",
    "\n",
    "Der beste Weg, um in Kontakt zu bleiben, ist, unserem Discord-Server beizutreten, um sich mit der Community und mit uns auszutauschen 👉🏻 https://discord.gg/ydHrjt3WP5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-mo_6rXIjRi"
   },
   "source": [
    "## Voraussetzungen 🏗️\n",
    "\n",
    "Bevor Sie sich mit dem Notebook beschäftigen, müssen Sie:\n",
    "\n",
    "🔲 📚 **Studieren Sie [Q-Learning by reading Unit 2](https://huggingface.co/deep-rl-course/unit2/introduction)** 🤗"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2ONOODsyrMU"
   },
   "source": [
    "## Eine kleine Zusammenfassung von Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V68VveLacfxJ"
   },
   "source": [
    "*Q-Learning* **ist der RL-Algorithmus, der**:\n",
    "\n",
    "- Eine *Q-Funktion* trainiert, eine **Aktionswertfunktion**, die im internen Speicher durch eine *Q-Tabelle* **kodiert wird, die alle Werte der Zustands-Aktionspaare enthält**.\n",
    "\n",
    "- Wenn ein Zustand und eine Aktion gegeben sind, sucht unsere Q-Funktion **in der Q-Tabelle nach dem entsprechenden Wert**.\n",
    "    \n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-function-2.jpg\" alt=\"Q-Funktion\" width=\"100%\"/>\n",
    "\n",
    "- Wenn das Training abgeschlossen ist, **haben wir eine optimale Q-Funktion, also eine optimale Q-Tabelle.**\n",
    "    \n",
    "- Und wenn wir **eine optimale Q-Funktion** haben, haben wir\n",
    "haben wir eine optimale Politik, da wir **für jeden Zustand die beste Aktion kennen, die zu ergreifen ist.**\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/link-value-policy.jpg\" alt=\"Link value policy\" width=\"100%\"/>\n",
    "\n",
    "\n",
    "Aber am Anfang ist unsere **Q-Tabelle nutzlos, da sie für jedes Zustands-Aktionspaar einen beliebigen Wert angibt (meistens initialisieren wir die Q-Tabelle mit 0 Werten)**. Aber wenn wir die Umgebung erkunden und unsere Q-Tabelle aktualisieren, wird sie uns immer bessere Annäherungen liefern\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/q-learning.jpeg\" alt=\"q-learning.jpeg\" width=\"100%\"/>\n",
    "\n",
    "Dies ist der Pseudocode für das Q-Learning:\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg\" alt=\"Q-Learning\" width=\"100%\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HEtx8Y8MqKfH"
   },
   "source": [
    "# Programmieren wir unseren ersten Reinforcement Learning Algorithmus 🚀."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kdxb1IhzTn0v"
   },
   "source": [
    "Um dieses Hands-On für den [Zertifizierungsprozess] (https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process) zu validieren, müssen Sie Ihr trainiertes Taximodell an den Hub senden und **ein Ergebnis von >= 4,5** erhalten.\n",
    "\n",
    "Um Ihr Ergebnis zu finden, gehen Sie zur [Bestenliste] (https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) und suchen Sie Ihr Modell, **das Ergebnis = mean_reward - std of reward**\n",
    "\n",
    "Weitere Informationen über den Zertifizierungsprozess finden Sie in diesem Abschnitt 👉 https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4gpxC1_kqUYe"
   },
   "source": [
    "## Abhängigkeiten installieren und einen virtuellen Bildschirm erstellen 🔽.\n",
    "\n",
    "Im Notebook müssen wir ein Wiederholungsvideo erstellen. Dazu benötigen wir mit Colab **einen virtuellen Bildschirm, um die Umgebung zu rendern** (und somit die Bilder aufzunehmen).\n",
    "\n",
    "Daher wird die folgende Zelle die Bibliotheken installieren und einen virtuellen Bildschirm erstellen und starten 🖥\n",
    "\n",
    "Wir werden mehrere Bibliotheken installieren:\n",
    "\n",
    "- `gymnasium`: Enthält die Umgebungen FrozenLake-v1 ⛄ und Taxi-v3 🚕.\n",
    "- pygame\": Wird für die FrozenLake-v1- und Taxi-v3-Benutzeroberfläche verwendet.\n",
    "- `numpy`: Wird für die Handhabung unserer Q-Tabelle verwendet.\n",
    "\n",
    "Der Hugging Face Hub 🤗 dient als zentraler Ort, an dem jeder Modelle und Datensätze teilen und erforschen kann. Er bietet Versionierung, Metriken, Visualisierungen und andere Funktionen, die eine einfache Zusammenarbeit mit anderen ermöglichen.\n",
    "\n",
    "Sie können alle verfügbaren Deep-RL-Modelle (sofern sie Q Learning verwenden) hier sehen 👉 https://huggingface.co/models?other=q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9XaULfDZDvrC"
   },
   "outputs": [],
   "source": [
    "!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n71uTX7qqzz2"
   },
   "outputs": [],
   "source": [
    "!sudo apt-get update\n",
    "!sudo apt-get install -y python3-opengl\n",
    "!apt install ffmpeg xvfb\n",
    "!pip3 install pyvirtualdisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K6XC13pTfFiD"
   },
   "source": [
    "Um sicherzustellen, dass die neu installierten Bibliotheken verwendet werden, **ist es manchmal erforderlich, die Laufzeit des Notebooks neu zu starten**. Die nächste Zelle wird die **Laufzeitumgebung zum Absturz bringen, so dass Sie eine neue Verbindung herstellen und den Code von hier aus ausführen müssen**. Dank dieses Tricks **können wir unseren virtuellen Bildschirm ausführen**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3kuZbWAkfHdg"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DaY1N4dBrabi"
   },
   "outputs": [],
   "source": [
    "# Virtual display\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "virtual_display = Display(visible=0, size=(1400, 900))\n",
    "virtual_display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W-7f-Swax_9x"
   },
   "source": [
    "## Importieren Sie die Pakete 📦\n",
    "\n",
    "Zusätzlich zu den installierten Bibliotheken verwenden wir auch:\n",
    "\n",
    "- `random`: Um Zufallszahlen zu erzeugen (die für die Epsilon-Greedy-Politik nützlich sind).\n",
    "- `imageio`: Zur Erzeugung eines Wiederholungsvideos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VcNvOAQlysBJ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import random\n",
    "import imageio\n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "import pickle5 as pickle\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xp4-bXKIy1mQ"
   },
   "source": [
    "Wir sind jetzt bereit, unseren Q-Learning-Algorithmus zu programmieren 🔥."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xya49aNJWVvv"
   },
   "source": [
    "# Teil 1: Gefrorener See ⛄ (nicht rutschige Version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NAvihuHdy9tw"
   },
   "source": [
    "## Erstellen und verstehen [FrozenLake-Umgebung ⛄]((https://gymnasium.farama.org/environments/toy_text/frozen_lake/)\n",
    "---\n",
    "\n",
    "💡 Eine gute Angewohnheit, wenn Sie anfangen, eine Umgebung zu benutzen, ist es, ihre Dokumentation zu überprüfen\n",
    "\n",
    "👉 https://gymnasium.farama.org/environments/toy_text/frozen_lake/\n",
    "\n",
    "---\n",
    "\n",
    "Wir werden unseren Q-Learning-Agenten darauf trainieren, **vom Startzustand (S) zum Zielzustand (G) zu navigieren, indem wir nur auf gefrorenen Kacheln (F) laufen und Löcher (H)** vermeiden.\n",
    "\n",
    "Wir können zwei Größen von Umgebungen haben:\n",
    "\n",
    "- `map_name=\"4x4\"`: eine 4x4-Gitterversion\n",
    "- `map_name=\"8x8\"`: eine 8x8-Gitterversion\n",
    "\n",
    "\n",
    "Die Umgebung hat zwei Modi:\n",
    "\n",
    "- `is_slippery=False`: Der Agent bewegt sich immer **in die beabsichtigte Richtung**, da der gefrorene See nicht rutschig ist (deterministisch).\n",
    "- is_slippery=True`: Der Agent **bewegt sich aufgrund der glatten Beschaffenheit des gefrorenen Sees nicht immer in die beabsichtigte Richtung** (stochastisch)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UaW_LHfS0PY2"
   },
   "source": [
    "Für den Moment halten wir es einfach mit der 4x4 Karte und nicht rutschig.\n",
    "Wir fügen einen Parameter namens `render_mode` hinzu, der angibt, wie die Umgebung visualisiert werden soll. In unserem Fall, weil wir **am Ende ein Video der Umgebung aufnehmen wollen, müssen wir render_mode auf rgb_array** setzen.\n",
    "\n",
    "Wie [in der Dokumentation erklärt](https://gymnasium.farama.org/api/env/#gymnasium.Env.render) \"rgb_array\": Gibt ein einzelnes Bild zurück, das den aktuellen Zustand der Umgebung darstellt. Ein Frame ist ein np.ndarray mit der Form (x, y, 3), das RGB-Werte für ein x-mal-y-Pixelbild darstellt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IzJnb8O3y8up"
   },
   "outputs": [],
   "source": [
    "# Create the FrozenLake-v1 environment using 4x4 map and non-slippery version and render_mode=\"rgb_array\"\n",
    "env = gym.make() # TODO use the correct parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ji_UrI5l2zzn"
   },
   "source": [
    "### Lösung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jNxUbPMP0akP"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False, render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KASNViqL4tZn"
   },
   "source": [
    "Sie können Ihr eigenes benutzerdefiniertes Raster wie dieses erstellen:\n",
    "\n",
    "```python\n",
    "desc=[\"SFFF\", \"FHFH\", \"FFFH\", \"HFFG\"]\n",
    "gym.make('FrozenLake-v1', desc=desc, is_slippery=True)\n",
    "```\n",
    "\n",
    "aber wir werden vorerst die Standardumgebung verwenden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXbTfdeJ1Xi9"
   },
   "source": [
    "### Mal sehen, wie die Umwelt aussieht:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZNPG0g_UGCfh"
   },
   "outputs": [],
   "source": [
    "# We create our environment with gym.make(\"<name_of_the_environment>\")- `is_slippery=False`: The agent always moves in the intended direction due to the non-slippery nature of the frozen lake (deterministic).\n",
    "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
    "print(\"Observation Space\", env.observation_space)\n",
    "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2MXc15qFE0M9"
   },
   "source": [
    "Mit \"Observation Space Shape Discrete(16)\" sehen wir, dass die Beobachtung eine ganze Zahl ist, die die **aktuelle Position des Agenten als current_row * ncols + current_col darstellt (wobei sowohl row als auch col bei 0 beginnen)**.\n",
    "\n",
    "Zum Beispiel kann die Zielposition in der 4x4-Karte wie folgt berechnet werden: 3 * 4 + 3 = 15. Die Anzahl der möglichen Beobachtungen ist abhängig von der Größe der Karte. **Die 4x4-Karte hat zum Beispiel 16 mögliche Beobachtungen**.\n",
    "\n",
    "\n",
    "So sieht zum Beispiel der Zustand = 0 aus:\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/frozenlake.png\" alt=\"FrozenLake\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "We5WqOBGLoSm"
   },
   "outputs": [],
   "source": [
    "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
    "print(\"Action Space Shape\", env.action_space.n)\n",
    "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MyxXwkI2Magx"
   },
   "source": [
    "Der Aktionsraum (die Menge der möglichen Aktionen, die der Agent ausführen kann) ist diskret mit 4 verfügbaren Aktionen 🎮:\n",
    "- 0: LINKS GEHEN\n",
    "- 1: ABWÄRTS GEHEN\n",
    "- 2: RECHTS GEHEN\n",
    "- 3: NACH OBEN GEHEN\n",
    "\n",
    "Belohnungsfunktion 💰:\n",
    "- Erreichen des Ziels: +1\n",
    "- Loch erreichen: 0\n",
    "- Gefrorenes Ziel erreichen: 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1pFhWblk3Awr"
   },
   "source": [
    "## Erstellen und Initialisieren der Q-Tabelle 🗄️\n",
    "\n",
    "(👀 Schritt 1 des Pseudocodes)\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg\" alt=\"Q-Learning\" width=\"100%\"/>\n",
    "\n",
    "\n",
    "Es ist an der Zeit, unsere Q-Tabelle zu initialisieren! Um zu wissen, wie viele Zeilen (Zustände) und Spalten (Aktionen) wir verwenden sollen, müssen wir den Aktions- und Beobachtungsraum kennen. Wir kennen ihre Werte bereits von früher, aber wir wollen sie programmatisch erhalten, damit unser Algorithmus für verschiedene Umgebungen verallgemeinert werden kann. Gym bietet uns eine Möglichkeit, dies zu tun: `env.action_space.n` und `env.observation_space.n`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y3ZCdluj3k0l"
   },
   "outputs": [],
   "source": [
    "state_space =\n",
    "print(\"There are \", state_space, \" possible states\")\n",
    "\n",
    "action_space =\n",
    "print(\"There are \", action_space, \" possible actions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rCddoOXM3UQH"
   },
   "outputs": [],
   "source": [
    "# Let's create our Qtable of size (state_space, action_space) and initialized each values at 0 using np.zeros. np.zeros needs a tuple (a,b)\n",
    "def initialize_q_table(state_space, action_space):\n",
    "  Qtable =\n",
    "  return Qtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9YfvrqRt3jdR"
   },
   "outputs": [],
   "source": [
    "Qtable_frozenlake = initialize_q_table(state_space, action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67OdoKL63eDD"
   },
   "source": [
    "### Lösung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HuTKv3th3ohG"
   },
   "outputs": [],
   "source": [
    "state_space = env.observation_space.n\n",
    "print(\"There are \", state_space, \" possible states\")\n",
    "\n",
    "action_space = env.action_space.n\n",
    "print(\"There are \", action_space, \" possible actions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lnrb_nX33fJo"
   },
   "outputs": [],
   "source": [
    "# Let's create our Qtable of size (state_space, action_space) and initialized each values at 0 using np.zeros\n",
    "def initialize_q_table(state_space, action_space):\n",
    "  Qtable = np.zeros((state_space, action_space))\n",
    "  return Qtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y0WlgkVO3Jf9"
   },
   "outputs": [],
   "source": [
    "Qtable_frozenlake = initialize_q_table(state_space, action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Atll4Z774gri"
   },
   "source": [
    "## Definieren Sie die gierige Politik 🤖.\n",
    "\n",
    "Denken Sie daran, dass wir zwei Strategien haben, da Q-Learning ein **off-policy** Algorithmus ist. Das bedeutet, dass wir eine **unterschiedliche Strategie für das Handeln und die Aktualisierung der Wertfunktion** verwenden.\n",
    "\n",
    "- Epsilon-Greedy-Strategie (handelnde Strategie)\n",
    "- Greedy-Politik (Aktualisierungspolitik)\n",
    "\n",
    "Die Greedy-Policy wird auch die endgültige Policy sein, die wir haben, wenn der Q-Learning-Agent das Training abgeschlossen hat. Die Greedy-Politik wird verwendet, um eine Aktion anhand der Q-Tabelle auszuwählen.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/off-on-4.jpg\" alt=\"Q-Learning\" width=\"100%\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E3SCLmLX5bWG"
   },
   "outputs": [],
   "source": [
    "def greedy_policy(Qtable, state):\n",
    "  # Exploitation: take the action with the highest state, action value\n",
    "  action =\n",
    "\n",
    "  return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B2_-8b8z5k54"
   },
   "source": [
    "#### Lösung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "se2OzWGW5kYJ"
   },
   "outputs": [],
   "source": [
    "def greedy_policy(Qtable, state):\n",
    "  # Exploitation: take the action with the highest state, action value\n",
    "  action = np.argmax(Qtable[state][:])\n",
    "\n",
    "  return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "flILKhBU3yZ7"
   },
   "source": [
    "##Definieren Sie die Epsilon-Greedy-Politik 🤖.\n",
    "\n",
    "Epsilon-Greedy ist die Trainingsstrategie, die den Kompromiss zwischen Erkundung und Ausbeutung behandelt.\n",
    "\n",
    "Die Idee mit Epsilon-Greedy:\n",
    "\n",
    "- Mit *Wahrscheinlichkeit 1 - ɛ* : **wir machen Exploitation** (d.h. unser Agent wählt die Aktion mit dem höchsten Wert des Zustands-Aktionspaares).\n",
    "\n",
    "- Mit *Wahrscheinlichkeit ɛ*: **Exploration** (wir versuchen eine zufällige Aktion).\n",
    "\n",
    "Mit fortschreitendem Training wird der Epsilon-Wert schrittweise **verringert, da wir immer weniger Exploration und mehr Exploitation benötigen**.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-4.jpg\" alt=\"Q-Learning\" width=\"100%\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Bj7x3in3_Pq"
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(Qtable, state, epsilon):\n",
    "  # Randomly generate a number between 0 and 1\n",
    "  random_num =\n",
    "  # if random_num > greater than epsilon --> exploitation\n",
    "  if random_num > epsilon:\n",
    "    # Take the action with the highest value given a state\n",
    "    # np.argmax can be useful here\n",
    "    action =\n",
    "  # else --> exploration\n",
    "  else:\n",
    "    action = # Take a random action\n",
    "\n",
    "  return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8R5ej1fS4P2V"
   },
   "source": [
    "#### Lösung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cYxHuckr4LiG"
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(Qtable, state, epsilon):\n",
    "  # Randomly generate a number between 0 and 1\n",
    "  random_num = random.uniform(0,1)\n",
    "  # if random_num > greater than epsilon --> exploitation\n",
    "  if random_num > epsilon:\n",
    "    # Take the action with the highest value given a state\n",
    "    # np.argmax can be useful here\n",
    "    action = greedy_policy(Qtable, state)\n",
    "  # else --> exploration\n",
    "  else:\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "  return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hW80DealcRtu"
   },
   "source": [
    "## Definieren Sie die Hyperparameter ⚙️\n",
    "\n",
    "Die Hyperparameter, die sich auf die Exploration beziehen, gehören zu den wichtigsten Parametern.\n",
    "\n",
    "- Wir müssen sicherstellen, dass unser Agent **genug vom Zustandsraum erforscht**, um eine gute Wertannäherung zu lernen. Um dies zu erreichen, müssen wir einen progressiven Verfall von epsilon haben.\n",
    "- Wenn man epsilon zu schnell verringert (zu hohe decay_rate), **geht man das Risiko ein, dass der Agent feststeckt**, da er den Zustandsraum nicht ausreichend erforscht hat und daher das Problem nicht lösen kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y1tWn0tycWZ1"
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "n_training_episodes = 10000  # Total training episodes\n",
    "learning_rate = 0.7          # Learning rate\n",
    "\n",
    "# Evaluation parameters\n",
    "n_eval_episodes = 100        # Total number of test episodes\n",
    "\n",
    "# Environment parameters\n",
    "env_id = \"FrozenLake-v1\"     # Name of the environment\n",
    "max_steps = 99               # Max steps per episode\n",
    "gamma = 0.95                 # Discounting rate\n",
    "eval_seed = []               # The evaluation seed of the environment\n",
    "\n",
    "# Exploration parameters\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.05            # Minimum exploration probability\n",
    "decay_rate = 0.0005            # Exponential decay rate for exploration prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cDb7Tdx8atfL"
   },
   "source": [
    "## Erstellen Sie die Trainingsschleifenmethode\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg\" alt=\"Q-Learning\" width=\"100%\"/>\n",
    "\n",
    "Die Trainingsschleife sieht folgendermaßen aus:\n",
    "\n",
    "```\n",
    "Für eine Episode in der Gesamtheit der Trainingsepisoden:\n",
    "\n",
    "Epsilon reduzieren (da wir immer weniger Erkundung brauchen)\n",
    "Die Umgebung zurücksetzen\n",
    "\n",
    "  Für Schritt in maximalen Zeitschritten:    \n",
    "    Wählen Sie die Aktion At mit epsilon gierig Politik\n",
    "    Ausführen der Aktion (a) und Beobachten des Ergebniszustands (s') und der Belohnung (r)\n",
    "    Aktualisierung des Q-Wertes Q(s,a) unter Verwendung der Bellman-Gleichung Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "    Wenn fertig, beende die Episode\n",
    "    Unser nächster Zustand ist der neue Zustand\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "paOynXy3aoJW"
   },
   "outputs": [],
   "source": [
    "def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n",
    "  for episode in tqdm(range(n_training_episodes)):\n",
    "    # Reduce epsilon (because we need less and less exploration)\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
    "    # Reset the environment\n",
    "    state, info = env.reset()\n",
    "    step = 0\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "\n",
    "    # repeat\n",
    "    for step in range(max_steps):\n",
    "      # Choose the action At using epsilon greedy policy\n",
    "      action =\n",
    "\n",
    "      # Take action At and observe Rt+1 and St+1\n",
    "      # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "      new_state, reward, terminated, truncated, info =\n",
    "\n",
    "      # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "      Qtable[state][action] =\n",
    "\n",
    "      # If terminated or truncated finish the episode\n",
    "      if terminated or truncated:\n",
    "        break\n",
    "\n",
    "      # Our next state is the new state\n",
    "      state = new_state\n",
    "  return Qtable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pnpk2ePoem3r"
   },
   "source": [
    "#### Lösung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IyZaYbUAeolw"
   },
   "outputs": [],
   "source": [
    "def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n",
    "  for episode in tqdm(range(n_training_episodes)):\n",
    "    # Reduce epsilon (because we need less and less exploration)\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
    "    # Reset the environment\n",
    "    state, info = env.reset()\n",
    "    step = 0\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "\n",
    "    # repeat\n",
    "    for step in range(max_steps):\n",
    "      # Choose the action At using epsilon greedy policy\n",
    "      action = epsilon_greedy_policy(Qtable, state, epsilon)\n",
    "\n",
    "      # Take action At and observe Rt+1 and St+1\n",
    "      # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "      new_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "      # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "      Qtable[state][action] = Qtable[state][action] + learning_rate * (reward + gamma * np.max(Qtable[new_state]) - Qtable[state][action])\n",
    "\n",
    "      # If terminated or truncated finish the episode\n",
    "      if terminated or truncated:\n",
    "        break\n",
    "\n",
    "      # Our next state is the new state\n",
    "      state = new_state\n",
    "  return Qtable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WLwKQ4tUdhGI"
   },
   "source": [
    "## Trainieren Sie den Q-Learning-Agenten 🏃."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DPBxfjJdTCOH"
   },
   "outputs": [],
   "source": [
    "Qtable_frozenlake = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_frozenlake)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yVeEhUCrc30L"
   },
   "source": [
    "## Mal sehen, wie unsere Q-Learning-Tabelle jetzt aussieht 👀."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nmfchsTITw4q"
   },
   "outputs": [],
   "source": [
    "Qtable_frozenlake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pUrWkxsHccXD"
   },
   "source": [
    "## Die Bewertungsmethode 📝\n",
    "\n",
    "- Wir haben die Auswertungsmethode definiert, mit der wir unseren Q-Learning-Agenten testen werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jNl0_JO2cbkm"
   },
   "outputs": [],
   "source": [
    "def evaluate_agent(env, max_steps, n_eval_episodes, Q, seed):\n",
    "  \"\"\"\n",
    "  Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n",
    "  :param env: The evaluation environment\n",
    "  :param max_steps: Maximum number of steps per episode\n",
    "  :param n_eval_episodes: Number of episode to evaluate the agent\n",
    "  :param Q: The Q-table\n",
    "  :param seed: The evaluation seed array (for taxi-v3)\n",
    "  \"\"\"\n",
    "  episode_rewards = []\n",
    "  for episode in tqdm(range(n_eval_episodes)):\n",
    "    if seed:\n",
    "      state, info = env.reset(seed=seed[episode])\n",
    "    else:\n",
    "      state, info = env.reset()\n",
    "    step = 0\n",
    "    truncated = False\n",
    "    terminated = False\n",
    "    total_rewards_ep = 0\n",
    "\n",
    "    for step in range(max_steps):\n",
    "      # Take the action (index) that have the maximum expected future reward given that state\n",
    "      action = greedy_policy(Q, state)\n",
    "      new_state, reward, terminated, truncated, info = env.step(action)\n",
    "      total_rewards_ep += reward\n",
    "\n",
    "      if terminated or truncated:\n",
    "        break\n",
    "      state = new_state\n",
    "    episode_rewards.append(total_rewards_ep)\n",
    "  mean_reward = np.mean(episode_rewards)\n",
    "  std_reward = np.std(episode_rewards)\n",
    "\n",
    "  return mean_reward, std_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0jJqjaoAnxUo"
   },
   "source": [
    "## Bewerten Sie unseren Q-Learning-Agenten 📈\n",
    "\n",
    "- Normalerweise sollte man eine mittlere Belohnung von 1,0 haben.\n",
    "- Die **Umgebung ist relativ einfach**, da der Zustandsraum sehr klein ist (16). Man kann versuchen, sie durch die schlüpfrige Version zu ersetzen (https://gymnasium.farama.org/environments/toy_text/frozen_lake/), was Stochastizität einführt und die Umgebung komplexer macht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fAgB7s0HEFMm"
   },
   "outputs": [],
   "source": [
    "# Evaluate our Agent\n",
    "mean_reward, std_reward = evaluate_agent(env, max_steps, n_eval_episodes, Qtable_frozenlake, eval_seed)\n",
    "print(f\"Mean_reward={mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yxaP3bPdg1DV"
   },
   "source": [
    "## Veröffentliche unser trainiertes Modell auf dem Hub 🔥\n",
    "\n",
    "Nun, da wir nach dem Training gute Ergebnisse gesehen haben, **können wir unser trainiertes Modell mit einer Zeile Code auf dem Hub 🤗 veröffentlichen**.\n",
    "\n",
    "Hier ist ein Beispiel für eine Model Card:\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/modelcard.png\" alt=\"Modellkarte\" width=\"100%\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kv0k1JQjpMq3"
   },
   "source": [
    "Unter der Haube verwendet der Hub Git-basierte Repositories (keine Sorge, wenn Sie nicht wissen, was Git ist), was bedeutet, dass Sie das Modell mit neuen Versionen aktualisieren können, wenn Sie experimentieren und Ihren Agenten verbessern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QZ5LrR-joIHD"
   },
   "source": [
    "#### Dieser Code darf nicht verändert werden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jex3i9lZ8ksX"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, snapshot_download\n",
    "from huggingface_hub.repocard import metadata_eval_result, metadata_save\n",
    "\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qo57HBn3W74O"
   },
   "outputs": [],
   "source": [
    "def record_video(env, Qtable, out_directory, fps=1):\n",
    "  \"\"\"\n",
    "  Generate a replay video of the agent\n",
    "  :param env\n",
    "  :param Qtable: Qtable of our agent\n",
    "  :param out_directory\n",
    "  :param fps: how many frame per seconds (with taxi-v3 and frozenlake-v1 we use 1)\n",
    "  \"\"\"\n",
    "  images = []\n",
    "  terminated = False\n",
    "  truncated = False\n",
    "  state, info = env.reset(seed=random.randint(0,500))\n",
    "  img = env.render()\n",
    "  images.append(img)\n",
    "  while not terminated or truncated:\n",
    "    # Take the action (index) that have the maximum expected future reward given that state\n",
    "    action = np.argmax(Qtable[state][:])\n",
    "    state, reward, terminated, truncated, info = env.step(action) # We directly put next_state = state for recording logic\n",
    "    img = env.render()\n",
    "    images.append(img)\n",
    "  imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4mdUTKkGnUd"
   },
   "outputs": [],
   "source": [
    "def push_to_hub(\n",
    "    repo_id, model, env, video_fps=1, local_repo_path=\"hub\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate, Generate a video and Upload a model to Hugging Face Hub.\n",
    "    This method does the complete pipeline:\n",
    "    - It evaluates the model\n",
    "    - It generates the model card\n",
    "    - It generates a replay video of the agent\n",
    "    - It pushes everything to the Hub\n",
    "\n",
    "    :param repo_id: repo_id: id of the model repository from the Hugging Face Hub\n",
    "    :param env\n",
    "    :param video_fps: how many frame per seconds to record our video replay\n",
    "    (with taxi-v3 and frozenlake-v1 we use 1)\n",
    "    :param local_repo_path: where the local repository is\n",
    "    \"\"\"\n",
    "    _, repo_name = repo_id.split(\"/\")\n",
    "\n",
    "    eval_env = env\n",
    "    api = HfApi()\n",
    "\n",
    "    # Step 1: Create the repo\n",
    "    repo_url = api.create_repo(\n",
    "        repo_id=repo_id,\n",
    "        exist_ok=True,\n",
    "    )\n",
    "\n",
    "    # Step 2: Download files\n",
    "    repo_local_path = Path(snapshot_download(repo_id=repo_id))\n",
    "\n",
    "    # Step 3: Save the model\n",
    "    if env.spec.kwargs.get(\"map_name\"):\n",
    "        model[\"map_name\"] = env.spec.kwargs.get(\"map_name\")\n",
    "        if env.spec.kwargs.get(\"is_slippery\", \"\") == False:\n",
    "            model[\"slippery\"] = False\n",
    "\n",
    "    # Pickle the model\n",
    "    with open((repo_local_path) / \"q-learning.pkl\", \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "    # Step 4: Evaluate the model and build JSON with evaluation metrics\n",
    "    mean_reward, std_reward = evaluate_agent(\n",
    "        eval_env, model[\"max_steps\"], model[\"n_eval_episodes\"], model[\"qtable\"], model[\"eval_seed\"]\n",
    "    )\n",
    "\n",
    "    evaluate_data = {\n",
    "        \"env_id\": model[\"env_id\"],\n",
    "        \"mean_reward\": mean_reward,\n",
    "        \"n_eval_episodes\": model[\"n_eval_episodes\"],\n",
    "        \"eval_datetime\": datetime.datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "    # Write a JSON file called \"results.json\" that will contain the\n",
    "    # evaluation results\n",
    "    with open(repo_local_path / \"results.json\", \"w\") as outfile:\n",
    "        json.dump(evaluate_data, outfile)\n",
    "\n",
    "    # Step 5: Create the model card\n",
    "    env_name = model[\"env_id\"]\n",
    "    if env.spec.kwargs.get(\"map_name\"):\n",
    "        env_name += \"-\" + env.spec.kwargs.get(\"map_name\")\n",
    "\n",
    "    if env.spec.kwargs.get(\"is_slippery\", \"\") == False:\n",
    "        env_name += \"-\" + \"no_slippery\"\n",
    "\n",
    "    metadata = {}\n",
    "    metadata[\"tags\"] = [env_name, \"q-learning\", \"reinforcement-learning\", \"custom-implementation\"]\n",
    "\n",
    "    # Add metrics\n",
    "    eval = metadata_eval_result(\n",
    "        model_pretty_name=repo_name,\n",
    "        task_pretty_name=\"reinforcement-learning\",\n",
    "        task_id=\"reinforcement-learning\",\n",
    "        metrics_pretty_name=\"mean_reward\",\n",
    "        metrics_id=\"mean_reward\",\n",
    "        metrics_value=f\"{mean_reward:.2f} +/- {std_reward:.2f}\",\n",
    "        dataset_pretty_name=env_name,\n",
    "        dataset_id=env_name,\n",
    "    )\n",
    "\n",
    "    # Merges both dictionaries\n",
    "    metadata = {**metadata, **eval}\n",
    "\n",
    "    model_card = f\"\"\"\n",
    "  # **Q-Learning** Agent playing1 **{env_id}**\n",
    "  This is a trained model of a **Q-Learning** agent playing **{env_id}** .\n",
    "\n",
    "  ## Usage\n",
    "\n",
    "  ```python\n",
    "\n",
    "  model = load_from_hub(repo_id=\"{repo_id}\", filename=\"q-learning.pkl\")\n",
    "\n",
    "  # Don't forget to check if you need to add additional attributes (is_slippery=False etc)\n",
    "  env = gym.make(model[\"env_id\"])\n",
    "  ```\n",
    "  \"\"\"\n",
    "\n",
    "    evaluate_agent(env, model[\"max_steps\"], model[\"n_eval_episodes\"], model[\"qtable\"], model[\"eval_seed\"])\n",
    "\n",
    "    readme_path = repo_local_path / \"README.md\"\n",
    "    readme = \"\"\n",
    "    print(readme_path.exists())\n",
    "    if readme_path.exists():\n",
    "        with readme_path.open(\"r\", encoding=\"utf8\") as f:\n",
    "            readme = f.read()\n",
    "    else:\n",
    "        readme = model_card\n",
    "\n",
    "    with readme_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(readme)\n",
    "\n",
    "    # Save our metrics to Readme metadata\n",
    "    metadata_save(readme_path, metadata)\n",
    "\n",
    "    # Step 6: Record a video\n",
    "    video_path = repo_local_path / \"replay.mp4\"\n",
    "    record_video(env, model[\"qtable\"], video_path, video_fps)\n",
    "\n",
    "    # Step 7. Push everything to the Hub\n",
    "    api.upload_folder(\n",
    "        repo_id=repo_id,\n",
    "        folder_path=repo_local_path,\n",
    "        path_in_repo=\".\",\n",
    "    )\n",
    "\n",
    "    print(\"Your model is pushed to the Hub. You can view your model here: \", repo_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81J6cet_ogSS"
   },
   "source": [
    "### .\n",
    "\n",
    "Mit \"push_to_hub\" **werten Sie aus, zeichnen ein Replay auf, generieren eine Modellkarte Ihres Agenten und schieben sie an den Hub**.\n",
    "\n",
    "This way:\n",
    "- Sie können **unsere Arbeit vorführen** 🔥.\n",
    "- Sie können **Ihren Agenten beim Spielen visualisieren** 👀\n",
    "- Du kannst **einen Agenten mit der Community teilen, den andere nutzen können** 💾\n",
    "- Sie können **auf eine Bestenliste zugreifen 🏆 um zu sehen, wie gut Ihr Agent im Vergleich zu Ihren Klassenkameraden abschneidet** 👉 https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cWnFC0iZooTw"
   },
   "source": [
    "Um Ihr Modell mit der Gemeinschaft teilen zu können, sind drei weitere Schritte erforderlich:\n",
    "\n",
    "1️⃣ (Falls noch nicht geschehen) Erstellen Sie ein Konto für HF ➡ https://huggingface.co/join\n",
    "\n",
    "2️⃣ Melde dich an und speichere dann dein Authentifizierungs-Token von der Hugging Face Website.\n",
    "- Erstellen Sie ein neues Token (https://huggingface.co/settings/tokens) **mit Schreibrolle**\n",
    "\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/create-token.jpg\" alt=\"HF-Token erstellen\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QB5nIcxR8paT"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GyWc1x3-o3xG"
   },
   "source": [
    "Wenn Sie kein Google Colab oder ein Jupyter Notebook verwenden möchten, müssen Sie stattdessen diesen Befehl verwenden: `huggingface-cli login` (oder `login`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gc5AfUeFo3xH"
   },
   "source": [
    "3️⃣ Wir sind jetzt bereit, unseren trainierten Agenten mit der Funktion \"push_to_hub()\" an den 🤗 Hub 🔥 zu senden.\n",
    "\n",
    "- Erstellen wir **das Modellwörterbuch, das die Hyperparameter und die Q_table** enthält."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FiMqxqVHg0I4"
   },
   "outputs": [],
   "source": [
    "model = {\n",
    "    \"env_id\": env_id,\n",
    "    \"max_steps\": max_steps,\n",
    "    \"n_training_episodes\": n_training_episodes,\n",
    "    \"n_eval_episodes\": n_eval_episodes,\n",
    "    \"eval_seed\": eval_seed,\n",
    "\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"gamma\": gamma,\n",
    "\n",
    "    \"max_epsilon\": max_epsilon,\n",
    "    \"min_epsilon\": min_epsilon,\n",
    "    \"decay_rate\": decay_rate,\n",
    "\n",
    "    \"qtable\": Qtable_frozenlake\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9kld-AEso3xH"
   },
   "source": [
    "Füllen wir die Funktion \"push_to_hub\":\n",
    "\n",
    "- `repo_id`: der Name des Hugging Face Hub Repository, das erstellt/aktualisiert werden soll `\n",
    "(repo_id = {Benutzername}/{repo_name})`\n",
    "💡 Eine gute `repo_id` ist `{username}/q-{env_id}`\n",
    "- model\": unser Modellwörterbuch mit den Hyperparametern und der Q-Tabelle.\n",
    "- `env`: die Umgebung.\n",
    "- `commit_message`: die Nachricht der Übergabe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5sBo2umnXpPd"
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RpOTtSt83kPZ"
   },
   "outputs": [],
   "source": [
    "username = \"\" # FILL THIS\n",
    "repo_name = \"q-FrozenLake-v1-4x4-noSlippery\"\n",
    "push_to_hub(\n",
    "    repo_id=f\"{username}/{repo_name}\",\n",
    "    model=model,\n",
    "    env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E2875IGsprzq"
   },
   "source": [
    "Herzlichen Glückwunsch 🥳 Sie haben soeben Ihren ersten Reinforcement Learning Agent von Grund auf implementiert, trainiert und hochgeladen.\n",
    "FrozenLake-v1 no_slippery ist eine sehr einfache Umgebung, versuchen wir eine schwierigere 🔥."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18lN8Bz7yvLt"
   },
   "source": [
    "# Teil 2: Taxi-v3 🚖\n",
    "\n",
    "## Erstellen und verstehen [Taxi-v3 🚕](https://gymnasium.farama.org/environments/toy_text/taxi/)\n",
    "---\n",
    "\n",
    "💡 Eine gute Angewohnheit, wenn du anfängst, eine Umgebung zu benutzen, ist es, ihre Dokumentation zu überprüfen.\n",
    "\n",
    "👉 https://gymnasium.farama.org/environments/toy_text/taxi/\n",
    "\n",
    "---\n",
    "\n",
    "In `Taxi-v3` 🚕 gibt es vier bestimmte Orte in der Gitterwelt, die mit R(ed), G(reen), Y(ellow) und B(lue) bezeichnet werden.\n",
    "\n",
    "Zu Beginn der Episode **startet das Taxi an einem zufälligen Platz** und der Fahrgast befindet sich an einem zufälligen Ort. Das Taxi fährt zum Standort des Fahrgastes, **nimmt den Fahrgast auf**, fährt zum Zielort des Fahrgastes (einem anderen der vier angegebenen Orte) und setzt den Fahrgast dann **ab**. Sobald der Fahrgast abgesetzt ist, endet die Episode.\n",
    "\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/taxi.png\" alt=\"Taxi\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gL0wpeO8gpej"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\", render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gBOaXgtsrmtT"
   },
   "source": [
    "Es gibt **500 diskrete Zustände, da es 25 Taxipositionen, 5 mögliche Standorte des Fahrgastes** (einschließlich des Falles, in dem sich der Fahrgast im Taxi befindet) und **4 Zielstandorte** gibt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_TPNaGSZrgqA"
   },
   "outputs": [],
   "source": [
    "state_space = env.observation_space.n\n",
    "print(\"There are \", state_space, \" possible states\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CdeeZuokrhit"
   },
   "outputs": [],
   "source": [
    "action_space = env.action_space.n\n",
    "print(\"There are \", action_space, \" possible actions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R1r50Advrh5Q"
   },
   "source": [
    "Der Aktionsraum (die Menge der möglichen Aktionen, die der Agent ausführen kann) ist diskret mit **6 verfügbaren Aktionen 🎮**:\n",
    "\n",
    "- 0: nach Süden gehen\n",
    "- 1: nach Norden ziehen\n",
    "- 2: nach Osten ziehen\n",
    "- 3: nach Westen fahren\n",
    "- 4: Fahrgast aufnehmen\n",
    "- 5: Fahrgast absetzen\n",
    "\n",
    "Belohnungsfunktion 💰:\n",
    "\n",
    "-1 pro Schritt, wenn keine andere Belohnung ausgelöst wird.\n",
    "- +20 Fahrgast abliefern.\n",
    "-10 Unerlaubtes Ausführen der Aktionen \"Abholen\" und \"Absetzen\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "US3yDXnEtY9I"
   },
   "outputs": [],
   "source": [
    "# Create our Q table with state_size rows and action_size columns (500x6)\n",
    "Qtable_taxi = initialize_q_table(state_space, action_space)\n",
    "print(Qtable_taxi)\n",
    "print(\"Q-table shape: \", Qtable_taxi .shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gUMKPH0_LJyH"
   },
   "source": [
    "## Definieren Sie die Hyperparameter ⚙️\n",
    "\n",
    "⚠ VERÄNDERN SIE NICHT EVAL_SEED: Das eval_seed-Array **ermöglicht es uns, Ihren Agenten mit denselben Taxistartpositionen für jeden Klassenkameraden zu bewerten**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AB6n__hhg7YS"
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "n_training_episodes = 25000   # Total training episodes\n",
    "learning_rate = 0.7           # Learning rate\n",
    "\n",
    "# Evaluation parameters\n",
    "n_eval_episodes = 100        # Total number of test episodes\n",
    "\n",
    "# DO NOT MODIFY EVAL_SEED\n",
    "eval_seed = [16,54,165,177,191,191,120,80,149,178,48,38,6,125,174,73,50,172,100,148,146,6,25,40,68,148,49,167,9,97,164,176,61,7,54,55,\n",
    " 161,131,184,51,170,12,120,113,95,126,51,98,36,135,54,82,45,95,89,59,95,124,9,113,58,85,51,134,121,169,105,21,30,11,50,65,12,43,82,145,152,97,106,55,31,85,38,\n",
    " 112,102,168,123,97,21,83,158,26,80,63,5,81,32,11,28,148] # Evaluation seed, this ensures that all classmates agents are trained on the same taxi starting position\n",
    "                                                          # Each seed has a specific starting state\n",
    "\n",
    "# Environment parameters\n",
    "env_id = \"Taxi-v3\"           # Name of the environment\n",
    "max_steps = 99               # Max steps per episode\n",
    "gamma = 0.95                 # Discounting rate\n",
    "\n",
    "# Exploration parameters\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.05           # Minimum exploration probability\n",
    "decay_rate = 0.005            # Exponential decay rate for exploration prob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1TMORo1VLTsX"
   },
   "source": [
    "## Trainieren Sie unseren Q-Learning-Agenten 🏃."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WwP3Y2z2eS-K"
   },
   "outputs": [],
   "source": [
    "Qtable_taxi = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_taxi)\n",
    "Qtable_taxi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wPdu0SueLVl2"
   },
   "source": [
    "## Erstellen Sie ein Modellwörterbuch 💾 und veröffentlichen Sie unser trainiertes Modell auf dem Hub 🔥.\n",
    "\n",
    "- Wir erstellen ein Modell-Wörterbuch, das alle Trainings-Hyperparameter für die Reproduzierbarkeit und die Q-Tabelle enthalten wird.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0a1FpE_3hNYr"
   },
   "outputs": [],
   "source": [
    "model = {\n",
    "    \"env_id\": env_id,\n",
    "    \"max_steps\": max_steps,\n",
    "    \"n_training_episodes\": n_training_episodes,\n",
    "    \"n_eval_episodes\": n_eval_episodes,\n",
    "    \"eval_seed\": eval_seed,\n",
    "\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"gamma\": gamma,\n",
    "\n",
    "    \"max_epsilon\": max_epsilon,\n",
    "    \"min_epsilon\": min_epsilon,\n",
    "    \"decay_rate\": decay_rate,\n",
    "\n",
    "    \"qtable\": Qtable_taxi\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dhQtiQozhOn1"
   },
   "outputs": [],
   "source": [
    "username = \"\" # FILL THIS\n",
    "repo_name = \"\" # FILL THIS\n",
    "push_to_hub(\n",
    "    repo_id=f\"{username}/{repo_name}\",\n",
    "    model=model,\n",
    "    env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZgSdjgbIpRti"
   },
   "source": [
    "Jetzt, wo es auf dem Hub ist, kannst du die Ergebnisse deines Taxi-v3 mit deinen Klassenkameraden in der Bestenliste vergleichen 🏆 👉 https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard\n",
    "\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/taxi-leaderboard.png\" alt=\"Taxi-Rangliste\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bzgIO70c0bu2"
   },
   "source": [
    "# Teil 3: Laden vom Hub 🔽.\n",
    "\n",
    "Das Erstaunliche an Hugging Face Hub 🤗 ist, dass du ganz einfach leistungsstarke Modelle aus der Community laden kannst.\n",
    "\n",
    "Das Laden eines gespeicherten Modells aus dem Hub ist wirklich einfach:\n",
    "\n",
    "1. Du gehst auf https://huggingface.co/models?other=q-learning, um die Liste aller gespeicherten q-learning-Modelle zu sehen.\n",
    "2. Sie wählen eines aus und kopieren seine repo_id\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/copy-id.png\" alt=\"ID kopieren\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTth6thRoC6X"
   },
   "source": [
    "3. Dann müssen wir nur `load_from_hub` mit verwenden:\n",
    "- Die Repo_id\n",
    "- Der Dateiname: das gespeicherte Modell innerhalb der Repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EtrfoTaBoNrd"
   },
   "source": [
    "#### Dieser Code darf nicht verändert werden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eo8qEzNtCaVI"
   },
   "outputs": [],
   "source": [
    "from urllib.error import HTTPError\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "\n",
    "def load_from_hub(repo_id: str, filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Download a model from Hugging Face Hub.\n",
    "    :param repo_id: id of the model repository from the Hugging Face Hub\n",
    "    :param filename: name of the model zip file from the repository\n",
    "    \"\"\"\n",
    "    # Get the model from the Hub, download and cache the model on your local disk\n",
    "    pickle_model = hf_hub_download(\n",
    "        repo_id=repo_id,\n",
    "        filename=filename\n",
    "    )\n",
    "\n",
    "    with open(pickle_model, 'rb') as f:\n",
    "      downloaded_model_file = pickle.load(f)\n",
    "\n",
    "    return downloaded_model_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b_sM2gNioPZH"
   },
   "source": [
    "### ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JUm9lz2gCQcU"
   },
   "outputs": [],
   "source": [
    "model = load_from_hub(repo_id=\"ThomasSimonini/q-Taxi-v3\", filename=\"q-learning.pkl\") # Try to use another model\n",
    "\n",
    "print(model)\n",
    "env = gym.make(model[\"env_id\"])\n",
    "\n",
    "evaluate_agent(env, model[\"max_steps\"], model[\"n_eval_episodes\"], model[\"qtable\"], model[\"eval_seed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O7pL8rg1MulN"
   },
   "outputs": [],
   "source": [
    "model = load_from_hub(repo_id=\"ThomasSimonini/q-FrozenLake-v1-no-slippery\", filename=\"q-learning.pkl\") # Try to use another model\n",
    "\n",
    "env = gym.make(model[\"env_id\"], is_slippery=False)\n",
    "\n",
    "evaluate_agent(env, model[\"max_steps\"], model[\"n_eval_episodes\"], model[\"qtable\"], model[\"eval_seed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQAwLnYFPk-s"
   },
   "source": [
    "## Einige zusätzliche Herausforderungen 🏆\n",
    "\n",
    "Die beste Art zu lernen **ist, Dinge selbst auszuprobieren**! Wie Sie gesehen haben, ist der derzeitige Agent nicht besonders gut. Als ersten Vorschlag können Sie für mehr Schritte trainieren. Bei 1.000.000 Schritten haben wir tolle Ergebnisse gesehen!\n",
    "\n",
    "In der [Rangliste] (https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) findest du deine Agenten. Können Sie sich an die Spitze setzen?\n",
    "\n",
    "Hier sind einige Ideen, um in der Rangliste aufzusteigen:\n",
    "\n",
    "* Trainiere mehr Schritte\n",
    "* Probiere verschiedene Hyperparameter aus, indem du dir ansiehst, was deine Klassenkameraden gemacht haben.\n",
    "* Pushe dein neu trainiertes Modell** auf dem Hub 🔥.\n",
    "\n",
    "Sind Ihnen das Laufen auf Eis und das Fahren von Taxis zu langweilig? Versuche, die **Umgebung** zu verändern, warum nicht die rutschige Version von FrozenLake-v1 benutzen? Schau dir an, wie sie funktionieren [mit Hilfe der Turnhallendokumentation] (https://gymnasium.farama.org/) und hab Spaß 🎉."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-fW-EU5WejJ"
   },
   "source": [
    "_____________________________________________________________________\n",
    "Herzlichen Glückwunsch 🥳, Sie haben soeben Ihren ersten Reinforcement Learning Agent implementiert, trainiert und hochgeladen.\n",
    "\n",
    "Das Verständnis von Q-Learning ist ein **wichtiger Schritt zum Verständnis wertbasierter Methoden**.\n",
    "\n",
    "In der nächsten Einheit mit Deep Q-Learning werden wir sehen, dass das Erstellen und Aktualisieren einer Q-Tabelle zwar eine gute Strategie war - **aber nicht skalierbar ist.**\n",
    "\n",
    "Stellen Sie sich zum Beispiel vor, Sie erstellen einen Agenten, der lernt, Doom zu spielen.\n",
    "\n",
    "<img src=\"https://vizdoom.cs.put.edu.pl/user/pages/01.tutorial/basic.png\" alt=\"Doom\"/>\n",
    "\n",
    "Doom ist eine große Umgebung mit einem riesigen Zustandsraum (Millionen von verschiedenen Zuständen). Das Erstellen und Aktualisieren einer Q-Tabelle für diese Umgebung wäre nicht effizient.\n",
    "\n",
    "Deshalb werden wir uns in der nächsten Einheit mit Deep Q-Learning beschäftigen, einem Algorithmus, **bei dem wir ein neuronales Netz verwenden, das bei einem bestimmten Zustand die verschiedenen Q-Werte für jede Aktion annähert**.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/atari-envs.gif\" alt=\"Umgebungen\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BjLhT70TEZIn"
   },
   "source": [
    "Wir sehen uns in Referat 3! 🔥\n",
    "\n",
    "## Lernt weiter, bleibt fantastisch 🤗"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "67OdoKL63eDD",
    "B2_-8b8z5k54",
    "8R5ej1fS4P2V",
    "Pnpk2ePoem3r"
   ],
   "include_colab_link": true,
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
