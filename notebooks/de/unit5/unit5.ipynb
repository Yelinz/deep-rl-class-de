{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/notebooks/unit5/unit5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"In Colab √∂ffnen\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2D3NL_e4crQv"
   },
   "source": [
    "# Unit 5: Einf√ºhrung in ML-Agenten\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97ZiytXEgqIz"
   },
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/thumbnail.png\" alt=\"Vorschaubild\"/>\n",
    "\n",
    "In diesem Notizbuch lernen Sie ML-Agenten kennen und trainieren zwei Agenten.\n",
    "\n",
    "- Der erste wird lernen, **Schneeb√§lle auf spawnende Ziele zu schie√üen**.\n",
    "- Der zweite muss einen Knopf dr√ºcken, um eine Pyramide zu spawnen, dann zur Pyramide navigieren, sie umsto√üen **und sich zum Goldstein an der Spitze bewegen**. Dazu muss er seine Umgebung erkunden, und wir werden eine Technik namens Neugier anwenden.\n",
    "\n",
    "Danach kannst du **deinen Agenten direkt in deinem Browser beim Spielen beobachten**.\n",
    "\n",
    "Weitere Informationen √ºber den Zertifizierungsprozess finden Sie in diesem Abschnitt üëâ https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FMYrDriDujzX"
   },
   "source": [
    "‚¨áÔ∏è Hier ist ein Beispiel daf√ºr, was **Sie am Ende dieser Einheit erreichen werden**. ‚¨áÔ∏è\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cBmFlh8suma-"
   },
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/pyramids.gif\" alt=\"Pyramiden\"/>\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/snowballtarget.gif\" alt=\"SnowballTarget\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A-cYE0K5iL-w"
   },
   "source": [
    "### üéÆ Umgebungen:\n",
    "\n",
    "- [Pyramiden](https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Learning-Environment-Examples.md#pyramids)\n",
    "- SchneeballZiel\n",
    "\n",
    "### üìö RL-Library:\n",
    "\n",
    "- [ML-Agenten](https://github.com/Unity-Technologies/ml-agents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qEhtaFh9i31S"
   },
   "source": [
    "Wir versuchen st√§ndig, unsere Tutorials zu verbessern, also **wenn Sie Probleme in diesem Notizbuch** finden, √∂ffnen Sie bitte [einen Fehler im GitHub Repo](https://github.com/huggingface/deep-rl-class/issues)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7f63r3Yi5vE"
   },
   "source": [
    "## Ziele dieses Notizbuchs üèÜ\n",
    "\n",
    "Am Ende des Notizbuchs werden Sie:\n",
    "\n",
    "- Verstehen, wie **ML-Agents**, die Umgebungsbibliothek, funktioniert.\n",
    "- In der Lage sein, **Agenten in Unity-Umgebungen** zu trainieren.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "viNzVbVaYvY3"
   },
   "source": [
    "## Dieses Notizbuch stammt aus dem Deep Reinforcement Learning Kurs.\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/deep-rl-course-illustration.jpg\" alt=\"Deep RL Course illustration\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6p5HnEefISCB"
   },
   "source": [
    "In diesem kostenlosen Kurs lernen Sie:\n",
    "\n",
    "- üìñ Deep Reinforcement Learning in **Theorie und Praxis** studieren.\n",
    "- üßë‚Äçüíª Lernen Sie, **ber√ºhmte Deep RL-Bibliotheken** wie Stable Baselines3, RL Baselines3 Zoo, CleanRL und Sample Factory 2.0 zu verwenden.\n",
    "- ü§ñ Trainieren Sie **Agenten in einzigartigen Umgebungen**.\n",
    "\n",
    "Und mehr, siehe üìö den Lehrplan üëâ https://huggingface.co/deep-rl-course/communication/publishing-schedule\n",
    "\n",
    "Vergessen Sie nicht, sich **<a href=\"http://eepurl.com/ic5ZUD\">f√ºr den Kurs anzumelden</a>** (wir sammeln Ihre E-Mail, um Ihnen **die Links zu senden, wenn die einzelnen Einheiten ver√∂ffentlicht werden, und Sie √ºber die Herausforderungen und Aktualisierungen zu informieren).**\n",
    "\n",
    "\n",
    "Der beste Weg, um in Kontakt zu bleiben, ist, unserem Discord-Server beizutreten, um sich mit der Community und mit uns auszutauschen üëâüèª https://discord.gg/ydHrjt3WP5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-mo_6rXIjRi"
   },
   "source": [
    "## Voraussetzungen üèóÔ∏è\n",
    "Bevor Sie sich mit dem Notebook besch√§ftigen, m√ºssen Sie:\n",
    "\n",
    "üî≤ üìö **Lesen Sie [was ML-Agenten sind und wie sie funktionieren, indem Sie Unit 5 lesen](https://huggingface.co/deep-rl-course/unit5/introduction)** ü§ó"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xYO1uD5Ujgdh"
   },
   "source": [
    "# Lasst uns unsere Agenten trainieren üöÄ\n",
    "\n",
    "**Um dieses Hands-on f√ºr den Zertifizierungsprozess zu validieren, m√ºssen Sie nur Ihre trainierten Modelle an den Hub senden**. Es m√ºssen keine Ergebnisse erzielt werden, um dies zu validieren. Aber wenn Sie sch√∂ne Ergebnisse erhalten m√∂chten, k√∂nnen Sie versuchen, diese zu erreichen:\n",
    "\n",
    "- F√ºr `Pyramiden` : Mittlere Belohnung = 1.75\n",
    "- F√ºr `SnowballTarget` : Mittlere Belohnung = 15 oder 30 getroffene Ziele in einer Episode.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DssdIjk_8vZE"
   },
   "source": [
    "## Die GPU einstellen üí™.\n",
    "- Um **das Training des Agenten zu beschleunigen, werden wir einen Grafikprozessor** verwenden. Gehen Sie dazu auf \"Runtime > Change Runtime type\".\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step1.jpg\" alt=\"GPU Schritt 1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sTfCXHy68xBv"
   },
   "source": [
    "- Hardware-Beschleuniger > GPU\".\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step2.jpg\" alt=\"GPU Schritt 2\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "an3ByrXYQ4iK"
   },
   "source": [
    "## Klonen Sie das Repository und installieren Sie die Abh√§ngigkeiten üîΩ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6WNoL04M7rTa"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Clone the repository\n",
    "!git clone --depth 1 https://github.com/Unity-Technologies/ml-agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d8wmVcMk7xKo"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Go inside the repository and install the package\n",
    "%cd ml-agents\n",
    "!pip3 install -e ./ml-agents-envs\n",
    "!pip3 install -e ./ml-agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5_7Ptd_kEcG"
   },
   "source": [
    "## SnowballTarget ‚õÑ\n",
    "\n",
    "Wenn Sie eine Auffrischung ben√∂tigen, wie diese Umgebungen funktionieren, lesen Sie diesen Abschnitt üëâ\n",
    "https://huggingface.co/deep-rl-course/unit5/snowball-target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HRY5ufKUKfhI"
   },
   "source": [
    "### Laden Sie die Umgebungs-Zip-Datei herunter und verschieben Sie sie in `./training-envs-executables/linux/`\n",
    "- Unsere ausf√ºhrbare Umgebungsdatei befindet sich in einer Zip-Datei.\n",
    "- Wir m√ºssen sie herunterladen und in `./training-envs-executables/linux/` ablegen.\n",
    "- Wir verwenden eine ausf√ºhrbare Datei f√ºr Linux, weil wir colab verwenden und das Betriebssystem der colab-Maschinen Ubuntu (Linux) ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C9Ls6_6eOKiA"
   },
   "outputs": [],
   "source": [
    "# Here, we create training-envs-executables and linux\n",
    "!mkdir ./training-envs-executables\n",
    "!mkdir ./training-envs-executables/linux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekSh8LWawkB5"
   },
   "source": [
    "Wir haben die Datei SnowballTarget.zip von https://github.com/huggingface/Snowball-Target mit `wget` heruntergeladen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6LosWO50wa77"
   },
   "outputs": [],
   "source": [
    "!wget \"https://github.com/huggingface/Snowball-Target/raw/main/SnowballTarget.zip\" -O ./training-envs-executables/linux/SnowballTarget.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_LLVaEEK3ayi"
   },
   "source": [
    "Wir entpacken die Datei executable.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8FPx0an9IAwO"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!unzip -d ./training-envs-executables/linux/ ./training-envs-executables/linux/SnowballTarget.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nyumV5XfPKzu"
   },
   "source": [
    "Stellen Sie sicher, dass Ihre Datei zug√§nglich ist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EdFsLJ11JvQf"
   },
   "outputs": [],
   "source": [
    "!chmod -R 755 ./training-envs-executables/linux/SnowballTarget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NAuEq32Mwvtz"
   },
   "source": [
    "### Definieren Sie die SnowballTarget-Konfigurationsdatei\n",
    "- In ML-Agents definieren Sie die **Trainings-Hyperparameter in config.yaml-Dateien**.\n",
    "\n",
    "Es gibt eine Vielzahl von Hyperparametern. Um sie besser kennenzulernen, sollten Sie jede Erkl√§rung in der [Dokumentation](https://github.com/Unity-Technologies/ml-agents/blob/release_20_docs/docs/Training-Configuration-File.md) nachlesen.\n",
    "\n",
    "\n",
    "Sie m√ºssen also eine Konfigurationsdatei `SnowballTarget.yaml` in ./content/ml-agents/config/ppo/ erstellen.\n",
    "\n",
    "Wir geben Ihnen hier eine erste Version dieser Konfigurationsdatei (zum Kopieren und Einf√ºgen in Ihre `SnowballTarget.yaml-Datei`), **aber Sie sollten sie √§ndern**.\n",
    "\n",
    "```\n",
    "Verhaltensweisen:\n",
    "  SnowballTarget:\n",
    "    trainer_type: ppo\n",
    "    zusammenfassung_freq: 10000\n",
    "    keep_checkpoints: 10\n",
    "    checkpoint_interval: 50000\n",
    "    max_steps: 200000\n",
    "    zeit_horizont: 64\n",
    "    threaded: wahr\n",
    "    Hyperparameter:\n",
    "      learning_rate: 0.0003\n",
    "      lernrate_schedule: linear\n",
    "      batch_size: 128\n",
    "      puffer_gr√∂√üe: 2048\n",
    "      beta: 0.005\n",
    "      epsilon: 0.2\n",
    "      lambd: 0.95\n",
    "      num_epoch: 3\n",
    "    Netzwerk_Einstellungen:\n",
    "      normalize: false\n",
    "      versteckte_Einheiten: 256\n",
    "      num_layers: 2\n",
    "      vis_encode_type: einfach\n",
    "    Belohnungs-Signale:\n",
    "      extrinsisch:\n",
    "        gamma: 0.99\n",
    "        St√§rke: 1.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4U3sRH4N4h_l"
   },
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/snowballfight_config1.png\" alt=\"Config SnowballTarget\"/>\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/snowballfight_config2.png\" alt=\"Config SnowballTarget\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJJdo_5AyoGo"
   },
   "source": [
    "Zum Experimentieren sollten Sie auch versuchen, einige andere Hyperparameter zu √§ndern. Unity bietet eine sehr [gute Dokumentation, in der jeder einzelne Parameter erkl√§rt wird] (https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Training-Configuration-File.md).\n",
    "\n",
    "Nun, da du die Konfigurationsdatei erstellt hast und verstehst, was die meisten Hyperparameter tun, sind wir bereit, unseren Agenten zu trainieren üî•."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9fI555bO12v"
   },
   "source": [
    "### Ausbildung des Agenten\n",
    "\n",
    "Um unseren Agenten zu trainieren, m√ºssen wir nur **mlagents-learn starten und die ausf√ºhrbare Datei ausw√§hlen, die die Umgebung enth√§lt**.\n",
    "\n",
    "Wir definieren vier Parameter:\n",
    "\n",
    "1. mlagents-learn <config>\": der Pfad, in dem sich die Konfigurationsdatei der Hyperparameter befindet.\n",
    "2. `--env`: wo sich die ausf√ºhrbare Umgebung befindet.\n",
    "3. `--run_id`: der Name, den Sie Ihrer Trainingslauf-ID geben wollen.\n",
    "4. `--no-graphics`: um die Visualisierung w√§hrend des Trainings nicht zu starten.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/mlagentslearn.png\" alt=\"MlAgents learn\"/>\n",
    "\n",
    "Trainieren Sie das Modell und verwenden Sie das `--resume` Flag, um das Training im Falle einer Unterbrechung fortzusetzen.\n",
    "\n",
    "> Es wird beim ersten Mal fehlschlagen, wenn Sie `--resume` verwenden. Versuchen Sie, den Block erneut auszuf√ºhren, um den Fehler zu umgehen.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lN32oWF8zPjs"
   },
   "source": [
    "Das Training dauert je nach Konfiguration zwischen 10 und 35 Minuten, ein ‚òïÔ∏èyou ist es wert ü§ó."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bS-Yh1UdHfzy"
   },
   "outputs": [],
   "source": [
    "!mlagents-learn ./config/ppo/SnowballTarget.yaml --env=./training-envs-executables/linux/SnowballTarget/SnowballTarget --run-id=\"SnowballTarget1\" --no-graphics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Vue94AzPy1t"
   },
   "source": [
    "### Schieben Sie den Agenten zum ü§ó Hub\n",
    "\n",
    "- Nun, da wir unseren Agenten trainiert haben, sind wir **bereit, ihn in den Hub zu pushen, um ihn in deinem Browser abspielen zu k√∂nnenüî•.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "izT6FpgNzZ6R"
   },
   "source": [
    "Um Ihr Modell mit der Gemeinschaft teilen zu k√∂nnen, sind drei weitere Schritte erforderlich:\n",
    "\n",
    "1Ô∏è‚É£ (Falls noch nicht geschehen) Erstellen Sie ein Konto f√ºr HF ‚û° https://huggingface.co/join\n",
    "\n",
    "2Ô∏è‚É£ Melde dich an und speichere dann dein Authentifizierungs-Token von der Hugging Face Website.\n",
    "- Erstellen Sie ein neues Token (https://huggingface.co/settings/tokens) **mit Schreibrolle**\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/create-token.jpg\" alt=\"HF-Token erstellen\">\n",
    "\n",
    "- Kopieren Sie das Token\n",
    "- F√ºhren Sie die Zelle unten aus und f√ºgen Sie das Token ein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rKt2vsYoK56o"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSU9qD9_6dem"
   },
   "source": [
    "Wenn Sie kein Google Colab oder ein Jupyter Notebook verwenden m√∂chten, m√ºssen Sie stattdessen diesen Befehl verwenden: `huggingface-cli login`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KK4fPfnczunT"
   },
   "source": [
    "Dann m√ºssen wir einfach `mlagents-push-to-hf` ausf√ºhren.\n",
    "\n",
    "Und wir definieren 4 Parameter:\n",
    "\n",
    "1. Lauf-id\": der Name des Trainingslaufs (id).\n",
    "2. `--local-dir`: wo der Agent gespeichert wurde, es ist results/<run_id name>, also in meinem Fall results/First Training.\n",
    "3. `--repo-id`: der Name des Hugging Face Repos, das du erstellen oder aktualisieren willst. Es ist immer <Ihr Hugging-Face-Benutzername>/<Der Repo-Name>\n",
    "Wenn das Repo nicht existiert, wird es automatisch erstellt**.\n",
    "4. `--commit-message`: Da HF-Repos Git-Repos sind, m√ºssen Sie eine Commit-Message definieren.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/mlagentspushtohub.png\" alt=\"Push to Hub\"/>\n",
    "\n",
    "Zum Beispiel:\n",
    "\n",
    "`!mlagents-push-to-hf --run-id=\"SnowballTarget1\" --local-dir=\"./results/SnowballTarget1\" --repo-id=\"ThomasSimonini/ppo-SnowballTarget\" --commit-message=\"First Push\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kAFzVB7OYj_H"
   },
   "outputs": [],
   "source": [
    "!mlagents-push-to-hf --run-id=\"SnowballTarget1\" --local-dir=\"./results/SnowballTarget1\" --repo-id=\"ThomasSimonini/ppo-SnowballTarget\" --commit-message=\"First Push\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dGEFAIboLVc6"
   },
   "outputs": [],
   "source": [
    "!mlagents-push-to-hf  --run-id= # Add your run id  --local-dir= # Your local dir  --repo-id= # Your repo id  --commit-message= # Your commit message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yborB0850FTM"
   },
   "source": [
    "Andernfalls, wenn alles funktioniert hat, sollten Sie am Ende des Prozesses folgendes Ergebnis haben (allerdings mit einer anderen URL üòÜ):\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "Ihr Modell wird in den Hub √ºbertragen. Du kannst dein Modell hier sehen: https://huggingface.co/ThomasSimonini/ppo-SnowballTarget\n",
    "```\n",
    "\n",
    "Es ist der Link zu Ihrem Modell, es enth√§lt eine Modellkarte, die erkl√§rt, wie man es benutzt, Ihr Tensorboard und Ihre Konfigurationsdatei. **Das Tolle ist, dass es sich um ein Git-Repository handelt, d.h. Sie k√∂nnen verschiedene Commits haben, Ihr Repository mit einem neuen Push aktualisieren usw.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Uaon2cg0NrL"
   },
   "source": [
    "Aber jetzt kommt das Beste: **Ihren Agenten online visualisieren zu k√∂nnen üëÄ.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VMc4oOsE0QiZ"
   },
   "source": [
    "### Beobachte deinen Agenten beim Spielen üëÄ\n",
    "\n",
    "Dieser Schritt ist ganz einfach:\n",
    "\n",
    "1. Merken Sie sich Ihre Repo-ID\n",
    "\n",
    "2. Gehen Sie hier: https://huggingface.co/spaces/ThomasSimonini/ML-Agents-SnowballTarget\n",
    "\n",
    "3. Starten Sie das Spiel und schalten Sie es in den Vollbildmodus, indem Sie auf die Schaltfl√§che unten rechts klicken\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/snowballtarget_load.png\" alt=\"Schneeballtarget laden\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Djs8c5rR0Z8a"
   },
   "source": [
    "1. In Schritt 1 w√§hlen Sie Ihr Modell-Repository, das die Modell-ID ist (in meinem Fall ThomasSimonini/ppo-SnowballTarget).\n",
    "\n",
    "2. In Schritt 2 **w√§hlen Sie das Modell aus, das Sie wiedergeben m√∂chten**:\n",
    "  - Ich habe mehrere, da wir alle 500000 Zeitschritte ein Modell gespeichert haben.\n",
    "  - Aber wenn ich das aktuellste Modell m√∂chte, w√§hle ich \"SnowballTarget.onnx\".\n",
    "\n",
    "üëâ Das Sch√∂ne ist, **dass man verschiedene Modelle ausprobieren kann, um die Verbesserung des Agenten zu sehen**.\n",
    "\n",
    "Und z√∂gere nicht, die beste Punktzahl deines Agenten auf Discord im #rl-i-made-this Kanal zu teilen üî•.\n",
    "\n",
    "Versuchen wir nun eine schwierigere Umgebung namens Pyramiden..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVMwRi4y_tmx"
   },
   "source": [
    "## Pyramiden üèÜ\n",
    "\n",
    "### Laden Sie die Umgebungs-Zip-Datei herunter und verschieben Sie sie in `./training-envs-executables/linux/`\n",
    "- Unsere ausf√ºhrbare Umgebungsdatei befindet sich in einer Zip-Datei.\n",
    "- Wir m√ºssen sie herunterladen und in `./training-envs-executables/linux/` ablegen.\n",
    "- Wir verwenden eine ausf√ºhrbare Datei f√ºr Linux, weil wir colab verwenden und das Betriebssystem der colab-Maschinen Ubuntu (Linux) ist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NyqYYkLyAVMK"
   },
   "source": [
    "Laden Sie die Datei Pyramids.zip von https://drive.google.com/uc?export=download&id=1UiFNdKlsH0NTu32xV-giYUEVKV4-vc7H mit `wget` herunter. Sehen Sie sich die vollst√§ndige L√∂sung zum Herunterladen gro√üer Dateien von GDrive [hier] an (https://bcrf.biochem.wisc.edu/2021/02/05/download-google-drive-files-using-wget/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AxojCsSVAVMP"
   },
   "outputs": [],
   "source": [
    "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UiFNdKlsH0NTu32xV-giYUEVKV4-vc7H' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1UiFNdKlsH0NTu32xV-giYUEVKV4-vc7H\" -O ./training-envs-executables/linux/Pyramids.zip && rm -rf /tmp/cookies.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bfs6CTJ1AVMP"
   },
   "source": [
    "**ODER** Laden Sie die Datei direkt auf den lokalen Rechner herunter und ziehen Sie sie dann per Drag-and-Drop in das Verzeichnis `./training-envs-executables/linux`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H7JmgOwcSSmF"
   },
   "source": [
    "Warten Sie, bis der Upload abgeschlossen ist, und f√ºhren Sie dann den folgenden Befehl aus.\n",
    "\n",
    "![Bild.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASYAAAAfCAYAAABKxmALAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAAAmZSURBVHhe7d0NTNTnHQfwL+rxcigHFHUH9LCCVuaKCWgB4029Gq7NcBuLPdPKEhh6iRhEmauLY2V0ptEUSZGwTJTA5jSBOnGDmpyxEMUqxEH0rOIQfDmtt/oCnsof9BT2PPAcx8nbeTQi9vcxF56X+///h8n98vs9/xfcYmJiukEIIS+RCeInIYS8NChjIoS4LCwsDMHBwfDx8cEc76u4dNcT5y63orm5WbzDNRSYCCHPhQejuLg4xMbGwtvbW4wCymNLRAuQFAtwpW0Sio+34+uzN8So8ygwEUKc4uXlheTkZCxbtkyMOOofmGy63P1R37EAm4u+QUdHhxgdGa0xEUJGxLOknJycQYPSvXv3cPnyZTx0n4Fut4litNeEx61YMNGAQxun9uzDWZQxEUKGxQNKVlaWQ9nGg9Hhw4dx8uRJ3Lx5U4wCQYouvD3bB+qfTEWsT70Ytfv1l287tf5EgYkQMiRevvFMSalUihHAYDCgpKQEjx49EiODWzwL+MN73Zji1iZGgO/8l+ODbQ0jlnXjtJQLheZ9HTQzRXcAHbKK8pEeK7rfi5GOScirh68p9Q9K+/fvx65du0YMStyxS0DcTjecuh8lRoDprRXY8pvFoje0IQJTNNLzilBU5PjK35qO+Dly8Z6xFAF1nBZaTbjovwhjcUxCxg4v4fqvKfFM6cCBA6LnvIziK7jdFSh6wBKfE4iaGyJ6gxs2Y5LOFiIlJYW90pC5rQT1CEfCulRoxzw2lSN7TQo272kU/RdhLI5JyNjhlwTY8DUlXr65avMBq2gBkyQTUjUeoje4IdaYeMakR1hLIdJ21okxJigJ2z+JgeVQBpoj86GVVSEzcx/M/ecqMvCVajv0Ac1okEcg0s8EQ0o2KiNX4XeJaqgUMqDLCkuLAQXb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iWUUcs0_794U"
   },
   "source": [
    "Unzip it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i2E3K4V2AVMP"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!unzip -d ./training-envs-executables/linux/ ./training-envs-executables/linux/Pyramids.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KmKYBgHTAVMP"
   },
   "source": [
    "Stellen Sie sicher, dass Ihre Datei zug√§nglich ist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Im-nwvLPAVMP"
   },
   "outputs": [],
   "source": [
    "!chmod -R 755 ./training-envs-executables/linux/Pyramids/Pyramids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fqceIATXAgih"
   },
   "source": [
    "### √Ñndern Sie die PyramidsRND-Konfigurationsdatei\n",
    "- Im Gegensatz zur ersten Umgebung, die eine benutzerdefinierte Umgebung war, wurde **Pyramids vom Unity-Team** erstellt.\n",
    "- Die PyramidsRND-Konfigurationsdatei existiert also bereits und befindet sich in ./content/ml-agents/config/ppo/PyramidsRND.yaml\n",
    "- Sie fragen sich vielleicht, warum \"RND\" in PyramidsRND. RND steht f√ºr *random network distillation* es ist ein Weg, um Neugierbelohnungen zu generieren. Wenn Sie mehr dar√ºber wissen wollen, haben wir einen Artikel geschrieben, der diese Technik erkl√§rt: https://medium.com/data-from-the-trenches/curiosity-driven-learning-through-random-network-distillation-488ffd8e5938\n",
    "\n",
    "F√ºr dieses Training werden wir eine Sache √§ndern:\n",
    "- Der Hyperparameter f√ºr die Gesamtzahl der Trainingsschritte ist zu hoch, da wir den Benchmark (mittlere Belohnung = 1,75) in nur 1M Trainingsschritten erreichen k√∂nnen.\n",
    "üëâ Dazu gehen wir zu config/ppo/PyramidsRND.yaml,**und √§ndern diese auf max_steps auf 1000000.**\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/pyramids-config.png\" alt=\"Pyramids config\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RI-5aPL7BWVk"
   },
   "source": [
    "Als Experiment sollten Sie auch versuchen, einige andere Hyperparameter zu √§ndern, Unity bietet eine sehr [gute Dokumentation, die jeden von ihnen hier erkl√§rt] (https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Training-Configuration-File.md).\n",
    "\n",
    "Wir sind jetzt bereit, unseren Agenten zu trainieren üî•."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s5hr1rvIBdZH"
   },
   "source": [
    "### Ausbildung des Agenten\n",
    "\n",
    "Das Training dauert je nach Rechner 30 bis 45 Minuten, gehen Sie auf ‚òïÔ∏èyou und verdienen Sie es ü§ó."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fXi4-IaHBhqD"
   },
   "outputs": [],
   "source": [
    "!mlagents-learn ./config/ppo/PyramidsRND.yaml --env=./training-envs-executables/linux/Pyramids/Pyramids --run-id=\"Pyramids Training\" --no-graphics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "txonKxuSByut"
   },
   "source": [
    "### Schieben Sie den Agenten zum ü§ó Hub\n",
    "\n",
    "- Nun, da wir unseren Agenten trainiert haben, sind wir **bereit, ihn in den Hub zu pushen, um ihn in deinem Browser abspielen zu k√∂nnenüî•.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yiEQbv7rB4mU"
   },
   "outputs": [],
   "source": [
    "!mlagents-push-to-hf  --run-id= # Add your run id  --local-dir= # Your local dir  --repo-id= # Your repo id  --commit-message= # Your commit message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7aZfgxo-CDeQ"
   },
   "source": [
    "### Beobachte deinen Agenten beim Spielen üëÄ\n",
    "\n",
    "üëâ https://huggingface.co/spaces/unity/ML-Agents-Pyramids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGG_oq2n0wjB"
   },
   "source": [
    "### üéÅ Bonus: Warum nicht in einer anderen Umgebung trainieren?\n",
    "Da Sie nun wissen, wie Sie einen Agenten mit MLAgents trainieren k√∂nnen, **warum nicht eine andere Umgebung ausprobieren?**\n",
    "\n",
    "MLAgents bietet 17 verschiedene Umgebungen an, und wir sind dabei, einige eigene zu entwickeln. Der beste Weg, um zu lernen, ist, Dinge selbst auszuprobieren, Spa√ü zu haben.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KSAkJxSr0z6-"
   },
   "source": [
    "![cover](https://miro.medium.com/max/1400/0*xERdThTRRM2k_U9f.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YiyF4FX-04JB"
   },
   "source": [
    "Die vollst√§ndige Liste der offiziellen Unity-Umgebungen finden Sie hier üëâ https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Learning-Environment-Examples.md\n",
    "\n",
    "F√ºr die Demos zur Visualisierung deines Agenten üëâ https://huggingface.co/unity\n",
    "\n",
    "F√ºr jetzt haben wir integriert:\n",
    "- [Worm](https://huggingface.co/spaces/unity/ML-Agents-Worm) Demo, in der man einem **Wurm das Krabbeln** beibringt.\n",
    "- [Walker](https://huggingface.co/spaces/unity/ML-Agents-Walker) Demo, in der man einem Agenten beibringt, **zu einem Ziel zu laufen**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PI6dPWmh064H"
   },
   "source": [
    "Das war's f√ºr heute. Herzlichen Gl√ºckwunsch zur Fertigstellung dieses Tutorials!\n",
    "\n",
    "Der beste Weg zu lernen ist, zu √ºben und Dinge auszuprobieren. Warum nicht eine andere Umgebung ausprobieren? ML-Agents hat 17 verschiedene Umgebungen, aber Sie k√∂nnen auch Ihre eigene erstellen? Schauen Sie in die Dokumentation und haben Sie Spa√ü!\n",
    "\n",
    "Wir sehen uns in Unit 6 üî•,\n",
    "\n",
    "## Keep Learning, Stay awesome ü§ó"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
