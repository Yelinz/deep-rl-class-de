{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Yelinz/deep-rl-class-de/blob/main/notebooks/de/unit5/unit5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"In Colab √∂ffnen\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2D3NL_e4crQv"
   },
   "source": [
    "# Unit 5: Einf√ºhrung in ML-Agenten\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97ZiytXEgqIz"
   },
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/thumbnail.png\" alt=\"Vorschaubild\"/>\n",
    "\n",
    "In diesem Notizbuch lernen Sie ML-Agenten kennen und trainieren zwei Agenten.\n",
    "\n",
    "- Der erste wird lernen, **Schneeb√§lle auf spawnende Ziele zu schie√üen**.\n",
    "- Der zweite muss einen Knopf dr√ºcken, um eine Pyramide zu spawnen, dann zur Pyramide navigieren, sie umsto√üen **und sich zum Goldstein an der Spitze bewegen**. Dazu muss er seine Umgebung erkunden, und wir werden eine Technik namens Neugier anwenden.\n",
    "\n",
    "Danach kannst du **deinen Agenten direkt in deinem Browser beim Spielen beobachten**.\n",
    "\n",
    "Weitere Informationen √ºber den Zertifizierungsprozess finden Sie in diesem Abschnitt üëâ https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FMYrDriDujzX"
   },
   "source": [
    "‚¨áÔ∏è Hier ist ein Beispiel daf√ºr, was **Sie am Ende dieser Einheit erreichen werden**. ‚¨áÔ∏è\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cBmFlh8suma-"
   },
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/pyramids.gif\" alt=\"Pyramiden\"/>\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/snowballtarget.gif\" alt=\"SnowballTarget\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A-cYE0K5iL-w"
   },
   "source": [
    "### üéÆ Umgebungen:\n",
    "\n",
    "- [Pyramiden](https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Learning-Environment-Examples.md#pyramids)\n",
    "- SchneeballZiel\n",
    "\n",
    "### üìö RL-Library:\n",
    "\n",
    "- [ML-Agenten](https://github.com/Unity-Technologies/ml-agents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qEhtaFh9i31S"
   },
   "source": [
    "Wir versuchen st√§ndig, unsere Tutorials zu verbessern, also **wenn Sie Probleme in diesem Notizbuch** finden, √∂ffnen Sie bitte [einen Fehler im GitHub Repo](https://github.com/huggingface/deep-rl-class/issues)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7f63r3Yi5vE"
   },
   "source": [
    "## Ziele dieses Notizbuchs üèÜ\n",
    "\n",
    "Am Ende des Notizbuchs werden Sie:\n",
    "\n",
    "- Verstehen, wie **ML-Agents**, die Umgebungsbibliothek, funktioniert.\n",
    "- In der Lage sein, **Agenten in Unity-Umgebungen** zu trainieren.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "viNzVbVaYvY3"
   },
   "source": [
    "## Dieses Notizbuch stammt aus dem Deep Reinforcement Learning Kurs.\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/deep-rl-course-illustration.jpg\" alt=\"Deep RL Course illustration\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6p5HnEefISCB"
   },
   "source": [
    "In diesem kostenlosen Kurs lernen Sie:\n",
    "\n",
    "- üìñ Deep Reinforcement Learning in **Theorie und Praxis** studieren.\n",
    "- üßë‚Äçüíª Lernen Sie, **ber√ºhmte Deep RL-Bibliotheken** wie Stable Baselines3, RL Baselines3 Zoo, CleanRL und Sample Factory 2.0 zu verwenden.\n",
    "- ü§ñ Trainieren Sie **Agenten in einzigartigen Umgebungen**.\n",
    "\n",
    "Und mehr, siehe üìö den Lehrplan üëâ https://huggingface.co/deep-rl-course/communication/publishing-schedule\n",
    "\n",
    "Vergessen Sie nicht, sich **<a href=\"http://eepurl.com/ic5ZUD\">f√ºr den Kurs anzumelden</a>** (wir sammeln Ihre E-Mail, um Ihnen **die Links zu senden, wenn die einzelnen Einheiten ver√∂ffentlicht werden, und Sie √ºber die Herausforderungen und Aktualisierungen zu informieren).**\n",
    "\n",
    "\n",
    "Der beste Weg, um in Kontakt zu bleiben, ist, unserem Discord-Server beizutreten, um sich mit der Community und mit uns auszutauschen üëâüèª https://discord.gg/ydHrjt3WP5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eine Einf√ºhrung in Unity ML-Agenten\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/thumbnail.png\" alt=\"thumbnail\"/>\n",
    "\n",
    "Eine der Herausforderungen beim Reinforcement Learning ist die **Erstellung von Umgebungen**. Zu unserem Gl√ºck k√∂nnen wir daf√ºr Spiel-Engines verwenden.\n",
    "Diese Engines, wie [Unity](https://unity.com/), [Godot](https://godotengine.org/) oder [Unreal Engine](https://www.unrealengine.com/), sind Programme, die f√ºr die Entwicklung von Videospielen entwickelt wurden. Sie sind perfekt geeignet\n",
    "f√ºr die Erstellung von Umgebungen: Sie bieten Physiksysteme, 2D/3D-Rendering und mehr.\n",
    "\n",
    "\n",
    "Eines dieser Programme, [Unity](https://unity.com/), hat das [Unity ML-Agents Toolkit](https://github.com/Unity-Technologies/ml-agents) entwickelt, ein auf der Spiel-Engine Unity basierendes Plugin, das es uns erm√∂glicht, **die Unity Game Engine als Environment Builder f√ºr die Ausbildung von Agenten** zu verwenden. In der ersten Bonuseinheit haben wir Huggy damit trainiert, einen Stock zu fangen!\n",
    "\n",
    "√úbersetzt mit DeepL.com (kostenlose Version)\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit5/example-envs.png\" alt=\"MLAgents environments\"/>\n",
    "<figcaption>Source: <a href=\"https://github.com/Unity-Technologies/ml-agents\">ML-Agents documentation</a></figcaption>\n",
    "</figure>\n",
    "\n",
    "Das Unity ML-Agents Toolkit bietet viele au√üergew√∂hnliche vorgefertigte Umgebungen, vom Fu√üballspielen √ºber das Laufenlernen bis hin zum Springen √ºber gro√üe Mauern.\n",
    "\n",
    "In dieser Einheit werden wir lernen, ML-Agents zu verwenden, aber **keine Sorge, wenn Sie nicht wissen, wie man die Unity Game Engine** verwendet: Sie m√ºssen sie nicht verwenden, um Ihre Agenten zu trainieren.\n",
    "\n",
    "Heute werden wir also zwei Agenten trainieren:\n",
    "- Der erste wird lernen, **Schneeb√§lle auf ein spawnendes Ziel zu schie√üen**.\n",
    "- Der zweite muss **einen Knopf dr√ºcken, um eine Pyramide zu spawnen, dann zur Pyramide navigieren, sie umsto√üen und sich zum goldenen Stein auf der Spitze bewegen**. Dazu muss er seine Umgebung erkunden, was mit einer Technik namens Neugierde geschieht.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/envs.png\" alt=\"Umgebungen\" />\n",
    "\n",
    "Nach dem Training **schieben Sie die trainierten Agenten zum Hugging Face Hub**, und Sie k√∂nnen sie **direkt in Ihrem Browser spielen sehen, ohne den Unity-Editor benutzen zu m√ºssen**.\n",
    "\n",
    "Diese Einheit wird euch **auf die n√§chste Herausforderung vorbereiten: KI gegen KI, wo Sie Agenten in Umgebungen mit mehreren Agenten trainieren und gegen die Agenten Ihrer Klassenkameraden antreten**.\n",
    "\n",
    "Klingt spannend? Dann lasst uns anfangen!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wie funktionieren die ML-Agenten von Unity?\n",
    "\n",
    "Bevor wir unseren Agenten trainieren, m√ºssen wir verstehen **was ML-Agenten sind und wie sie funktionieren**.\n",
    "\n",
    "## Was ist Unity ML-Agents?\n",
    "\n",
    "[Unity ML-Agents] (https://github.com/Unity-Technologies/ml-agents) ist ein Toolkit f√ºr die Spiel-Engine Unity, mit dem wir **Umgebungen mit Unity erstellen oder vorgefertigte Umgebungen f√ºr das Training unserer Agenten verwenden k√∂nnen**.\n",
    "\n",
    "Es wird von [Unity Technologies](https://unity.com/) entwickelt, den Entwicklern von Unity, einer der bekanntesten Game Engines, die von den Machern von Firewatch, Cuphead und Cities: Skylines.\n",
    "<figure>\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit5/firewatch.jpeg\" alt=\"Firewatch\"/>\n",
    "<figcaption>Firewatch was made with Unity</figcaption>\n",
    "</figure>\n",
    "\n",
    "## Die sechs Komponenten\n",
    "\n",
    "Mit Unity ML-Agents haben Sie sechs wesentliche Komponenten:\n",
    "<figure>\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit5/mlagents-1.png\" alt=\"MLAgents\"/>\n",
    "<figcaption>Source: <a href=\"https://unity-technologies.github.io/ml-agents/\">Unity ML-Agents Documentation</a> </figcaption>\n",
    "</figure>\n",
    "\n",
    "- Die erste ist die *Lernumgebung*, die **die Unity-Szene (die Umgebung) und die Umgebungselemente** (Spielfiguren) enth√§lt.\n",
    "- Die zweite ist die *Python Low-Level-API*, die **die Low-Level-Python-Schnittstelle zur Interaktion und Manipulation der Umgebung** enth√§lt. Es ist die API, die wir zum Starten des Trainings verwenden.\n",
    "- Dann gibt es noch den *Externen Kommunikator*, der **die Lernumgebung (die mit C# erstellt wurde) mit der Low-Level-Python-API (Python)** verbindet.\n",
    "- Die *Python-Trainer*: die **Verst√§rkungsalgorithmen, die mit PyTorch erstellt wurden (PPO, SAC...)**.\n",
    "- Der *Gym-Wrapper*: um die RL-Umgebung in einem Gym-Wrapper zu kapseln.\n",
    "- Der *PettingZoo-Wrapper*: PettingZoo ist die Multi-Agenten-Version des Gym-Wrappers.\n",
    "\n",
    "## Innerhalb der Lernkomponente\n",
    "\n",
    "Innerhalb der Lernkomponente haben wir **zwei wichtige Elemente**:\n",
    "\n",
    "- Das erste ist die *Agenten-Komponente*, der Akteur der Szene. Wir **trainieren den Agenten, indem wir seine Strategie** optimieren (die uns sagt, welche Aktion wir in jedem Zustand durchf√ºhren sollen). Die Strategie wird *Gehirn* genannt.\n",
    "- Schlie√ülich gibt es noch die *Academy*. Diese Komponente **orchestriert die Agenten und ihre Entscheidungsprozesse**. Stellen Sie sich diese Akademie als einen Lehrer vor, der Python-API-Anfragen bearbeitet.\n",
    "\n",
    "Um ihre Rolle besser zu verstehen, sollten wir uns an den RL-Prozess erinnern. Dieser kann als eine Schleife modelliert werden, die wie folgt funktioniert:\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/RL_process.jpg\" alt=\"The RL process\" width=\"100%\">\n",
    "<figcaption>The RL Process: a loop of state, action, reward and next state</figcaption>\n",
    "<figcaption>Source: <a href=\"http://incompleteideas.net/book/RLbook2020.pdf\">Reinforcement Learning: An Introduction, Richard Sutton and Andrew G. Barto</a></figcaption>\n",
    "</figure>\n",
    "\n",
    "Stellen wir uns nun einen Agenten vor, der lernt, ein Plattformspiel zu spielen. Der RL-Prozess sieht wie folgt aus:\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/RL_process_game.jpg\" alt=\"Der RL-Prozess\" width=\"100%\">\n",
    "\n",
    "- Unser Agent erh√§lt den **Zustand \\\\(S_0\\\\)** von der **Umgebung** - wir erhalten den ersten Frame unseres Spiels (Umwelt).\n",
    "- Basierend auf diesem **Zustand \\\\(S_0\\\\),** f√ºhrt der Agent eine **Aktion \\\\(A_0\\\\)** aus - unser Agent wird sich nach rechts bewegen.\n",
    "- Die Umgebung geht in einen **neuen** **Zustand \\\\(S_1\\\\)** √ºber - neues Bild.\n",
    "- Die Umgebung gibt dem Agenten eine **Belohnung \\\\(R_1\\\\)** - wir sind nicht tot *(Positive Belohnung +1)*.\n",
    "\n",
    "Diese RL-Schleife gibt eine Sequenz von **Zustand, Aktion, Belohnung und n√§chstem Zustand aus.** Das Ziel des Agenten ist die **Maximierung der erwarteten kumulativen Belohnung**.\n",
    "\n",
    "Die Akademie wird diejenige sein, die **den Auftrag an unsere Agenten sendet und sicherstellt, dass die Agenten synchronisiert sind**:\n",
    "\n",
    "- Beobachtungen sammeln\n",
    "- W√§hlen Sie Ihre Aktion anhand Ihrer Richtlinie aus\n",
    "- F√ºhren Sie die Aktion durch\n",
    "- Zur√ºcksetzen, wenn Sie den maximalen Schritt erreicht haben oder wenn Sie fertig sind.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit5/academy.png\" alt=\"Die MLAgents Akademie\" width=\"100%\">\n",
    "\n",
    "\n",
    "Jetzt, da wir verstehen, wie ML-Agents funktioniert, **k√∂nnen wir unsere Agenten ausbilden**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Die SnowballTarget-Umgebung\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/snowballtarget.gif\" alt=\"SnowballTarget\"/>\n",
    "\n",
    "SnowballTarget ist eine Umgebung, die wir bei Hugging Face mit Assets von [Kay Lousberg] (https://kaylousberg.com/) erstellt haben. Wir haben einen optionalen Abschnitt am Ende dieser Einheit **wenn Sie lernen wollen, Unity zu benutzen und Ihre eigenen Umgebungen zu erstellen**.\n",
    "\n",
    "## Das Ziel des Agenten\n",
    "\n",
    "Der erste Agent, den du trainieren wirst, hei√üt Julien der B√§r üêª. Julien ist darauf trainiert, **Ziele mit Schneeb√§llen zu treffen**.\n",
    "\n",
    "Das Ziel in dieser Umgebung ist, dass Julien **so viele Ziele wie m√∂glich in der begrenzten Zeit** (1000 Zeitschritte) trifft. Dazu muss er sich **richtig in Bezug auf das Ziel positionieren und schie√üen**.\n",
    "\n",
    "Um \"Schneeball-Spamming\" zu vermeiden (d. h. bei jedem Zeitschritt einen Schneeball zu schie√üen), verf√ºgt Julien √ºber ein \"Cool-Off\"-System** (er muss nach einem Schuss 0,5 Sekunden warten, bevor er wieder schie√üen kann).\n",
    "\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/cooloffsystem.gif\" alt=\"Cool Off System\"/>\n",
    "<figcaption>The agent needs to wait 0.5s before being able to shoot a snowball again</figcaption>\n",
    "</figure>\n",
    "\n",
    "## Die Reward-Funktion und das Reward-Engineering-Problem\n",
    "\n",
    "\n",
    "Die Belohnungsfunktion ist einfach. **Die Umgebung gibt jedes Mal eine Belohnung von +1, wenn der Schneeball des Agenten ein Ziel trifft**. Da es das Ziel des Agenten ist, die erwartete kumulative Belohnung zu maximieren, **wird er versuchen, so viele Ziele wie m√∂glich zu treffen**.\n",
    "\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/snowballtarget_reward.png\" alt=\"Belohnungssystem\"/>\n",
    "\n",
    "\n",
    "Wir k√∂nnten eine komplexere Belohnungsfunktion haben (z. B. mit einer Strafe, die den Agenten dazu bringt, schneller zu fahren). Aber wenn man eine Umgebung entwirft, muss man das *Belohnungsentwicklungsproblem* vermeiden, d. h. eine zu komplexe Belohnungsfunktion, die den Agenten dazu zwingt, sich so zu verhalten, wie man es m√∂chte.\n",
    "\n",
    "Und warum? Weil Sie dadurch **interessante Strategien verpassen k√∂nnten, die der Agent mit einer einfacheren Belohnungsfunktion finden w√ºrde**.\n",
    "\n",
    "\n",
    "In Bezug auf den Code sieht das folgenderma√üen aus:\n",
    "\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/snowballtarget-reward-code.png\" alt=\"Reward\"/>\n",
    "\n",
    "\n",
    "\n",
    "## Der Beobachtungsraum\n",
    "\n",
    "Was die Beobachtung betrifft, so verwenden wir nicht das normale Sehverm√∂gen (Rahmen), sondern **wir verwenden Raycasts**.\n",
    "\n",
    "\n",
    "Stellen Sie sich Raycasts als Laser vor, die erkennen, wenn sie ein Objekt durchdringen.\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit5/raycasts.png\" alt=\"Raycasts\"/>\n",
    "<figcaption>Source: <a href=\"https://github.com/Unity-Technologies/ml-agents\">ML-Agents documentation</a></figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "In dieser Umgebung hat unser Agent mehrere S√§tze von Raycasts:\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/snowball_target_raycasts.png\" alt=\"Raycasts\"/>\n",
    "\n",
    "\n",
    "Zus√§tzlich zu den Raycasts erh√§lt der Agent eine \"Kann ich schie√üen\"-Bool als Beobachtung.\n",
    "\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/snowballtarget-obs-code.png\" alt=\"Obs\"/>\n",
    "\n",
    "\n",
    "## Der Aktionsraum\n",
    "\n",
    "\n",
    "Der Aktionsraum ist diskret:\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/snowballtarget_action_space.png\" alt=\"Action Space\"/>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Die Pyramidenumgebung\n",
    "\n",
    "\n",
    "Das Ziel in dieser Umgebung ist es, unseren Agenten darauf zu trainieren, **den goldenen Stein auf der Spitze der Pyramide zu holen. Dazu muss er einen Knopf dr√ºcken, um eine Pyramide zu erzeugen, zur Pyramide navigieren, sie umsto√üen und sich zum Goldstein auf der Spitze bewegen**.\n",
    "\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/pyramids.png\" alt=\"Pyramiden Umgebung\"/>\n",
    "\n",
    "\n",
    "\n",
    "## Die Belohnungsfunktion\n",
    "\n",
    "\n",
    "Die Belohnungsfunktion ist:\n",
    "\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/pyramids-reward.png\" alt=\"Pyramids Environment\"/>\n",
    "\n",
    "\n",
    "In Bezug auf den Code sieht es so aus\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/pyramids-reward-code.png\" alt=\"Pyramiden-Belohnung\"/>\n",
    "\n",
    "\n",
    "Um diesen neuen Agenten zu trainieren, der diesen Knopf und dann die zu zerst√∂rende Pyramide sucht, verwenden wir eine Kombination aus zwei Arten von Belohnungen:\n",
    "\n",
    "\n",
    "- Die *extrinsische* Belohnung, die von der Umgebung gegeben wird (siehe Abbildung oben).\n",
    "\n",
    "- Aber auch eine *intrinsische* Belohnung namens **Neugier**. Diese zweite Belohnung wird **unseren Agenten dazu anregen, neugierig zu sein, oder anders ausgedr√ºckt, seine Umgebung besser zu erkunden**.\n",
    "\n",
    "\n",
    "Wenn Sie mehr √ºber Neugierde wissen wollen, erkl√§rt Ihnen der n√§chste Abschnitt (optional) die Grundlagen.\n",
    "\n",
    "\n",
    "## Der Beobachtungsraum\n",
    "\n",
    "\n",
    "F√ºr die Beobachtung **benutzen wir 148 Raycasts, die jeweils Objekte erkennen k√∂nnen** (Schalter, Ziegelsteine, goldene Ziegelsteine und W√§nde.)\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/pyramids_raycasts.png\"/>\n",
    "\n",
    "Wir verwenden auch eine **Boolesche Variable, die den Zustand des Schalters** angibt (haben wir den Schalter ein- oder ausgeschaltet, um die Pyramide zu erzeugen) und einen Vektor, der **die Geschwindigkeit des Agenten** enth√§lt.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/pyramids-obs-code.png\" alt=\"Pyramids obs code\"/>\n",
    "\n",
    "\n",
    "## Der Aktionsraum\n",
    "\n",
    "Der Aktionsraum ist **diskret** mit vier m√∂glichen Aktionen:\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/pyramids-action.png\" alt=\"Pyramids Environment\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-mo_6rXIjRi"
   },
   "source": [
    "## Voraussetzungen üèóÔ∏è\n",
    "Bevor Sie sich mit dem Notebook besch√§ftigen, m√ºssen Sie:\n",
    "\n",
    "üî≤ üìö **Lesen Sie [was ML-Agenten sind und wie sie funktionieren, indem Sie Unit 5 lesen](https://huggingface.co/deep-rl-course/unit5/introduction)** ü§ó"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xYO1uD5Ujgdh"
   },
   "source": [
    "# Lasst uns unsere Agenten trainieren üöÄ\n",
    "\n",
    "**Um dieses Hands-on f√ºr den Zertifizierungsprozess zu validieren, m√ºssen Sie nur Ihre trainierten Modelle an den Hub senden**. Es m√ºssen keine Ergebnisse erzielt werden, um dies zu validieren. Aber wenn Sie sch√∂ne Ergebnisse erhalten m√∂chten, k√∂nnen Sie versuchen, diese zu erreichen:\n",
    "\n",
    "- F√ºr `Pyramiden` : Mittlere Belohnung = 1.75\n",
    "- F√ºr `SnowballTarget` : Mittlere Belohnung = 15 oder 30 getroffene Ziele in einer Episode.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "an3ByrXYQ4iK"
   },
   "source": [
    "## Klonen Sie das Repository und installieren Sie die Abh√§ngigkeiten üîΩ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6WNoL04M7rTa"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Clone the repository\n",
    "!git clone --depth 1 https://github.com/Unity-Technologies/ml-agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d8wmVcMk7xKo"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Go inside the repository and install the package\n",
    "%cd ml-agents\n",
    "!pip3 install -e ./ml-agents-envs\n",
    "!pip3 install -e ./ml-agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5_7Ptd_kEcG"
   },
   "source": [
    "## SnowballTarget ‚õÑ\n",
    "\n",
    "Wenn Sie eine Auffrischung ben√∂tigen, wie diese Umgebungen funktionieren, lesen Sie diesen Abschnitt üëâ\n",
    "https://huggingface.co/deep-rl-course/unit5/snowball-target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HRY5ufKUKfhI"
   },
   "source": [
    "### Laden Sie die Umgebungs-Zip-Datei herunter und verschieben Sie sie in `./training-envs-executables/linux/`\n",
    "- Unsere ausf√ºhrbare Umgebungsdatei befindet sich in einer Zip-Datei.\n",
    "- Wir m√ºssen sie herunterladen und in `./training-envs-executables/linux/` ablegen.\n",
    "- Wir verwenden eine ausf√ºhrbare Datei f√ºr Linux, weil wir colab verwenden und das Betriebssystem der colab-Maschinen Ubuntu (Linux) ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C9Ls6_6eOKiA"
   },
   "outputs": [],
   "source": [
    "# Here, we create training-envs-executables and linux\n",
    "!mkdir ./training-envs-executables\n",
    "!mkdir ./training-envs-executables/linux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekSh8LWawkB5"
   },
   "source": [
    "Wir haben die Datei SnowballTarget.zip von https://github.com/huggingface/Snowball-Target mit `wget` heruntergeladen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6LosWO50wa77"
   },
   "outputs": [],
   "source": [
    "!wget \"https://github.com/huggingface/Snowball-Target/raw/main/SnowballTarget.zip\" -O ./training-envs-executables/linux/SnowballTarget.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_LLVaEEK3ayi"
   },
   "source": [
    "Wir entpacken die Datei executable.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8FPx0an9IAwO"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!unzip -d ./training-envs-executables/linux/ ./training-envs-executables/linux/SnowballTarget.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nyumV5XfPKzu"
   },
   "source": [
    "Stellen Sie sicher, dass Ihre Datei zug√§nglich ist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EdFsLJ11JvQf"
   },
   "outputs": [],
   "source": [
    "!chmod -R 755 ./training-envs-executables/linux/SnowballTarget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NAuEq32Mwvtz"
   },
   "source": [
    "### Definieren Sie die SnowballTarget-Konfigurationsdatei\n",
    "- In ML-Agents definieren Sie die **Trainings-Hyperparameter in config.yaml-Dateien**.\n",
    "\n",
    "Es gibt eine Vielzahl von Hyperparametern. Um sie besser kennenzulernen, sollten Sie jede Erkl√§rung in der [Dokumentation](https://github.com/Unity-Technologies/ml-agents/blob/release_20_docs/docs/Training-Configuration-File.md) nachlesen.\n",
    "\n",
    "\n",
    "Sie m√ºssen also eine Konfigurationsdatei `SnowballTarget.yaml` in ./content/ml-agents/config/ppo/ erstellen.\n",
    "\n",
    "Wir geben Ihnen hier eine erste Version dieser Konfigurationsdatei (zum Kopieren und Einf√ºgen in Ihre `SnowballTarget.yaml-Datei`), **aber Sie sollten sie √§ndern**.\n",
    "\n",
    "```yaml\n",
    "behaviors:\n",
    "  SnowballTarget:\n",
    "    trainer_type: ppo\n",
    "    summary_freq: 10000\n",
    "    keep_checkpoints: 10\n",
    "    checkpoint_interval: 50000\n",
    "    max_steps: 200000\n",
    "    time_horizon: 64\n",
    "    threaded: true\n",
    "    hyperparameters:\n",
    "      learning_rate: 0.0003\n",
    "      learning_rate_schedule: linear\n",
    "      batch_size: 128\n",
    "      buffer_size: 2048\n",
    "      beta: 0.005\n",
    "      epsilon: 0.2\n",
    "      lambd: 0.95\n",
    "      num_epoch: 3\n",
    "    network_settings:\n",
    "      normalize: false\n",
    "      hidden_units: 256\n",
    "      num_layers: 2\n",
    "      vis_encode_type: simple\n",
    "    reward_signals:\n",
    "      extrinsic:\n",
    "        gamma: 0.99\n",
    "        strength: 1.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4U3sRH4N4h_l"
   },
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/snowballfight_config1.png\" alt=\"Config SnowballTarget\"/>\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/snowballfight_config2.png\" alt=\"Config SnowballTarget\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJJdo_5AyoGo"
   },
   "source": [
    "Zum Experimentieren sollten Sie auch versuchen, einige andere Hyperparameter zu √§ndern. Unity bietet eine sehr [gute Dokumentation, in der jeder einzelne Parameter erkl√§rt wird] (https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Training-Configuration-File.md).\n",
    "\n",
    "Nun, da du die Konfigurationsdatei erstellt hast und verstehst, was die meisten Hyperparameter tun, sind wir bereit, unseren Agenten zu trainieren üî•."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9fI555bO12v"
   },
   "source": [
    "### Ausbildung des Agenten\n",
    "\n",
    "Um unseren Agenten zu trainieren, m√ºssen wir nur **mlagents-learn starten und die ausf√ºhrbare Datei ausw√§hlen, die die Umgebung enth√§lt**.\n",
    "\n",
    "Wir definieren vier Parameter:\n",
    "\n",
    "1. mlagents-learn <config>\": der Pfad, in dem sich die Konfigurationsdatei der Hyperparameter befindet.\n",
    "2. `--env`: wo sich die ausf√ºhrbare Umgebung befindet.\n",
    "3. `--run_id`: der Name, den Sie Ihrer Trainingslauf-ID geben wollen.\n",
    "4. `--no-graphics`: um die Visualisierung w√§hrend des Trainings nicht zu starten.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/mlagentslearn.png\" alt=\"MlAgents learn\"/>\n",
    "\n",
    "Trainieren Sie das Modell und verwenden Sie das `--resume` Flag, um das Training im Falle einer Unterbrechung fortzusetzen.\n",
    "\n",
    "> Es wird beim ersten Mal fehlschlagen, wenn Sie `--resume` verwenden. Versuchen Sie, den Block erneut auszuf√ºhren, um den Fehler zu umgehen.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lN32oWF8zPjs"
   },
   "source": [
    "Das Training dauert je nach Konfiguration zwischen 10 und 35 Minuten, ein ‚òïÔ∏èyou ist es wert ü§ó."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bS-Yh1UdHfzy"
   },
   "outputs": [],
   "source": [
    "!mlagents-learn ./config/ppo/SnowballTarget.yaml --env=./training-envs-executables/linux/SnowballTarget/SnowballTarget --run-id=\"SnowballTarget1\" --no-graphics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Vue94AzPy1t"
   },
   "source": [
    "### Schieben Sie den Agenten zum ü§ó Hub\n",
    "\n",
    "- Nun, da wir unseren Agenten trainiert haben, sind wir **bereit, ihn in den Hub zu pushen, um ihn in deinem Browser abspielen zu k√∂nnenüî•.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "izT6FpgNzZ6R"
   },
   "source": [
    "Um Ihr Modell mit der Gemeinschaft teilen zu k√∂nnen, sind drei weitere Schritte erforderlich:\n",
    "\n",
    "1Ô∏è‚É£ (Falls noch nicht geschehen) Erstellen Sie ein Konto f√ºr HF ‚û° https://huggingface.co/join\n",
    "\n",
    "2Ô∏è‚É£ Melde dich an und speichere dann dein Authentifizierungs-Token von der Hugging Face Website.\n",
    "- Erstellen Sie ein neues Token (https://huggingface.co/settings/tokens) **mit Schreibrolle**\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/create-token.jpg\" alt=\"HF-Token erstellen\">\n",
    "\n",
    "- Kopieren Sie das Token\n",
    "- F√ºhren Sie die Zelle unten aus und f√ºgen Sie das Token ein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rKt2vsYoK56o"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSU9qD9_6dem"
   },
   "source": [
    "Wenn Sie kein Google Colab oder ein Jupyter Notebook verwenden m√∂chten, m√ºssen Sie stattdessen diesen Befehl verwenden: `huggingface-cli login`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KK4fPfnczunT"
   },
   "source": [
    "Dann m√ºssen wir einfach `mlagents-push-to-hf` ausf√ºhren.\n",
    "\n",
    "Und wir definieren 4 Parameter:\n",
    "\n",
    "1. Lauf-id\": der Name des Trainingslaufs (id).\n",
    "2. `--local-dir`: wo der Agent gespeichert wurde, es ist results/<run_id name>, also in meinem Fall results/First Training.\n",
    "3. `--repo-id`: der Name des Hugging Face Repos, das du erstellen oder aktualisieren willst. Es ist immer <Ihr Hugging-Face-Benutzername>/<Der Repo-Name>\n",
    "Wenn das Repo nicht existiert, wird es automatisch erstellt**.\n",
    "4. `--commit-message`: Da HF-Repos Git-Repos sind, m√ºssen Sie eine Commit-Message definieren.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/mlagentspushtohub.png\" alt=\"Push to Hub\"/>\n",
    "\n",
    "Zum Beispiel:\n",
    "\n",
    "`!mlagents-push-to-hf --run-id=\"SnowballTarget1\" --local-dir=\"./results/SnowballTarget1\" --repo-id=\"ThomasSimonini/ppo-SnowballTarget\" --commit-message=\"First Push\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kAFzVB7OYj_H"
   },
   "outputs": [],
   "source": [
    "!mlagents-push-to-hf --run-id=\"SnowballTarget1\" --local-dir=\"./results/SnowballTarget1\" --repo-id=\"ThomasSimonini/ppo-SnowballTarget\" --commit-message=\"First Push\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dGEFAIboLVc6"
   },
   "outputs": [],
   "source": [
    "!mlagents-push-to-hf  --run-id= # Add your run id  --local-dir= # Your local dir  --repo-id= # Your repo id  --commit-message= # Your commit message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yborB0850FTM"
   },
   "source": [
    "Andernfalls, wenn alles funktioniert hat, sollten Sie am Ende des Prozesses folgendes Ergebnis haben (allerdings mit einer anderen URL üòÜ):\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "Ihr Modell wird in den Hub √ºbertragen. Du kannst dein Modell hier sehen: https://huggingface.co/ThomasSimonini/ppo-SnowballTarget\n",
    "```\n",
    "\n",
    "Es ist der Link zu Ihrem Modell, es enth√§lt eine Modellkarte, die erkl√§rt, wie man es benutzt, Ihr Tensorboard und Ihre Konfigurationsdatei. **Das Tolle ist, dass es sich um ein Git-Repository handelt, d.h. Sie k√∂nnen verschiedene Commits haben, Ihr Repository mit einem neuen Push aktualisieren usw.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Uaon2cg0NrL"
   },
   "source": [
    "Aber jetzt kommt das Beste: **Ihren Agenten online visualisieren zu k√∂nnen üëÄ.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VMc4oOsE0QiZ"
   },
   "source": [
    "### Beobachte deinen Agenten beim Spielen üëÄ\n",
    "\n",
    "Dieser Schritt ist ganz einfach:\n",
    "\n",
    "1. Merken Sie sich Ihre Repo-ID\n",
    "\n",
    "2. Gehen Sie hier: https://huggingface.co/spaces/ThomasSimonini/ML-Agents-SnowballTarget\n",
    "\n",
    "3. Starten Sie das Spiel und schalten Sie es in den Vollbildmodus, indem Sie auf die Schaltfl√§che unten rechts klicken\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/snowballtarget_load.png\" alt=\"Schneeballtarget laden\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Djs8c5rR0Z8a"
   },
   "source": [
    "1. In Schritt 1 w√§hlen Sie Ihr Modell-Repository, das die Modell-ID ist (in meinem Fall ThomasSimonini/ppo-SnowballTarget).\n",
    "\n",
    "2. In Schritt 2 **w√§hlen Sie das Modell aus, das Sie wiedergeben m√∂chten**:\n",
    "  - Ich habe mehrere, da wir alle 500000 Zeitschritte ein Modell gespeichert haben.\n",
    "  - Aber wenn ich das aktuellste Modell m√∂chte, w√§hle ich \"SnowballTarget.onnx\".\n",
    "\n",
    "üëâ Das Sch√∂ne ist, **dass man verschiedene Modelle ausprobieren kann, um die Verbesserung des Agenten zu sehen**.\n",
    "\n",
    "Und z√∂gere nicht, die beste Punktzahl deines Agenten auf Discord im #rl-i-made-this Kanal zu teilen üî•.\n",
    "\n",
    "Versuchen wir nun eine schwierigere Umgebung namens Pyramiden..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVMwRi4y_tmx"
   },
   "source": [
    "## Pyramiden üèÜ\n",
    "\n",
    "### Laden Sie die Umgebungs-Zip-Datei herunter und verschieben Sie sie in `./training-envs-executables/linux/`\n",
    "- Unsere ausf√ºhrbare Umgebungsdatei befindet sich in einer Zip-Datei.\n",
    "- Wir m√ºssen sie herunterladen und in `./training-envs-executables/linux/` ablegen.\n",
    "- Wir verwenden eine ausf√ºhrbare Datei f√ºr Linux, weil wir colab verwenden und das Betriebssystem der colab-Maschinen Ubuntu (Linux) ist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NyqYYkLyAVMK"
   },
   "source": [
    "Laden Sie die Datei Pyramids.zip von https://drive.google.com/uc?export=download&id=1UiFNdKlsH0NTu32xV-giYUEVKV4-vc7H mit `wget` herunter. Sehen Sie sich die vollst√§ndige L√∂sung zum Herunterladen gro√üer Dateien von GDrive [hier] an (https://bcrf.biochem.wisc.edu/2021/02/05/download-google-drive-files-using-wget/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AxojCsSVAVMP"
   },
   "outputs": [],
   "source": [
    "!wget \"https://huggingface.co/spaces/unity/ML-Agents-Pyramids/resolve/main/Pyramids.zip\" -O ./training-envs-executables/linux/Pyramids.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bfs6CTJ1AVMP"
   },
   "source": [
    "**ODER** Laden Sie die Datei direkt auf den lokalen Rechner herunter und ziehen Sie sie dann per Drag-and-Drop in das Verzeichnis `./training-envs-executables/linux`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iWUUcs0_794U"
   },
   "source": [
    "Unzip it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i2E3K4V2AVMP"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!unzip -d ./training-envs-executables/linux/ ./training-envs-executables/linux/Pyramids.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KmKYBgHTAVMP"
   },
   "source": [
    "Stellen Sie sicher, dass Ihre Datei zug√§nglich ist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Im-nwvLPAVMP"
   },
   "outputs": [],
   "source": [
    "!chmod -R 755 ./training-envs-executables/linux/Pyramids/Pyramids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fqceIATXAgih"
   },
   "source": [
    "### √Ñndern Sie die PyramidsRND-Konfigurationsdatei\n",
    "- Im Gegensatz zur ersten Umgebung, die eine benutzerdefinierte Umgebung war, wurde **Pyramids vom Unity-Team** erstellt.\n",
    "- Die PyramidsRND-Konfigurationsdatei existiert also bereits und befindet sich in ./content/ml-agents/config/ppo/PyramidsRND.yaml\n",
    "- Sie fragen sich vielleicht, warum \"RND\" in PyramidsRND. RND steht f√ºr *random network distillation* es ist ein Weg, um Neugierbelohnungen zu generieren. Wenn Sie mehr dar√ºber wissen wollen, haben wir einen Artikel geschrieben, der diese Technik erkl√§rt: https://medium.com/data-from-the-trenches/curiosity-driven-learning-through-random-network-distillation-488ffd8e5938\n",
    "\n",
    "F√ºr dieses Training werden wir eine Sache √§ndern:\n",
    "- Der Hyperparameter f√ºr die Gesamtzahl der Trainingsschritte ist zu hoch, da wir den Benchmark (mittlere Belohnung = 1,75) in nur 1M Trainingsschritten erreichen k√∂nnen.\n",
    "üëâ Dazu gehen wir zu config/ppo/PyramidsRND.yaml,**und √§ndern diese auf max_steps auf 1000000.**\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/pyramids-config.png\" alt=\"Pyramids config\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RI-5aPL7BWVk"
   },
   "source": [
    "Als Experiment sollten Sie auch versuchen, einige andere Hyperparameter zu √§ndern, Unity bietet eine sehr [gute Dokumentation, die jeden von ihnen hier erkl√§rt] (https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Training-Configuration-File.md).\n",
    "\n",
    "Wir sind jetzt bereit, unseren Agenten zu trainieren üî•."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s5hr1rvIBdZH"
   },
   "source": [
    "### Ausbildung des Agenten\n",
    "\n",
    "Das Training dauert je nach Rechner 30 bis 45 Minuten, gehen Sie auf ‚òïÔ∏èyou und verdienen Sie es ü§ó."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fXi4-IaHBhqD"
   },
   "outputs": [],
   "source": [
    "!mlagents-learn ./config/ppo/PyramidsRND.yaml --env=./training-envs-executables/linux/Pyramids/Pyramids --run-id=\"Pyramids Training\" --no-graphics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "txonKxuSByut"
   },
   "source": [
    "### Schieben Sie den Agenten zum ü§ó Hub\n",
    "\n",
    "- Nun, da wir unseren Agenten trainiert haben, sind wir **bereit, ihn in den Hub zu pushen, um ihn in deinem Browser abspielen zu k√∂nnenüî•.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yiEQbv7rB4mU"
   },
   "outputs": [],
   "source": [
    "!mlagents-push-to-hf  --run-id= # Add your run id  --local-dir= # Your local dir  --repo-id= # Your repo id  --commit-message= # Your commit message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7aZfgxo-CDeQ"
   },
   "source": [
    "### Beobachte deinen Agenten beim Spielen üëÄ\n",
    "\n",
    "üëâ https://huggingface.co/spaces/unity/ML-Agents-Pyramids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGG_oq2n0wjB"
   },
   "source": [
    "### üéÅ Bonus: Warum nicht in einer anderen Umgebung trainieren?\n",
    "Da Sie nun wissen, wie Sie einen Agenten mit MLAgents trainieren k√∂nnen, **warum nicht eine andere Umgebung ausprobieren?**\n",
    "\n",
    "MLAgents bietet 17 verschiedene Umgebungen an, und wir sind dabei, einige eigene zu entwickeln. Der beste Weg, um zu lernen, ist, Dinge selbst auszuprobieren, Spa√ü zu haben.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KSAkJxSr0z6-"
   },
   "source": [
    "![cover](https://miro.medium.com/max/1400/0*xERdThTRRM2k_U9f.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YiyF4FX-04JB"
   },
   "source": [
    "Die vollst√§ndige Liste der offiziellen Unity-Umgebungen finden Sie hier üëâ https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Learning-Environment-Examples.md\n",
    "\n",
    "F√ºr die Demos zur Visualisierung deines Agenten üëâ https://huggingface.co/unity\n",
    "\n",
    "F√ºr jetzt haben wir integriert:\n",
    "- [Worm](https://huggingface.co/spaces/unity/ML-Agents-Worm) Demo, in der man einem **Wurm das Krabbeln** beibringt.\n",
    "- [Walker](https://huggingface.co/spaces/unity/ML-Agents-Walker) Demo, in der man einem Agenten beibringt, **zu einem Ziel zu laufen**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PI6dPWmh064H"
   },
   "source": [
    "Das war's f√ºr heute. Herzlichen Gl√ºckwunsch zur Fertigstellung dieses Tutorials!\n",
    "\n",
    "Der beste Weg zu lernen ist, zu √ºben und Dinge auszuprobieren. Warum nicht eine andere Umgebung ausprobieren? ML-Agents hat 17 verschiedene Umgebungen, aber Sie k√∂nnen auch Ihre eigene erstellen? Schauen Sie in die Dokumentation und haben Sie Spa√ü!\n",
    "\n",
    "Wir sehen uns in Unit 6 üî•,\n",
    "\n",
    "## Keep Learning, Stay awesome ü§ó"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
