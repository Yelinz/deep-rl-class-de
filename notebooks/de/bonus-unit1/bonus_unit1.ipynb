{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/notebooks/bonus-unit1/bonus_unit1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"In Colab √∂ffnen\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2D3NL_e4crQv"
   },
   "source": [
    "# Bonuseinheit 1: Huggy, der Hund üê∂, soll einen Stock apportieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FMYrDriDujzX"
   },
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit2/thumbnail.png\" alt=\"Bonus Unit 1Thumbnail\">\n",
    "\n",
    "In diesem Heft festigen wir das in der ersten Einheit Gelernte, indem wir **Huggy, dem Hund, beibringen, das St√∂ckchen zu holen und dann direkt in deinem Browser damit zu spielen**\n",
    "\n",
    "‚¨áÔ∏è Hier ist ein Beispiel f√ºr das, was **am Ende der Einheit erreicht wird** ‚¨áÔ∏è (‚ñ∂ starten, um zu sehen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PnVhs1yYNyUF"
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<video controls autoplay><source src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit-bonus1/huggy.mp4\" type=\"video/mp4\"></video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x7oR6R-ZIbeS"
   },
   "source": [
    "### Die Umwelt üéÆ\n",
    "\n",
    "- Huggy the Dog, eine von [Thomas Simonini](https://twitter.com/ThomasSimonini) geschaffene Umgebung, basierend auf [Puppo The Corgi](https://blog.unity.com/technology/puppo-the-corgi-cuteness-overload-with-the-unity-ml-agents-toolkit)\n",
    "\n",
    "### Die verwendete Bibliothek üìö\n",
    "\n",
    "- [MLAgents](https://github.com/Unity-Technologies/ml-agents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60yACvZwO0Cy"
   },
   "source": [
    "Wir versuchen st√§ndig, unsere Anleitungen zu verbessern. **Wenn Sie also Probleme in diesem Notizbuch** finden, √∂ffnen Sie bitte [ein Problem im Github Repo](https://github.com/huggingface/deep-rl-class/issues)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oks-ETYdO2Dc"
   },
   "source": [
    "## Ziele dieses Notizbuchs üèÜ\n",
    "\n",
    "Am Ende des Notizbuchs werden Sie:\n",
    "\n",
    "- Den **Zustandsraum, den Aktionsraum und die Belohnungsfunktion, die f√ºr das Training von Huggy** verwendet wurden, verstehen.\n",
    "- **Deinen eigenen Huggy** darauf trainieren, den Stock zu holen.\n",
    "- in der Lage sein, **mit deinem trainierten Huggy direkt in deinem Browser zu spielen**.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mUlVrqnBv2o1"
   },
   "source": [
    "## Dieses Notebook stammt aus dem Deep Reinforcement Learning Kurs.\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/deep-rl-course-illustration.jpg\" alt=\"Deep RL Course illustration\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pAMjaQpHwB_s"
   },
   "source": [
    "In diesem kostenlosen Kurs lernen Sie:\n",
    "\n",
    "- üìñ Deep Reinforcement Learning in **Theorie und Praxis** studieren.\n",
    "- üßë‚Äçüíª Lernen Sie, **ber√ºhmte Deep RL-Bibliotheken** wie Stable Baselines3, RL Baselines3 Zoo, CleanRL und Sample Factory 2.0 zu verwenden.\n",
    "- ü§ñ Trainieren Sie **Agenten in einzigartigen Umgebungen**.\n",
    "\n",
    "Und mehr, siehe üìö den Lehrplan üëâ https://simoninithomas.github.io/deep-rl-course\n",
    "\n",
    "Vergessen Sie nicht, sich **<a href=\"http://eepurl.com/ic5ZUD\">f√ºr den Kurs anzumelden</a>** (wir sammeln Ihre E-Mail, um Ihnen **die Links zu senden, wenn die einzelnen Einheiten ver√∂ffentlicht werden, und Sie √ºber die Herausforderungen und Aktualisierungen zu informieren).**\n",
    "\n",
    "\n",
    "Der beste Weg, um in Kontakt zu bleiben, ist, unserem Discord-Server beizutreten, um sich mit der Community und mit uns auszutauschen üëâüèª https://discord.gg/ydHrjt3WP5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6r7Hl0uywFSO"
   },
   "source": [
    "## Voraussetzungen üèóÔ∏è\n",
    "\n",
    "Bevor Sie sich mit dem Notebook besch√§ftigen, m√ºssen Sie:\n",
    "\n",
    "üî≤ üìö **ein Verst√§ndnis f√ºr die Grundlagen des Verst√§rkungslernens** (MC, TD, Belohnungshypothese...) entwickeln, indem Sie Einheit 1 bearbeiten\n",
    "\n",
    "üî≤ üìö **Lesen Sie die Einf√ºhrung zu Huggy**, indem Sie die Bonuseinheit 1 bearbeiten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DssdIjk_8vZE"
   },
   "source": [
    "## Die GPU einstellen üí™.\n",
    "- Um **das Training des Agenten zu beschleunigen, werden wir einen Grafikprozessor** verwenden. Gehen Sie dazu auf \"Runtime > Change Runtime type\".\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step1.jpg\" alt=\"GPU Schritt 1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sTfCXHy68xBv"
   },
   "source": [
    "- Hardware-Beschleuniger > GPU\".\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step2.jpg\" alt=\"GPU Schritt 2\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "an3ByrXYQ4iK"
   },
   "source": [
    "## Klonen Sie das Repository und installieren Sie die Abh√§ngigkeiten üîΩ.\n",
    "\n",
    "- Wir m√ºssen das Repository klonen, das **ML-Agents.** enth√§lt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6WNoL04M7rTa"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Clone the repository (can take 3min)\n",
    "!git clone --depth 1 https://github.com/Unity-Technologies/ml-agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d8wmVcMk7xKo"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Go inside the repository and install the package (can take 3min)\n",
    "%cd ml-agents\n",
    "!pip3 install -e ./ml-agents-envs\n",
    "!pip3 install -e ./ml-agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HRY5ufKUKfhI"
   },
   "source": [
    "## Laden Sie die Umgebungs-Zip-Datei herunter und verschieben Sie sie nach `./trained-envs-executables/linux/`.\n",
    "\n",
    "- Unsere ausf√ºhrbare Umgebungsdatei befindet sich in einer Zip-Datei.\n",
    "- Wir m√ºssen sie herunterladen und sie in `./trained-envs-executables/linux/` ablegen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C9Ls6_6eOKiA"
   },
   "outputs": [],
   "source": [
    "!mkdir ./trained-envs-executables\n",
    "!mkdir ./trained-envs-executables/linux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IHh_LXsRrrbM"
   },
   "source": [
    "Wir haben die Datei Huggy.zip von https://github.com/huggingface/Huggy mit `wget` heruntergeladen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8xNAD1tRpy0_"
   },
   "outputs": [],
   "source": [
    "!wget \"https://github.com/huggingface/Huggy/raw/main/Huggy.zip\" -O ./trained-envs-executables/linux/Huggy.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8FPx0an9IAwO"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!unzip -d ./trained-envs-executables/linux/ ./trained-envs-executables/linux/Huggy.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nyumV5XfPKzu"
   },
   "source": [
    "Stellen Sie sicher, dass Ihre Datei zug√§nglich ist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EdFsLJ11JvQf"
   },
   "outputs": [],
   "source": [
    "!chmod -R 755 ./trained-envs-executables/linux/Huggy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dYKVj8yUvj55"
   },
   "source": [
    "## Rekapitulieren wir, wie diese Umgebung funktioniert\n",
    "\n",
    "### Der Zustandsraum: was Huggy \"wahrnimmt\".\n",
    "\n",
    "Huggy \"sieht\" seine Umgebung nicht. Stattdessen liefern wir ihm Informationen √ºber die Umgebung:\n",
    "\n",
    "- Die Position des Ziels (Stock)\n",
    "- Die relative Position zwischen ihm und dem Ziel\n",
    "- Die Ausrichtung seiner Beine.\n",
    "\n",
    "Mit all diesen Informationen kann Huggy **entscheiden, welche Aktion er als N√§chstes durchf√ºhren muss, um sein Ziel zu erreichen**.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit-bonus1/huggy.jpg\" alt=\"Huggy\" width=\"100%\">\n",
    "\n",
    "\n",
    "### Der Aktionsraum: welche Bewegungen Huggy machen kann\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit-bonus1/huggy-action.jpg\" alt=\"Huggy Aktion\" width=\"100%\">\n",
    "\n",
    "**Gelenkmotoren treiben Huggys Beine an**. Das hei√üt, um das Ziel zu erreichen, muss Huggy **lernen, die Gelenkmotoren jedes seiner Beine richtig zu drehen, damit er sich bewegen kann**.\n",
    "\n",
    "### Die Belohnungsfunktion\n",
    "\n",
    "Die Belohnungsfunktion ist so konzipiert, dass **Huggy sein Ziel erreicht**: den Stock zu holen.\n",
    "\n",
    "Denken Sie daran, dass eine der Grundlagen des Verst√§rkungslernens die *Belohnungshypothese* ist: ein Ziel kann als **Maximierung der erwarteten kumulativen Belohnung** beschrieben werden.\n",
    "\n",
    "Hier ist unser Ziel, dass Huggy **zum Stock geht, ohne sich zu sehr zu drehen**. Daher muss unsere Belohnungsfunktion dieses Ziel umsetzen.\n",
    "\n",
    "Unsere Belohnungsfunktion:\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit-bonus1/reward.jpg\" alt=\"Huggy Belohnungsfunktion\" width=\"100%\">\n",
    "\n",
    "- *Orientierungsbonus*: wir **belohnen ihn, wenn er sich dem Ziel n√§hert**.\n",
    "- *Zeitstrafe*: eine feste Zeitstrafe bei jeder Aktion, um ihn zu **zwingen, so schnell wie m√∂glich zum Stock zu kommen**.\n",
    "- Rotationsstrafe*: Wir bestrafen Huggy, wenn er sich **zu sehr dreht und zu schnell dreht**.\n",
    "- *Belohnung f√ºr das Erreichen des Ziels*: Wir belohnen Huggy f√ºr das **Erreichen des Ziels**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NAuEq32Mwvtz"
   },
   "source": [
    "## Erstellen Sie die Huggy-Konfigurationsdatei\n",
    "\n",
    "- In ML-Agents definieren Sie die **Trainings-Hyperparameter in config.yaml-Dateien.**\n",
    "\n",
    "- Im Rahmen dieses Notizbuchs werden wir die Hyperparameter nicht √§ndern, aber wenn Sie es als Experiment versuchen wollen, sollten Sie auch versuchen, einige andere Hyperparameter zu √§ndern. Unity bietet eine sehr [gute Dokumentation, die jeden von ihnen hier erkl√§rt] (https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Training-Configuration-File.md).\n",
    "\n",
    "- Aber wir m√ºssen eine Konfigurationsdatei f√ºr Huggy erstellen.\n",
    "\n",
    "  - Klicke dazu auf das Ordner-Logo auf der linken Seite deines Bildschirms.\n",
    "\n",
    "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/create_file.png\" alt=\"Datei erstellen\" width=\"10%\">\n",
    "\n",
    "  - Gehen Sie zu `/content/ml-agents/config/ppo`\n",
    "  - Klicken Sie mit der rechten Maustaste und erstellen Sie eine neue Datei namens `Huggy.yaml`.\n",
    "\n",
    "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/create-huggy.png\" alt=\"Erstelle huggy.yaml\" width=\"20%\">\n",
    "\n",
    "- Kopieren Sie den folgenden Inhalt und f√ºgen Sie ihn ein üîΩ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "loQ0N5jhXW71"
   },
   "outputs": [],
   "source": [
    "behaviors:\n",
    "  Huggy:\n",
    "    trainer_type: ppo\n",
    "    hyperparameters:\n",
    "      batch_size: 2048\n",
    "      buffer_size: 20480\n",
    "      learning_rate: 0.0003\n",
    "      beta: 0.005\n",
    "      epsilon: 0.2\n",
    "      lambd: 0.95\n",
    "      num_epoch: 3\n",
    "      learning_rate_schedule: linear\n",
    "    network_settings:\n",
    "      normalize: true\n",
    "      hidden_units: 512\n",
    "      num_layers: 3\n",
    "      vis_encode_type: simple\n",
    "    reward_signals:\n",
    "      extrinsic:\n",
    "        gamma: 0.995\n",
    "        strength: 1.0\n",
    "    checkpoint_interval: 200000\n",
    "    keep_checkpoints: 15\n",
    "    max_steps: 2e6\n",
    "    time_horizon: 1000\n",
    "    summary_freq: 50000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oakN7UHwXdCX"
   },
   "source": [
    "- Vergessen Sie nicht, die Datei zu speichern!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9wv5NYGw-05"
   },
   "source": [
    "- Falls Sie die Hyperparameter** √§ndern m√∂chten, k√∂nnen Sie in Google Colab notebook hier klicken, um die config.yaml zu √∂ffnen: `/content/ml-agents/config/ppo/Huggy.yaml`\n",
    "\n",
    "- Zum Beispiel **wenn Sie mehr Modelle w√§hrend des Trainings speichern wollen** (im Moment speichern wir alle 200.000 Trainingszeitschritte). Sie m√ºssen Folgendes √§ndern:\n",
    "  - `checkpoint_interval`: Die Anzahl der Trainingszeitschritte, die zwischen den einzelnen Checkpoints gesammelt werden.\n",
    "  - keep_checkpoints\": Die maximale Anzahl der zu speichernden Modellpr√ºfpunkte.\n",
    "\n",
    "=> Behalten Sie im Hinterkopf, dass **eine Verringerung des `checkpoint_interval` bedeutet, dass mehr Modelle zum Hub hochgeladen werden m√ºssen und somit eine l√§ngere Hochladezeit**\n",
    "Wir sind nun bereit, unseren Agenten zu trainieren üî•."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9fI555bO12v"
   },
   "source": [
    "## Ausbildung unseres Agenten\n",
    "\n",
    "Um unseren Agenten zu trainieren, m√ºssen wir nur **mlagents-learn starten und die ausf√ºhrbare Datei ausw√§hlen, die die Umgebung enth√§lt**.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit-bonus1/mllearn.png\" alt=\"ml learn function\" width=\"100%\">\n",
    "\n",
    "Mit ML Agents f√ºhren wir ein Trainingsskript aus. Wir definieren vier Parameter:\n",
    "\n",
    "1. `mlagents-learn <config>`: der Pfad, in dem sich die Konfigurationsdatei f√ºr die Hyperparameter befindet.\n",
    "2. `--env`: wo sich die ausf√ºhrbare Umgebung befindet.\n",
    "3. Run-id\": Der Name, den Sie Ihrer Trainingslauf-ID geben wollen.\n",
    "4. `--no-graphics`: um die Visualisierung w√§hrend des Trainings nicht zu starten.\n",
    "\n",
    "Trainieren Sie das Modell und verwenden Sie das `--resume` Flag, um das Training im Falle einer Unterbrechung fortzusetzen.\n",
    "\n",
    "> Es wird beim ersten Mal fehlschlagen, wenn Sie `--resume` verwenden, versuchen Sie den Block erneut auszuf√ºhren, um den Fehler zu umgehen.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lN32oWF8zPjs"
   },
   "source": [
    "Das Training dauert zwischen 30 und 45 Minuten, je nach Computer (vergessen Sie nicht, eine GPU** einzurichten), gehen Sie auf ‚òïÔ∏èyou und verdienen Sie es ü§ó."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bS-Yh1UdHfzy"
   },
   "outputs": [],
   "source": [
    "!mlagents-learn ./config/ppo/Huggy.yaml --env=./trained-envs-executables/linux/Huggy/Huggy --run-id=\"Huggy2\" --no-graphics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Vue94AzPy1t"
   },
   "source": [
    "## Schieben Sie den Agenten zum ü§ó Hub\n",
    "\n",
    "- Nun, da wir unseren Agenten trainiert haben, sind wir **bereit, ihn zum Hub zu schieben, um mit Huggy im Browser spielen zu k√∂nnenüî•.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "izT6FpgNzZ6R"
   },
   "source": [
    "Um Ihr Modell mit der Gemeinschaft teilen zu k√∂nnen, sind drei weitere Schritte erforderlich:\n",
    "\n",
    "1Ô∏è‚É£ (Falls noch nicht geschehen) Erstellen Sie ein Konto f√ºr HF ‚û° https://huggingface.co/join\n",
    "\n",
    "2Ô∏è‚É£ Melde dich an und speichere dann dein Authentifizierungs-Token von der Hugging Face Website.\n",
    "- Erstellen Sie ein neues Token (https://huggingface.co/settings/tokens) **mit Schreibrolle**\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/create-token.jpg\" alt=\"HF-Token erstellen\">\n",
    "\n",
    "- Kopieren Sie das Token\n",
    "- F√ºhren Sie die Zelle unten aus und f√ºgen Sie das Token ein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rKt2vsYoK56o"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ew59mK19zjtN"
   },
   "source": [
    "Wenn Sie kein Google Colab oder ein Jupyter Notebook verwenden m√∂chten, m√ºssen Sie stattdessen diesen Befehl verwenden: `huggingface-cli login`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xi0y_VASRzJU"
   },
   "source": [
    "Dann m√ºssen wir einfach `mlagents-push-to-hf` ausf√ºhren.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit-bonus1/mlpush.png\" alt=\"ml learn function\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KK4fPfnczunT"
   },
   "source": [
    "Und wir definieren 4 Parameter:\n",
    "\n",
    "1. Lauf-ID\": der Name des Trainingslaufs.\n",
    "2. `--local-dir`: wo der Agent gespeichert wurde, es ist results/<run_id name>, also in meinem Fall results/First Training.\n",
    "3. `--repo-id`: der Name des Hugging Face Repos, das du erstellen oder aktualisieren willst. Es ist immer <Ihr Hugging-Face-Benutzername>/<Der Repo-Name>\n",
    "Wenn das Repo nicht existiert, wird es automatisch erstellt**.\n",
    "4. `--commit-message`: Da HF-Repos Git-Repos sind, m√ºssen Sie eine Commit-Message definieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dGEFAIboLVc6"
   },
   "outputs": [],
   "source": [
    "!mlagents-push-to-hf --run-id=\"HuggyTraining\" --local-dir=\"./results/Huggy\" --repo-id=\"ThomasSimonini/ppo-Huggy\" --commit-message=\"Huggy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yborB0850FTM"
   },
   "source": [
    "Andernfalls, wenn alles funktioniert hat, sollten Sie am Ende des Prozesses diese Seite haben (allerdings mit einer anderen URL üòÜ):\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "Ihr Modell wird in den Hub √ºbertragen. Du kannst dein Modell hier sehen: https://huggingface.co/ThomasSimonini/ppo-Huggy\n",
    "```\n",
    "\n",
    "Das ist der Link zu Ihrem Modell-Repository. Das Repository enth√§lt eine Modellkarte, die erkl√§rt, wie man das Modell benutzt, Ihre Tensorboard-Logs und Ihre Konfigurationsdatei. **Das Tolle ist, dass es ein Git-Repository ist, was bedeutet, dass Sie verschiedene Commits haben k√∂nnen, Ihr Repository mit einem neuen Push aktualisieren k√∂nnen, Pull Requests √∂ffnen k√∂nnen, etc.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit-bonus1/modelcard.png\" alt=\"ml learn function\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Uaon2cg0NrL"
   },
   "source": [
    "Aber jetzt kommt das Beste: **Mit Huggy online spielen zu k√∂nnen üëÄ.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VMc4oOsE0QiZ"
   },
   "source": [
    "## Spiel mit deinem Huggy üêï\n",
    "\n",
    "Dieser Schritt ist der einfachste:\n",
    "\n",
    "- √ñffne das Spiel Huggy in deinem Browser: https://huggingface.co/spaces/ThomasSimonini/Huggy\n",
    "\n",
    "- Klicken Sie auf Play with my Huggy model\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit-bonus1/load-huggy.jpg\" alt=\"load-huggy\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Djs8c5rR0Z8a"
   },
   "source": [
    "1. In Schritt 1 w√§hlen Sie Ihr Modell-Repository, das die Modell-ID ist (in meinem Fall ThomasSimonini/ppo-Huggy).\n",
    "\n",
    "2. In Schritt 2 **w√§hlen Sie das Modell aus, das Sie wiedergeben m√∂chten**:\n",
    "  - Ich habe mehrere Modelle, da wir alle 500000 Zeitschritte ein Modell gespeichert haben.\n",
    "  - Da ich aber das aktuellste m√∂chte, w√§hle ich \"Huggy.onnx\".\n",
    "\n",
    "üëâ Das Sch√∂ne ist, **dass man verschiedene Modellschritte ausprobieren kann, um die Verbesserung des Agenten zu sehen**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PI6dPWmh064H"
   },
   "source": [
    "Herzlichen Gl√ºckwunsch zum Abschluss dieser Bonuseinheit!\n",
    "\n",
    "Jetzt kannst du dich hinsetzen und mit deinem Huggy üê∂ spielen. Und vergesst **nicht, die Liebe zu verbreiten, indem ihr Huggy mit euren Freunden teilt ü§ó**. Und wenn du es in den sozialen Medien teilst, **bitte tagge uns @huggingface und mich @simoninithomas**.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit-bonus1/huggy-cover.jpeg\" alt=\"Huggy-Cover\" width=\"100%\">\n",
    "\n",
    "\n",
    "## Keep Learning, Stay awesome ü§ó"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
