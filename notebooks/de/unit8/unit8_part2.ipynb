{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/notebooks/unit8/unit8_part2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"In Colab √∂ffnen\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OVx1gdg9wt9t"
   },
   "source": [
    "# Unit 8 Teil 2: Fortgeschrittenes Deep Reinforcement Learning. Verwendung von Sample Factory zur Wiedergabe von Doom aus Pixeln\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit9/thumbnail2.png\" alt=\"Thumbnail\"/>\n",
    "\n",
    "In diesem Notizbuch lernen wir, wie man ein Deep Neural Network trainiert, um Objekte in einer 3D-Umgebung zu sammeln, die auf dem Spiel Doom basiert; ein Video der resultierenden Policy ist unten zu sehen. Wir trainieren diese Strategie mit [Sample Factory] (https://www.samplefactory.dev/), einer asynchronen Implementierung des PPO-Algorithmus.\n",
    "\n",
    "Bitte beachten Sie die folgenden Punkte:\n",
    "\n",
    "* [Sample Factory](https://www.samplefactory.dev/) ist ein fortgeschrittenes RL-Framework und funktioniert **nur unter Linux und Mac** (nicht unter Windows).\n",
    "\n",
    "* Das Framework funktioniert am besten auf einer **GPU-Maschine mit vielen CPU-Kernen**, wo es Geschwindigkeiten von 100k Interaktionen pro Sekunde erreichen kann. Die auf einem Standard-Colab-Notebook verf√ºgbaren Ressourcen **begrenzen die Leistung dieser Bibliothek**. Daher spiegelt die Geschwindigkeit in dieser Einstellung **nicht die reale Leistung wider**.\n",
    "* Benchmarks f√ºr Sample Factory sind in einer Reihe von Einstellungen verf√ºgbar, schauen Sie sich die [examples](https://github.com/alex-petrenko/sample-factory/tree/master/sf_examples) an, wenn Sie mehr erfahren m√∂chten.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I6_67HfI1CKg"
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"https://huggingface.co/edbeeching/doom_health_gathering_supreme_3333/resolve/main/replay.mp4\"\n",
    "  type=\"video/mp4\">Your browser does not support the video tag.</video>'''\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DgHRAsYEXdyw"
   },
   "source": [
    "Um dieses Hands-on f√ºr den [Zertifizierungsprozess] (https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process) zu validieren, m√ºssen Sie ein Modell schieben:\n",
    "\n",
    "- `doom_health_gathering_supreme` ein Ergebnis von >= 5 erhalten.\n",
    "\n",
    "Um Ihr Ergebnis zu ermitteln, gehen Sie zur [Bestenliste] (https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) und suchen Sie Ihr Modell, **das Ergebnis = mean_reward - std of reward**\n",
    "\n",
    "Wenn Sie Ihr Modell nicht finden, **gehen Sie zum Ende der Seite und klicken Sie auf die Schaltfl√§che \"Aktualisieren \"**.\n",
    "\n",
    "Weitere Informationen √ºber den Zertifizierungsprozess finden Sie in diesem Abschnitt üëâ https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PU4FVzaoM6fC"
   },
   "source": [
    "## Die GPU einstellen üí™.\n",
    "- Um **das Training des Agenten zu beschleunigen, werden wir einen Grafikprozessor** verwenden. Gehen Sie dazu auf \"Runtime > Change Runtime type\".\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step1.jpg\" alt=\"GPU Schritt 1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KV0NyFdQM9ZG"
   },
   "source": [
    "- Hardware-Beschleuniger > GPU\".\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step2.jpg\" alt=\"GPU Schritt 2\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-fSy5HzUcMWB"
   },
   "source": [
    "Bevor wir mit dem Training unseres Agenten beginnen, sollten wir **die Bibliothek und die Umgebungen studieren, die wir verwenden werden**.\n",
    "\n",
    "## Beispiel Fabrik\n",
    "\n",
    "[Sample Factory] (https://www.samplefactory.dev/) ist eine der **schnellsten RL-Bibliotheken, die sich auf sehr effiziente synchrone und asynchrone Implementierungen von Policy-Gradienten (PPO)** konzentriert.\n",
    "\n",
    "Sample Factory ist gr√ºndlich **getestet, wird von vielen Forschern und Praktikern** verwendet und wird aktiv gepflegt. Unsere Implementierung ist bekannt daf√ºr, dass sie **in einer Vielzahl von Dom√§nen SOTA-Leistung erreicht und gleichzeitig die Trainingszeit f√ºr RL-Experimente und die Hardwareanforderungen minimiert**.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit9/samplefactoryenvs.png\" alt=\"Musterfabrik\"/>\n",
    "\n",
    "\n",
    "\n",
    "### Hauptmerkmale\n",
    "\n",
    "- Hochgradig optimierte Algorithmus-[Architektur](https://www.samplefactory.dev/06-architecture/overview/) f√ºr maximalen Lerndurchsatz\n",
    "- Synchrone und asynchrone](https://www.samplefactory.dev/07-advanced-topics/sync-async/) Trainingsregime\n",
    "- Serieller (Einzelprozess-)Modus](https://www.samplefactory.dev/07-advanced-topics/serial-mode/) f√ºr einfaches Debugging\n",
    "- Optimale Leistung sowohl in CPU-basierten als auch [GPU-beschleunigten Umgebungen](https://www.samplefactory.dev/09-environment-integrations/isaacgym/)\n",
    "- Einzel- und Multi-Agenten-Training, Self-Play, unterst√ºtzt [Training mehrerer Policies](https://www.samplefactory.dev/07-advanced-topics/multi-policy-training/) gleichzeitig auf einem oder mehreren GPUs\n",
    "- Populationsbasiertes Training ([PBT](https://www.samplefactory.dev/07-advanced-topics/pbt/))\n",
    "- Diskrete, kontinuierliche, hybride Aktionsr√§ume\n",
    "- Vektorbasierte, bildbasierte, w√∂rterbuchbasierte Beobachtungsr√§ume\n",
    "- Erstellt automatisch eine Modellarchitektur durch Analyse der Spezifikation des Aktions-/Beobachtungsraums. Unterst√ºtzt [benutzerdefinierte Modellarchitekturen](https://www.samplefactory.dev/03-customization/custom-models/)\n",
    "- Entwickelt, um in andere Projekte importiert zu werden, sind [benutzerdefinierte Umgebungen](https://www.samplefactory.dev/03-customization/custom-environments/) B√ºrger erster Klasse\n",
    "- Detaillierte [WandB und Tensorboard Zusammenfassungen](https://www.samplefactory.dev/05-monitoring/metrics-reference/), [benutzerdefinierte Metriken](https://www.samplefactory.dev/05-monitoring/custom-metrics/)\n",
    "- [HuggingFace ü§ó Integration](https://www.samplefactory.dev/10-huggingface/huggingface/) (trainierte Modelle und Metriken in den Hub hochladen)\n",
    "- [Mehrere](https://www.samplefactory.dev/09-environment-integrations/mujoco/) [Beispiel](https://www.samplefactory.dev/09-environment-integrations/atari/) [Umgebung](https://www.samplefactory.dev/09-environment-integrations/vizdoom/) [Integrationen](https://www.samplefactory.dev/09-environment-integrations/dmlab/) mit abgestimmten Parametern und trainierten Modellen\n",
    "\n",
    "Alle oben genannten Richtlinien sind auf dem ü§ó Hub verf√ºgbar. Suchen Sie nach dem Tag [sample-factory](https://huggingface.co/models?library=sample-factory&sort=downloads)\n",
    "\n",
    "### Wie sample-factory funktioniert\n",
    "\n",
    "Sample-factory ist eine der **h√∂chst optimierten RL-Implementierungen, die der Community zur Verf√ºgung stehen**.\n",
    "\n",
    "Sie funktioniert, indem sie **mehrere Prozesse startet, die Rollout-Worker, Inferenz-Worker und einen Learner-Worker** ausf√ºhren.\n",
    "\n",
    "Die *Arbeiter* **kommunizieren √ºber gemeinsamen Speicher, was die Kommunikationskosten zwischen den Prozessen senkt**.\n",
    "\n",
    "Die *Rollout Worker* interagieren mit der Umgebung und senden Beobachtungen an die *Inference Worker*.\n",
    "\n",
    "Die *Inferenz-Worker* fragen eine feste Version der Richtlinie ab und **senden Aktionen zur√ºck an den Rollout-Worker**.\n",
    "\n",
    "Nach *k* Schritten senden die Rollout-Worker eine Trajektorie der Erfahrungen an den Learner-Worker, **die dieser zur Aktualisierung des Policy-Netzwerks des Agenten verwendet**.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit9/samplefactory.png\" alt=\"Beispielfabrik\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nB68Eb9UgC94"
   },
   "source": [
    "### Actor Critic Modelle in der Sample-Fabrik\n",
    "\n",
    "Actor Critic-Modelle in Sample Factory bestehen aus drei Komponenten:\n",
    "\n",
    "- **Encoder** - Verarbeitet Eingangsbeobachtungen (Bilder, Vektoren) und bildet sie auf einen Vektor ab. Dies ist der Teil des Modells, den Sie am ehesten anpassen m√∂chten.\n",
    "- **Kern** - Integriert Vektoren von einem oder mehreren Encodern, kann optional eine ein- oder mehrschichtige LSTM/GRU in einem speicherbasierten Agenten enthalten.\n",
    "- **Decoder** - Wenden Sie zus√§tzliche Schichten auf die Ausgabe des Modellkerns an, bevor Sie die Richtlinien- und Wertausgaben berechnen.\n",
    "\n",
    "Die Bibliothek wurde so konzipiert, dass sie automatisch beliebige Beobachtungs- und Aktionsr√§ume unterst√ºtzt. Benutzer k√∂nnen ihre eigenen Modelle leicht hinzuf√ºgen. Weitere Informationen finden Sie in der [Dokumentation] (https://www.samplefactory.dev/03-customization/custom-models/#actor-critic-models-in-sample-factory)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ez5UhUtYcWXF"
   },
   "source": [
    "## ViZDoom\n",
    "\n",
    "[ViZDoom](https://vizdoom.cs.put.edu.pl/) ist eine **Open-Source-Python-Schnittstelle f√ºr die Doom Engine**.\n",
    "\n",
    "Die Bibliothek wurde 2016 von Marek Wydmuch und Michal Kempka am Institut f√ºr Informatik der Technischen Universit√§t Poznan, Polen, entwickelt.\n",
    "\n",
    "Die Bibliothek erm√∂glicht das **Training von Agenten direkt von den Bildschirmpixeln in einer Reihe von Szenarien**, einschlie√ülich Team-Deathmatch, wie im Video unten gezeigt. Da die ViZDoom-Umgebung auf einem Spiel basiert, das in den 90er Jahren entwickelt wurde, kann sie auf moderner Hardware mit beschleunigter Geschwindigkeit ausgef√ºhrt werden, **was es uns erm√∂glicht, komplexe KI-Verhaltensweisen ziemlich schnell zu erlernen**.\n",
    "\n",
    "Die Bibliothek umfasst Funktionen wie:\n",
    "\n",
    "- Multiplattform (Linux, macOS, Windows),\n",
    "- API f√ºr Python und C++,\n",
    "- [OpenAI Gym](https://www.gymlibrary.dev/) Umgebungs-Wrapper\n",
    "- Leicht zu erstellende benutzerdefinierte Szenarien (visuelle Editoren, Skriptsprache und Beispiele verf√ºgbar),\n",
    "- Asynchroner und synchroner Einzel- und Mehrspielermodus,\n",
    "- Leichtgewichtig (wenige MB) und schnell (bis zu 7000 fps im Sync-Modus, single-threaded),\n",
    "- Anpassbare Aufl√∂sung und Rendering-Parameter,\n",
    "- Zugriff auf den Tiefenpuffer (3D-Vision),\n",
    "- Automatische Beschriftung der im Bild sichtbaren Spielobjekte,\n",
    "- Zugriff auf den Audiopuffer\n",
    "- Zugriff auf die Liste der Akteure/Objekte und die Kartengeometrie,\n",
    "- Off-Screen-Rendering und Episodenaufzeichnung,\n",
    "- Zeitskalierung im asynchronen Modus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wAMwza0d5QVj"
   },
   "source": [
    "## Zuerst m√ºssen wir einige Abh√§ngigkeiten installieren, die f√ºr die ViZDoom-Umgebung erforderlich sind\n",
    "\n",
    "Nachdem unsere Colab-Laufzeitumgebung nun eingerichtet ist, k√∂nnen wir mit der Installation der Abh√§ngigkeiten beginnen, die f√ºr die Ausf√ºhrung von ViZDoom unter Linux erforderlich sind.\n",
    "\n",
    "Wenn Sie auf Ihrem Mac arbeiten, folgen Sie bitte den Installationsanweisungen auf der [github-Seite] (https://github.com/Farama-Foundation/ViZDoom/blob/master/doc/Quickstart.md#-quickstart-for-macos-and-anaconda3-python-36)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RJMxkaldwIVx"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%%bash\n",
    "# Install ViZDoom deps from\n",
    "# https://github.com/mwydmuch/ViZDoom/blob/master/doc/Building.md#-linux\n",
    "\n",
    "apt-get install build-essential zlib1g-dev libsdl2-dev libjpeg-dev \\\n",
    "nasm tar libbz2-dev libgtk2.0-dev cmake git libfluidsynth-dev libgme-dev \\\n",
    "libopenal-dev timidity libwildmidi-dev unzip ffmpeg\n",
    "\n",
    "# Boost libraries\n",
    "apt-get install libboost-all-dev\n",
    "\n",
    "# Lua binding dependencies\n",
    "apt-get install liblua5.1-dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JT4att2c57MW"
   },
   "source": [
    "## Dann k√∂nnen wir Sample Factory und ViZDoom installieren.\n",
    "- Dies kann 7 Minuten dauern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bbqfPZnIsvA6"
   },
   "outputs": [],
   "source": [
    "# install python libraries\n",
    "# thanks toinsson\n",
    "!pip install faster-fifo==1.4.2\n",
    "!pip install vizdoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "alxUt7Au-O8e"
   },
   "outputs": [],
   "source": [
    "!pip install sample-factory==2.0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1jizouGpghUZ"
   },
   "source": [
    "## Einrichten der Doom-Umgebung in sample-factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bCgZbeiavcDU"
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "from sample_factory.algo.utils.context import global_model_factory\n",
    "from sample_factory.cfg.arguments import parse_full_cfg, parse_sf_args\n",
    "from sample_factory.envs.env_utils import register_env\n",
    "from sample_factory.train import run_rl\n",
    "\n",
    "from sf_examples.vizdoom.doom.doom_model import make_vizdoom_encoder\n",
    "from sf_examples.vizdoom.doom.doom_params import add_doom_env_args, doom_override_defaults\n",
    "from sf_examples.vizdoom.doom.doom_utils import DOOM_ENVS, make_doom_env_from_spec\n",
    "\n",
    "\n",
    "# Registers all the ViZDoom environments\n",
    "def register_vizdoom_envs():\n",
    "    for env_spec in DOOM_ENVS:\n",
    "        make_env_func = functools.partial(make_doom_env_from_spec, env_spec)\n",
    "        register_env(env_spec.name, make_env_func)\n",
    "\n",
    "# Sample Factory allows the registration of a custom Neural Network architecture\n",
    "# See https://github.com/alex-petrenko/sample-factory/blob/master/sf_examples/vizdoom/doom/doom_model.py for more details\n",
    "def register_vizdoom_models():\n",
    "    global_model_factory().register_encoder_factory(make_vizdoom_encoder)\n",
    "\n",
    "\n",
    "def register_vizdoom_components():\n",
    "    register_vizdoom_envs()\n",
    "    register_vizdoom_models()\n",
    "\n",
    "# parse the command line args and create a config\n",
    "def parse_vizdoom_cfg(argv=None, evaluation=False):\n",
    "    parser, _ = parse_sf_args(argv=argv, evaluation=evaluation)\n",
    "    # parameters specific to Doom envs\n",
    "    add_doom_env_args(parser)\n",
    "    # override Doom default values for algo parameters\n",
    "    doom_override_defaults(parser)\n",
    "    # second parsing pass yields the final configuration\n",
    "    final_cfg = parse_full_cfg(parser, argv)\n",
    "    return final_cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgRy6wnrgnij"
   },
   "source": [
    "Nun, da die Einrichtung abgeschlossen ist, k√∂nnen wir den Agenten trainieren. Wir haben uns hier f√ºr eine ViZDoom-Aufgabe namens \"Health Gathering Supreme\" entschieden.\n",
    "\n",
    "### Das Szenario: Health Gathering Supreme\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit9/Health-Gathering-Supreme.png\" alt=\"Health-Gathering-Supreme\"/>\n",
    "\n",
    "\n",
    "\n",
    "Das Ziel dieses Szenarios ist es, **dem Agenten beizubringen, wie er √ºberleben kann, ohne zu wissen, was ihn √ºberleben l√§sst**. Der Agent wei√ü nur, dass **das Leben kostbar** und der Tod schlecht ist, also **muss er lernen, was seine Existenz verl√§ngert und dass seine Gesundheit damit zusammenh√§ngt**.\n",
    "\n",
    "Die Karte ist ein Rechteck mit W√§nden und einem gr√ºnen, s√§urehaltigen Boden, der den Spieler regelm√§√üig **verletzt**. Anfangs gibt es einige Medkits, die gleichm√§√üig √ºber die Karte verteilt sind. Ab und zu f√§llt ein neues Medkit vom Himmel. **Medkits heilen einen Teil der Gesundheit des Spielers** - um zu √ºberleben, muss der Agent sie aufsammeln. Die Episode endet nach dem Tod des Spielers oder bei Zeit√ºberschreitung.\n",
    "\n",
    "Weitere Konfiguration:\n",
    "- Lebendige_Belohnung = 1\n",
    "- 3 verf√ºgbare Tasten: links drehen, rechts drehen, vorw√§rts gehen\n",
    "- 1 verf√ºgbare Spielvariable: GESUNDHEIT\n",
    "- Todesstrafe = 100\n",
    "\n",
    "Weitere Informationen √ºber die in ViZDoom verf√ºgbaren Szenarien finden Sie [hier](https://github.com/Farama-Foundation/ViZDoom/tree/master/scenarios).\n",
    "\n",
    "Es gibt auch eine Reihe von komplexeren Szenarien, die f√ºr ViZDoom erstellt wurden, wie z. B. die auf [dieser github-Seite](https://github.com/edbeeching/3d_control_deep_rl) beschriebenen.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "siHZZ34DiZEp"
   },
   "source": [
    "## Training des Agenten\n",
    "- Wir werden den Agenten f√ºr 4000000 Schritte trainieren, was etwa 20 Minuten dauern wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y_TeicMvyKHP"
   },
   "outputs": [],
   "source": [
    "## Start the training, this should take around 15 minutes\n",
    "register_vizdoom_components()\n",
    "\n",
    "# The scenario we train on today is health gathering\n",
    "# other scenarios include \"doom_basic\", \"doom_two_colors_easy\", \"doom_dm\", \"doom_dwango5\", \"doom_my_way_home\", \"doom_deadly_corridor\", \"doom_defend_the_center\", \"doom_defend_the_line\"\n",
    "env = \"doom_health_gathering_supreme\"\n",
    "cfg = parse_vizdoom_cfg(argv=[f\"--env={env}\", \"--num_workers=8\", \"--num_envs_per_worker=4\", \"--train_for_env_steps=4000000\"])\n",
    "\n",
    "status = run_rl(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5L0nBS9e_jqC"
   },
   "source": [
    "## Schauen wir uns die Leistung der trainierten Richtlinie an und geben ein Video des Agenten aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MGSA4Kg5_i0j"
   },
   "outputs": [],
   "source": [
    "from sample_factory.enjoy import enjoy\n",
    "cfg = parse_vizdoom_cfg(argv=[f\"--env={env}\", \"--num_workers=1\", \"--save_video\", \"--no_render\", \"--max_num_episodes=10\"], evaluation=True)\n",
    "status = enjoy(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lj5L1x0WLxwB"
   },
   "source": [
    "## Nun wollen wir die Leistung des Agenten visualisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WsXhBY7JNOdJ"
   },
   "outputs": [],
   "source": [
    "from base64 import b64encode\n",
    "from IPython.display import HTML\n",
    "\n",
    "mp4 = open('/content/train_dir/default_experiment/replay.mp4','rb').read()\n",
    "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "HTML(\"\"\"\n",
    "<video width=640 controls>\n",
    "      <source src=\"%s\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\" % data_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2A4pf_1VwPqR"
   },
   "source": [
    "Der Agent hat etwas gelernt, aber seine Leistung k√∂nnte besser sein. Wir m√ºssten eindeutig l√§nger trainieren. Aber lassen Sie uns dieses Modell in den Hub hochladen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CSQVWF0kNuy9"
   },
   "source": [
    "## Jetzt k√∂nnen wir deinen Checkpoint und dein Video in den Hugging Face Hub hochladen.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JquRrWytA6eo"
   },
   "source": [
    "Um Ihr Modell mit der Gemeinschaft teilen zu k√∂nnen, sind drei weitere Schritte erforderlich:\n",
    "\n",
    "1Ô∏è‚É£ (Falls noch nicht geschehen) Erstellen Sie ein Konto f√ºr HF ‚û° https://huggingface.co/join\n",
    "\n",
    "2Ô∏è‚É£ Melde dich an und speichere dann dein Authentifizierungs-Token von der Hugging Face Website.\n",
    "- Erstellen Sie ein neues Token (https://huggingface.co/settings/tokens) **mit Schreibrolle**\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/create-token.jpg\" alt=\"HF-Token erstellen\">\n",
    "\n",
    "- Kopieren Sie das Token\n",
    "- F√ºhren Sie die Zelle unten aus und f√ºgen Sie das Token ein"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_tsf2uv0g_4p"
   },
   "source": [
    "Wenn Sie kein Google Colab oder ein Jupyter Notebook verwenden m√∂chten, m√ºssen Sie stattdessen diesen Befehl verwenden: `huggingface-cli login`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GoQm_jYSOts0"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "!git config --global credential.helper store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sEawW_i0OvJV"
   },
   "outputs": [],
   "source": [
    "from sample_factory.enjoy import enjoy\n",
    "\n",
    "hf_username = \"ThomasSimonini\" # insert your HuggingFace username here\n",
    "\n",
    "cfg = parse_vizdoom_cfg(argv=[f\"--env={env}\", \"--num_workers=1\", \"--save_video\", \"--no_render\", \"--max_num_episodes=10\", \"--max_num_frames=100000\", \"--push_to_hub\", f\"--hf_repository={hf_username}/rl_course_vizdoom_health_gathering_supreme\"], evaluation=True)\n",
    "status = enjoy(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9PzeXx-qxVvw"
   },
   "source": [
    "## Lassen Sie uns ein anderes Modell laden\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mHZAWSgL5F7P"
   },
   "source": [
    "Die Leistung dieses Agenten war gut, kann aber noch besser werden! Laden wir einen Agenten, der f√ºr 10B Zeitschritte trainiert wurde, vom Hub herunter und visualisieren ihn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ud6DwAUl5S-l"
   },
   "outputs": [],
   "source": [
    "#download the agent from the hub\n",
    "!python -m sample_factory.huggingface.load_from_hub -r edbeeching/doom_health_gathering_supreme_2222 -d ./train_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qoUJhL6x6sY5"
   },
   "outputs": [],
   "source": [
    "!ls train_dir/doom_health_gathering_supreme_2222"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lZskc8LG8qr8"
   },
   "outputs": [],
   "source": [
    "env = \"doom_health_gathering_supreme\"\n",
    "cfg = parse_vizdoom_cfg(argv=[f\"--env={env}\", \"--num_workers=1\", \"--save_video\", \"--no_render\", \"--max_num_episodes=10\", \"--experiment=doom_health_gathering_supreme_2222\", \"--train_dir=train_dir\"], evaluation=True)\n",
    "status = enjoy(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BtzXBoj65Wmq"
   },
   "outputs": [],
   "source": [
    "mp4 = open('/content/train_dir/doom_health_gathering_supreme_2222/replay.mp4','rb').read()\n",
    "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "HTML(\"\"\"\n",
    "<video width=640 controls>\n",
    "      <source src=\"%s\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\" % data_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ie5YWC3NyKO8"
   },
   "source": [
    "## Einige zus√§tzliche Herausforderungen üèÜ: Doom Deathmatch\n",
    "\n",
    "Einen Agenten f√ºr ein Doom-Deathmatch zu trainieren **dauert viele Stunden auf einem leistungsf√§higeren Rechner, als er in Colab verf√ºgbar ist**.\n",
    "\n",
    "Gl√ºcklicherweise haben wir **bereits einen Agenten in diesem Szenario trainiert und er ist im ü§ó Hub verf√ºgbar!** Laden wir das Modell herunter und visualisieren wir die Leistung des Agenten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fq3WFeus81iI"
   },
   "outputs": [],
   "source": [
    "# Download the agent from the hub\n",
    "!python -m sample_factory.huggingface.load_from_hub -r edbeeching/doom_deathmatch_bots_2222 -d ./train_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7AX_LwxR2FQ0"
   },
   "source": [
    "Wenn der Agent sehr lange spielt, kann die Videoerstellung **10 Minuten** dauern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0hq6XL__85Bv"
   },
   "outputs": [],
   "source": [
    "\n",
    "from sample_factory.enjoy import enjoy\n",
    "register_vizdoom_components()\n",
    "env = \"doom_deathmatch_bots\"\n",
    "cfg = parse_vizdoom_cfg(argv=[f\"--env={env}\", \"--num_workers=1\", \"--save_video\", \"--no_render\", \"--max_num_episodes=1\", \"--experiment=doom_deathmatch_bots_2222\", \"--train_dir=train_dir\"], evaluation=True)\n",
    "status = enjoy(cfg)\n",
    "mp4 = open('/content/train_dir/doom_deathmatch_bots_2222/replay.mp4','rb').read()\n",
    "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "HTML(\"\"\"\n",
    "<video width=640 controls>\n",
    "      <source src=\"%s\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\" % data_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N6mEC-4zyihx"
   },
   "source": [
    "\n",
    "Sie **k√∂nnen versuchen, Ihren Agenten in dieser Umgebung** zu trainieren, indem Sie den obigen Code verwenden, aber nicht auf colab.\n",
    "**Viel Gl√ºck ü§û**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YnDAngN6zeeI"
   },
   "source": [
    "Wenn Sie ein einfacheres Szenario bevorzugen, **k√∂nnen Sie versuchen, in einem anderen ViZDoom-Szenario zu trainieren, z. B. `doom_deadly_corridor` oder `doom_defend_the_center`.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Damit ist die letzte Einheit abgeschlossen. Aber wir sind noch nicht fertig! Der folgende **Bonusabschnitt enth√§lt einige der interessantesten, fortgeschrittensten und innovativsten Arbeiten im Bereich des Deep Reinforcement Learning**.\n",
    "\n",
    "## Lernen Sie weiter, bleiben Sie fantastisch ü§ó"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "PU4FVzaoM6fC",
    "nB68Eb9UgC94",
    "ez5UhUtYcWXF",
    "sgRy6wnrgnij"
   ],
   "include_colab_link": true,
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
