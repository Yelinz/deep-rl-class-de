{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njb_ProuHiOe"
   },
   "source": [
    "# Unit 1: Trainiere deinen ersten Deep Reinforcement Learning Agent ü§ñ.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/thumbnail.jpg\" alt=\"Illustration_1\" width=\"50%\">\n",
    "\n",
    "In diesem Notizbuch werden Sie Ihren **ersten Deep Reinforcement Learning-Agenten** trainieren, einen Lunar Lander-Agenten, der lernen soll, **richtig auf dem Mond zu landen üåï**. Verwenden Sie [Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/), eine Deep Reinforcement Learning-Bibliothek, teilen Sie sie mit der Community und experimentieren Sie mit verschiedenen Konfigurationen\n",
    "\n",
    "‚¨áÔ∏è Hier ist ein Beispiel daf√ºr, was **man in nur wenigen Minuten erreichen kann** ‚¨áÔ∏è\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PF46MwbZD00b"
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<video controls autoplay><source src=\"https://huggingface.co/sb3/ppo-LunarLander-v2/resolve/main/replay.mp4\" type=\"video/mp4\"></video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x7oR6R-ZIbeS"
   },
   "source": [
    "### Die Umgebung üéÆ\n",
    "\n",
    "- [LunarLander-v2](https://gymnasium.farama.org/environments/box2d/lunar_lander/)\n",
    "\n",
    "### Die verwendete Bibliothek üìö\n",
    "\n",
    "- [Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OwEcFHe9RRZW"
   },
   "source": [
    "Wir versuchen st√§ndig, unsere Anleitungen zu verbessern. **Wenn Sie also Probleme in diesem Notizbuch** finden, √∂ffnen Sie bitte [ein Problem im Github Repo](https://github.com/huggingface/deep-rl-class/issues)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4i6tjI2tHQ8j"
   },
   "source": [
    "## Ziele dieses Notizbuchs üèÜ\n",
    "\n",
    "Am Ende des Notizbuchs werden Sie:\n",
    "\n",
    "- In der Lage sein, **Gymnasium**, die Umgebungsbibliothek, zu benutzen.\n",
    "- In der Lage sein, **Stable-Baselines3**, die Deep Reinforcement Learning-Bibliothek, zu verwenden.\n",
    "- In der Lage sein, **Ihren trainierten Agenten auf den Hub** zu pushen, mit einer sch√∂nen Videowiedergabe und einem Bewertungsergebnis üî•.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ff-nyJdzJPND"
   },
   "source": [
    "## Dieses Notebook ist aus dem Deep Reinforcement Learning Kurs\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/deep-rl-course-illustration.jpg\" alt=\"Deep RL Course illustration\" width=\"30%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6p5HnEefISCB"
   },
   "source": [
    "In diesem kostenlosen Kurs lernen Sie:\n",
    "\n",
    "- üìñ Deep Reinforcement Learning in **Theorie und Praxis** studieren.\n",
    "- üßë‚Äçüíª Lernen Sie, **ber√ºhmte Deep RL-Bibliotheken** wie Stable Baselines3, RL Baselines3 Zoo, CleanRL und Sample Factory 2.0 zu verwenden.\n",
    "- ü§ñ Trainieren Sie **Agenten in einzigartigen Umgebungen**.\n",
    "- üéì **Erwerben Sie ein Abschlusszertifikat**, wenn Sie 80% der Aufgaben erf√ºllen.\n",
    "\n",
    "Und mehr!\n",
    "\n",
    "Pr√ºfen Sie üìö den Lehrplan üëâ https://simoninithomas.github.io/deep-rl-course\n",
    "\n",
    "Vergessen Sie nicht, sich **<a href=\"http://eepurl.com/ic5ZUD\">f√ºr den Kurs anzumelden</a>** (wir sammeln Ihre E-Mail, um Ihnen **die Links zu schicken, wenn die einzelnen Einheiten ver√∂ffentlicht werden, und Sie √ºber die Herausforderungen und Aktualisierungen zu informieren).**\n",
    "\n",
    "Der beste Weg, um in Kontakt zu bleiben und Fragen zu stellen, ist **unserem Discord-Server** beizutreten, um sich mit der Community und uns auszutauschen üëâüèª https://discord.gg/ydHrjt3WP5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KI und Reinforcement Learning\n",
    "Wie Reinforcement Learning (RL) im zusammenhang mit KI und deren anderen Feldern zusammenh√§ngt\n",
    "\n",
    "<img src=\"https://i.imgur.com/xGGaw0k.png\" alt=\"KI Felder\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-mo_6rXIjRi"
   },
   "source": [
    "# Was ist Reinforcement Learning?\n",
    "\n",
    "Um Reinforcement Learning zu verstehen, sollten wir mit dem gro√üen Bild beginnen.\n",
    "\n",
    "## Das gro√üe Bild\n",
    "\n",
    "Die Idee hinter Reinforcement Learning ist, dass ein Agent (eine KI) von der Umwelt lernt, indem er **mit ihr interagiert** (durch Versuch und Irrtum) und **Belohnungen** (negativ oder positiv) als R√ºckmeldung f√ºr ausgef√ºhrte Aktionen erh√§lt.\n",
    "\n",
    "Das Lernen aus Interaktionen mit der Umwelt **entsteht aus unseren nat√ºrlichen Erfahrungen**.\n",
    "\n",
    "Stellen Sie sich zum Beispiel vor, Sie setzen Ihren kleinen Bruder vor ein Videospiel, das er noch nie gespielt hat, geben ihm einen Controller und lassen ihn allein.\n",
    "\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/Illustration_1.jpg\" alt=\"Illustration_1\" width=\"100%\">\n",
    "\n",
    "Ihr Bruder interagiert mit der Umgebung (dem Videospiel), indem er die richtige Taste (Aktion) dr√ºckt. Er hat eine M√ºnze bekommen, das ist eine +1 Belohnung. Das ist positiv, er hat gerade verstanden, dass er in diesem Spiel **die M√ºnzen bekommen muss.**\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/Illustration_2.jpg\" alt=\"Illustration_2\" width=\"100%\">\n",
    "\n",
    "Aber dann **dr√ºckt er wieder die rechte Taste** und ber√ºhrt einen Gegner. Er ist gerade gestorben, also ist das eine -1 Belohnung.\n",
    "\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/Illustration_3.jpg\" alt=\"Abbildung_3\" width=\"100%\">\n",
    "\n",
    "Indem er durch Ausprobieren mit seiner Umgebung interagiert, versteht dein kleiner Bruder, dass er in dieser Umgebung **M√ºnzen bekommen, aber die Feinde meiden muss**.\n",
    "\n",
    "**Ohne jegliche Aufsicht** wird das Kind das Spiel immer besser beherrschen.\n",
    "\n",
    "So lernen Menschen und Tiere, **durch Interaktion.** Das Verst√§rkungslernen ist nur ein **computergest√ºtzter Ansatz des Lernens aus Handlungen.**\n",
    "\n",
    "\n",
    "### Eine formale Definition\n",
    "\n",
    "Wir k√∂nnen nun eine formale Definition vornehmen:\n",
    "\n",
    "Verst√§rkungslernen ist ein Rahmen f√ºr die L√∂sung von Steuerungsaufgaben (auch Entscheidungsprobleme genannt) durch den Aufbau von Agenten, die von der Umwelt lernen, indem sie mit ihr durch Versuch und Irrtum interagieren und Belohnungen (positiv oder negativ) als einzigartiges Feedback erhalten.\n",
    "\n",
    "Aber wie funktioniert Reinforcement Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Das Reinforcement Learning Framework\n",
    "\n",
    "## Der RL-Prozess\n",
    "<figure>\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/RL_process.jpg\" alt=\"The RL process\" width=\"100%\">\n",
    "<figcaption>The RL Process: a loop of state, action, reward and next state</figcaption>\n",
    "<figcaption>Source: <a href=\"http://incompleteideas.net/book/RLbook2020.pdf\">Reinforcement Learning: An Introduction, Richard Sutton and Andrew G. Barto</a></figcaption>\n",
    "</figure>\n",
    "\n",
    "Um den RL-Prozess zu verstehen, stellen wir uns einen Agenten vor, der lernt, ein Plattformspiel zu spielen:\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/RL_process_game.jpg\" alt=\"Der RL-Prozess\" width=\"100%\">\n",
    "\n",
    "- Unser Agent erh√§lt den **Zustand \\\\(S_0\\\\)** von der **Umgebung** - wir erhalten den ersten Frame unseres Spiels (Umgebung).\n",
    "- Basierend auf diesem **Zustand \\\\(S_0\\\\),** f√ºhrt der Agent eine **Aktion \\\\(A_0\\\\)** aus - unser Agent wird sich nach rechts bewegen.\n",
    "- Die Umgebung geht in einen **neuen** **Zustand \\\\(S_1\\\\)** √ºber - neues Bild.\n",
    "- Die Umgebung gibt dem Agenten eine **Belohnung \\\\(R_1\\\\)** - wir sind nicht tot *(Positive Belohnung +1)*.\n",
    "\n",
    "Diese RL-Schleife gibt eine Sequenz von **Zustand, Aktion, Belohnung und n√§chstem Zustand** aus.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/sars.jpg\" alt=\"State, Action, Reward, Next State\" width=\"100%\">\n",
    "\n",
    "Das Ziel des Agenten ist die _Maximierung_ seiner kumulativen Belohnung, die **erwartete Rendite genannt wird.**\n",
    "\n",
    "## Die Belohnungshypothese: die zentrale Idee des Reinforcement Learning\n",
    "\n",
    "‚áí Warum ist das Ziel des Agenten die Maximierung des erwarteten Gewinns?\n",
    "\n",
    "Weil RL auf der **Belohnungshypothese** basiert, die besagt, dass alle Ziele als **Maximierung des erwarteten Ertrags** (erwartete kumulative Belohnung) beschrieben werden k√∂nnen.\n",
    "\n",
    "Aus diesem Grund versuchen wir beim Reinforcement Learning, **das beste Verhalten** zu erlernen, das **die erwartete kumulative Belohnung** maximiert.\n",
    "\n",
    "\n",
    "## Markov-Eigenschaft\n",
    "\n",
    "In der Literatur wird der RL-Prozess als **Markov Decision Process** (MDP) bezeichnet.\n",
    "\n",
    "Wir werden in den folgenden Einheiten erneut √ºber die Markov-Eigenschaft sprechen. Die Markov-Eigenschaft impliziert, dass unser Agent **nur den aktuellen Zustand ben√∂tigt, um zu entscheiden**, welche Aktion er ausf√ºhren soll, und **nicht die Geschichte aller Zust√§nde und Aktionen**, die er zuvor ausgef√ºhrt hat.\n",
    "\n",
    "## Raum f√ºr Beobachtungen/Zust√§nde\n",
    "\n",
    "Beobachtungen/Zust√§nde sind die **Informationen, die unser Agent aus der Umgebung erh√§lt.** Im Falle eines Videospiels kann es sich um einen Frame (einen Screenshot) handeln. Im Falle des Handelsagenten kann es sich um den Wert einer bestimmten Aktie usw. handeln.\n",
    "\n",
    "Es muss jedoch zwischen *Beobachtung* und *Zustand* unterschieden werden:\n",
    "\n",
    "- *Zustand s*: ist **eine vollst√§ndige Beschreibung des Zustands der Welt** (es gibt keine versteckten Informationen). In einer vollst√§ndig beobachteten Umgebung.\n",
    "\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/chess.jpg\" alt=\"Chess\">\n",
    "<figcaption>In chess game, we receive a state from the environment since we have access to the whole check board information.</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "Bei einer Schachpartie haben wir Zugang zu den Informationen des gesamten Brettes, wir erhalten also einen Zustand aus der Umgebung. Mit anderen Worten, die Umgebung wird vollst√§ndig beobachtet.\n",
    "\n",
    "- *Beobachtung o*: ist eine **teilweise Beschreibung des Zustands.** In einer teilweise beobachteten Umgebung.\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/mario.jpg\" alt=\"Mario\">\n",
    "<figcaption>In Super Mario Bros. sehen wir nur den Teil des Levels, der sich in der N√§he des Spielers befindet, also erhalten wir eine Beobachtung.</figcaption>\n",
    "</figure>\n",
    "\n",
    "In Super Mario Bros. sehen wir nur den Teil des Levels, der sich in der N√§he des Spielers befindet, also erhalten wir eine Beobachtung.\n",
    "\n",
    "In Super Mario Bros. befinden wir uns in einer teilweise beobachteten Umgebung. Wir erhalten eine Beobachtung, **weil wir nur einen Teil des Levels sehen.**\n",
    "\n",
    "In diesem Kurs verwenden wir den Begriff \"Zustand\", um sowohl den Zustand als auch die Beobachtung zu bezeichnen, aber wir werden die Unterscheidung in den Implementierungen machen.\n",
    "\n",
    "Um es noch einmal zusammenzufassen:\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/obs_space_recap.jpg\" alt=\"Obs space recap\" width=\"100%\">\n",
    "\n",
    "\n",
    "## Action Space\n",
    "\n",
    "Der Aktionsraum ist die Menge **aller m√∂glichen Aktionen in einer Umgebung.**\n",
    "\n",
    "Die Aktionen k√∂nnen aus einem *diskreten* oder *kontinuierlichen Raum* stammen:\n",
    "\n",
    "- *Diskreter Raum*: die Anzahl der m√∂glichen Aktionen ist **unendlich**.\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/mario.jpg\" alt=\"Mario\">\n",
    "<figcaption>In Super Mario Bros, we have only 4 possible actions: left, right, up (jumping) and down (crouching).</figcaption>\n",
    "\n",
    "</figure>\n",
    "\n",
    "Auch in Super Mario Bros. haben wir eine endliche Anzahl von Aktionen, da wir nur 4 Richtungen haben.\n",
    "\n",
    "- *Kontinuierlicher Raum*: die Anzahl der m√∂glichen Aktionen ist **unendlich**.\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/self_driving_car.jpg\" alt=\"Self Driving Car\">\n",
    "<figcaption>A Self Driving Car agent has an infinite number of possible actions since it can turn left 20¬∞, 21,1¬∞, 21,2¬∞, honk, turn right 20¬∞‚Ä¶\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Um es noch einmal zusammenzufassen:\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/action_space.jpg\" alt=\"Action space recap\" width=\"100%\">\n",
    "\n",
    "Die Ber√ºcksichtigung dieser Informationen ist von entscheidender Bedeutung, weil sie **bei der Wahl des RL-Algorithmus in der Zukunft von Bedeutung sein werden.\n",
    "\n",
    "## Belohnungen und die Diskontierung\n",
    "\n",
    "Die Belohnung ist im RL von grundlegender Bedeutung, weil sie **die einzige R√ºckmeldung** f√ºr den Agenten ist. Dank ihr wei√ü unser Agent, **ob die durchgef√ºhrte Aktion gut war oder nicht**.\n",
    "\n",
    "Die kumulative Belohnung zu jedem Zeitschritt **t** kann wie folgt geschrieben werden:\n",
    "<figure>\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/rewards_1.jpg\" alt=\"Rewards\">\n",
    "<figcaption>The cumulative reward equals the sum of all rewards in the sequence.\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Was gleichbedeutend ist mit:\n",
    "\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/rewards_2.jpg\" alt=\"Rewards\">\n",
    "<figcaption>The cumulative reward = rt+1 (rt+k+1 = rt+0+1 = rt+1)+ rt+2 (rt+k+1 = rt+1+1 = rt+2) + ...\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "In der Realit√§t k√∂nnen wir sie jedoch **nicht einfach so addieren.** Die Belohnungen, die fr√ºher (zu Beginn des Spiels) kommen, **sind wahrscheinlicher**, da sie besser vorhersehbar sind als die langfristige zuk√ºnftige Belohnung.\n",
    "\n",
    "Nehmen wir an, dein Agent ist diese kleine Maus, die sich in jedem Zeitschritt um ein Pl√§ttchen bewegen kann, und dein Gegner ist die Katze (die sich ebenfalls bewegen kann). Das Ziel der Maus ist es, **die maximale Menge an K√§se zu essen, bevor sie von der Katze gefressen wird**.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/rewards_3.jpg\" alt=\"Belohnungen\" width=\"100%\">\n",
    "\n",
    "Wie wir im Diagramm sehen k√∂nnen, **ist es wahrscheinlicher, den K√§se in unserer N√§he zu essen als den K√§se in der N√§he der Katze** (je n√§her wir der Katze sind, desto gef√§hrlicher ist sie).\n",
    "\n",
    "Folglich wird **die Belohnung in der N√§he der Katze, auch wenn sie gr√∂√üer ist (mehr K√§se), st√§rker abgewertet**, da wir nicht sicher sind, ob wir sie essen k√∂nnen.\n",
    "\n",
    "Um die Belohnungen zu diskontieren, gehen wir wie folgt vor:\n",
    "\n",
    "1. Wir definieren einen Abzinsungssatz namens Gamma. **Er muss zwischen 0 und 1 liegen**. Meistens liegt er zwischen **0,95 und 0,99**.\n",
    "- Je gr√∂√üer das Gamma, desto kleiner der Abschlag. Das bedeutet, dass unser Agent **sich mehr um die langfristige Belohnung k√ºmmert**.\n",
    "- Andererseits, je kleiner das Gamma, desto gr√∂√üer der Abschlag. Das bedeutet, dass unser **Agent sich mehr um die kurzfristige Belohnung (den n√§chsten K√§se) k√ºmmert.**\n",
    "\n",
    "2. Dann wird jede Belohnung um gamma mit dem Exponenten des Zeitschritts abgezinst. Je gr√∂√üer der Zeitschritt, desto n√§her kommt die Katze, **so dass die zuk√ºnftige Belohnung immer unwahrscheinlicher wird.**\n",
    "\n",
    "Unsere diskontierte erwartete kumulative Belohnung ist:\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/rewards_4.jpg\" alt=\"Rewards\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Der Exploration/Exploitation-Tradeoff\n",
    "\n",
    "Bevor wir uns mit den verschiedenen Methoden zur L√∂sung von Reinforcement-Learning-Problemen befassen, m√ºssen wir noch ein weiteres sehr wichtiges Thema behandeln: Der *Exploration/Exploitation Trade-off*.\n",
    "\n",
    "- *Exploration* ist das Erforschen der Umgebung durch das Ausprobieren zuf√§lliger Aktionen, um **mehr Informationen √ºber die Umgebung zu finden.**\n",
    "- *Ausbeutung* ist die **Ausbeutung bekannter Informationen zur Maximierung der Belohnung**.\n",
    "\n",
    "Denken Sie daran, dass das Ziel unseres RL-Agenten darin besteht, die erwartete kumulative Belohnung zu maximieren. Wir k√∂nnen jedoch **in eine h√§ufige Falle tappen**.\n",
    "\n",
    "Nehmen wir ein Beispiel:\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/exp_1.jpg\" alt=\"Exploration\" width=\"100%\">\n",
    "\n",
    "In diesem Spiel kann unsere Maus eine **unendliche Anzahl von kleinen K√§sen** (je +1) haben. Aber am oberen Ende des Labyrinths gibt es eine gigantische Menge K√§se (+1000).\n",
    "\n",
    "Wenn wir uns jedoch nur auf die Ausbeutung konzentrieren, wird unser Agent niemals die gigantische K√§sesumme erreichen. Stattdessen wird er nur **die n√§chstgelegene Quelle von Belohnungen** ausbeuten, selbst wenn diese Quelle klein ist (Ausbeutung).\n",
    "\n",
    "Wenn unser Agent jedoch ein wenig erkundet, kann er **die gro√üe Belohnung** (den Haufen des gro√üen K√§ses) entdecken.\n",
    "\n",
    "Dies nennen wir den Kompromiss zwischen Erkundung und Ausbeutung. We need to balance how much we **explore the environment** and how much we **exploit what we know about the environment.**\n",
    "\n",
    "Deshalb m√ºssen wir **eine Regel definieren, die uns hilft, mit diesem Kompromiss umzugehen**. Wir werden die verschiedenen M√∂glichkeiten, damit umzugehen, in den n√§chsten Einheiten sehen.\n",
    "\n",
    "Wenn es immer noch verwirrend ist, **denke an ein echtes Problem: die Wahl eines Restaurants:**\n",
    "\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/exp_2.jpg\" alt=\"Exploration\">\n",
    "<figcaption>Source: <a href=\"https://inst.eecs.berkeley.edu/~cs188/sp20/assets/lecture/lec15_6up.pdf\"> Berkley AI Course</a>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "- *Ausbeutung*: Man geht jeden Tag in das gleiche Restaurant, von dem man wei√ü, dass es gut ist, und **geht das Risiko ein, ein anderes, besseres Restaurant zu verpassen*.\n",
    "- *Exploration*: Probieren Sie Restaurants aus, in denen Sie noch nie waren, mit dem Risiko, eine schlechte Erfahrung zu machen, **aber mit der wahrscheinlichen Chance auf eine fantastische Erfahrung.**\n",
    "\n",
    "To recap:\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/expexpltradeoff.jpg\" alt=\"Exploration Exploitation Tradeoff\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zwei Hauptans√§tze zur L√∂sung von RL-Problemen \n",
    "\n",
    "Mit anderen Worten, wie bauen wir einen RL-Agenten, der **die Aktionen ausw√§hlen kann, die seine erwartete kumulative Belohnung maximieren?**\n",
    "\n",
    "## Die Policy œÄ: das Gehirn des Agenten\n",
    "\n",
    "Die Policy **œÄ** ist das **Gehirn unseres Agenten**, sie ist die Funktion, die uns sagt, welche **Aktion wir angesichts des Zustands, in dem wir uns befinden, ausf√ºhren sollen.** Sie **definiert also das Verhalten des Agenten** zu einem bestimmten Zeitpunkt.\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/policy_1.jpg\" alt=\"Policy\" />\n",
    "<figcaption>Think of policy as the brain of our agent, the function that will tell us the action to take given a state</figcaption>\n",
    "</figure>\n",
    "\n",
    "Diese Policy **ist die Funktion, die wir lernen wollen**, unser Ziel ist es, die optimale Policy œÄ\\* zu finden, die Policy, die **den erwarteten Ertrag** maximiert, wenn der Agent danach handelt. Dieses œÄ\\* finden wir **durch Training**.\n",
    "\n",
    "Es gibt zwei Ans√§tze, um unseren Agenten darauf zu trainieren, diese optimale Strategie œÄ\\* zu finden:\n",
    "\n",
    "- **Direkt,** indem wir dem Agenten beibringen, zu lernen, welche **Handlung er angesichts des aktuellen Zustands ergreifen sollte,**: **Policy-Based Methods.**\n",
    "- Indirekt **lernen die Agenten, welcher Zustand wertvoller ist** und ergreifen dann die Ma√ünahmen, die **zu den wertvolleren Zust√§nden** f√ºhren: Wertbasierte Methoden.\n",
    "\n",
    "## Policy basierte Methoden\n",
    "\n",
    "Bei richtlinienbasierten Methoden **lernen wir direkt eine Richtlinienfunktion**.\n",
    "\n",
    "Diese Funktion definiert eine Abbildung von jedem Zustand auf die beste entsprechende Aktion. Alternativ kann sie auch **eine Wahrscheinlichkeitsverteilung √ºber die Menge der m√∂glichen Aktionen in diesem Zustand** definieren.\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/policy_2.jpg\" alt=\"Policy\" />\n",
    "<figcaption>As we can see here, the policy (deterministic) <b>directly indicates the action to take for each step.</b></figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "Es gibt zwei Arten von Policy:\n",
    "\n",
    "\n",
    "- *Deterministisch*: eine Policy in einem bestimmten Zustand **liefert immer die gleiche Aktion**.\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/policy_3.jpg\" alt=\"Policy\"/>\n",
    "<figcaption>action = policy(state)</figcaption>\n",
    "</figure>\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/policy_4.jpg\" alt=\"Policy\" width=\"100%\"/>\n",
    "\n",
    "- *Stochastisch*: gibt **eine Wahrscheinlichkeitsverteilung √ºber Aktionen aus.\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/policy_5.jpg\" alt=\"Policy\"/>\n",
    "<figcaption>Policy(Aktionen | Zustand) = Wahrscheinlichkeitsverteilung √ºber die Menge der Aktionen bei aktuellem Zustand</figcaption>\n",
    "</figure>\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/policy-based.png\" alt=\"Policy Based\"/>\n",
    "<figcaption>Ausgehend von einem Anfangszustand gibt unsere stochastische Strategie Wahrscheinlichkeitsverteilungen √ºber die m√∂glichen Aktionen in diesem Zustand aus.</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "Wenn wir rekapitulieren:\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/pbm_1.jpg\" alt=\"Pbm recap\" width=\"100%\" />\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/pbm_2.jpg\" alt=\"Pbm rekapitulieren\" width=\"100%\" />\n",
    "\n",
    "\n",
    "## Wertbasierte Methoden\n",
    "\n",
    "Bei wertbasierten Methoden lernen wir statt einer Policyfunktion eine **Wertfunktion**, die einen Zustand auf den erwarteten Wert **dieses Zustands abbildet.**\n",
    "\n",
    "Der Wert eines Zustands ist der **erwartete diskontierte Ertrag**, den der Agent erhalten kann, wenn er **in diesem Zustand beginnt und dann gem√§√ü unserer Strategie handelt**.\n",
    "\n",
    "\"Gem√§√ü unserer Policy handeln\" bedeutet lediglich, dass unsere Policy **\"den Zustand mit dem h√∂chsten Wert anstrebt\".**\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/value_1.jpg\" alt=\"Wertbasierte RL\" width=\"100%\" />\n",
    "\n",
    "Hier sehen wir, dass unsere Wertfunktion **f√ºr jeden m√∂glichen Zustand Werte definiert**.\n",
    "\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/value_2.jpg\" alt=\"Value based RL\"/>\n",
    "\n",
    "Dank unserer Wertfunktion wird unsere Strategie bei jedem Schritt den Zustand mit dem gr√∂√üten Wert ausw√§hlen, der durch die Wertfunktion definiert ist: -7, dann -6, dann -5 (und so weiter), um das Ziel zu erreichen.\n",
    "\n",
    "Wenn wir rekapitulieren:\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/vbm_1.jpg\" alt=\"Vbm recap\" width=\"100%\" />\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/vbm_2.jpg\" alt=\"Vbm recap\" width=\"100%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HoeqMnr5LuYE"
   },
   "source": [
    "## Eine kleine Zusammenfassung von Deep Reinforcement Learning üìö\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/RL_process_game.jpg\" alt=\"Der RL-Prozess\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcQYx9ynaFMD"
   },
   "source": [
    "Lassen Sie uns kurz rekapitulieren, was wir in der ersten Einheit gelernt haben:\n",
    "\n",
    "- Verst√§rkungslernen ist ein **computergest√ºtzter Ansatz zum Lernen aus Handlungen**. Wir bauen einen Agenten, der von der Umwelt lernt, indem er **durch Versuch und Irrtum** mit ihr interagiert und Belohnungen (negativ oder positiv) als Feedback erh√§lt.\n",
    "\n",
    "- Das Ziel eines jeden RL-Agenten ist die **Maximierung seiner erwarteten kumulativen Belohnung** (auch erwarteter Ertrag genannt), da RL auf der _Belohnungshypothese_ basiert, die besagt, dass alle Ziele als Maximierung einer erwarteten kumulativen Belohnung beschrieben werden k√∂nnen.\n",
    "\n",
    "- Der RL-Prozess ist eine **Schleife, die eine Folge von Zustand, Aktion, Belohnung und n√§chstem Zustand** ausgibt.\n",
    "\n",
    "- Um die erwartete kumulative Belohnung (erwartete Rendite) zu berechnen, **diskontieren wir die Belohnungen**: Die Belohnungen, die fr√ºher (zu Beginn des Spiels) eintreten, sind wahrscheinlicher, da sie besser vorhersehbar sind als die langfristige zuk√ºnftige Belohnung.\n",
    "\n",
    "- Um ein RL-Problem zu l√∂sen, wollen Sie **eine optimale Strategie** finden; die Strategie ist das \"Gehirn\" Ihrer KI, das uns sagt, welche Aktion wir in einem bestimmten Zustand ergreifen sollen. Die optimale Strategie ist diejenige, die Ihnen die Aktionen liefert, die den erwarteten Gewinn maximieren.\n",
    "\n",
    "Es gibt **zwei** M√∂glichkeiten, die optimale Strategie zu finden:\n",
    "1. Durch **direktes Trainieren Ihrer Strategie**: Strategie-basierte Methoden.\n",
    "2. Durch **Trainieren einer Wertfunktion**, die uns den erwarteten Ertrag angibt, den der Agent in jedem Zustand erh√§lt, und Verwendung dieser Funktion zur Festlegung unserer Strategie: wertbasierte Methoden.\n",
    "\n",
    "\n",
    "- Schlie√ülich haben wir √ºber Deep RL gesprochen, weil **wir tiefe neuronale Netze einsetzen, um die zu ergreifenden Ma√ünahmen (policy-basiert) oder den Wert eines Zustands (wertbasiert) zu sch√§tzen, daher der Name \"deep\" **."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qDploC3jSH99"
   },
   "source": [
    "# Trainieren wir unseren ersten Deep Reinforcement Learning-Agenten und laden wir ihn in den Hub hoch üöÄ.\n",
    "\n",
    "## Get a certificate üéì\n",
    "\n",
    "Um dieses Hands-On f√ºr den [Zertifizierungsprozess] (https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process) zu validieren, m√ºssen Sie Ihr trainiertes Modell zum Hub hochladen und **ein Ergebnis von >= 200** erhalten.\n",
    "\n",
    "Um Ihr Ergebnis zu finden, gehen Sie zur [Bestenliste](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) und suchen Sie Ihr Modell, **das Ergebnis = mean_reward - std of reward**\n",
    "\n",
    "Weitere Informationen √ºber den Zertifizierungsprozess finden Sie in diesem Abschnitt üëâ https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jeDAH0h0EBiG"
   },
   "source": [
    "## Abh√§ngigkeiten installieren und einen virtuellen Bildschirm erstellen üîΩ.\n",
    "\n",
    "Der erste Schritt ist die Installation der Abh√§ngigkeiten, wir werden mehrere installieren.\n",
    "\n",
    "- `gymnasium[box2d]`: Enth√§lt die LunarLander-v2-Umgebung üåõ.\n",
    "- `stable-baselines3[extra]`: Die Deep Reinforcement Learning-Bibliothek.\n",
    "- `huggingface_sb3`: Zus√§tzlicher Code f√ºr Stable-baselines3 zum Laden und Hochladen von Modellen aus dem Hugging Face ü§ó Hub.\n",
    "\n",
    "Um die Dinge zu vereinfachen, haben wir ein Skript erstellt, das all diese Abh√§ngigkeiten installiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yQIGLPDkGhgG"
   },
   "outputs": [],
   "source": [
    "!apt install swig cmake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9XaULfDZDvrC"
   },
   "outputs": [],
   "source": [
    "!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BEKeXQJsQCYm"
   },
   "source": [
    "W√§hrend der Arbeit mit dem Notebook m√ºssen wir ein Replay-Video erstellen. Dazu ben√∂tigen wir mit colab **einen virtuellen Bildschirm, um die Umgebung zu rendern** (und somit die Bilder aufzunehmen).\n",
    "\n",
    "Daher wird die folgende Zelle die Bibliotheken f√ºr virtuelle Bildschirme installieren und einen virtuellen Bildschirm erstellen und ausf√ºhren üñ•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j5f2cGkdP-mb"
   },
   "outputs": [],
   "source": [
    "!sudo apt-get update\n",
    "!sudo apt-get install -y python3-opengl\n",
    "!apt install ffmpeg\n",
    "!apt install xvfb\n",
    "!pip3 install pyvirtualdisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCwBTAwAW9JJ"
   },
   "source": [
    "Um sicherzustellen, dass die neu installierten Bibliotheken verwendet werden, **ist es manchmal erforderlich, die Laufzeit des Notebooks neu zu starten**. Die n√§chste Zelle wird die **Laufzeitumgebung zum Absturz bringen, so dass Sie eine neue Verbindung herstellen und den Code von hier aus ausf√ºhren m√ºssen**. Dank dieses Tricks **k√∂nnen wir unseren virtuellen Bildschirm ausf√ºhren**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cYvkbef7XEMi"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BE5JWP5rQIKf"
   },
   "outputs": [],
   "source": [
    "# Virtual display\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "virtual_display = Display(visible=0, size=(1400, 900))\n",
    "virtual_display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wrgpVFqyENVf"
   },
   "source": [
    "## Importieren Sie die Pakete üì¶\n",
    "\n",
    "Eine zus√§tzliche Bibliothek, die wir importieren, ist huggingface_hub **um trainierte Modelle aus dem Hub hoch- und herunterladen zu k√∂nnen**.\n",
    "\n",
    "\n",
    "Der Hugging Face Hub ü§ó funktioniert als zentraler Ort, an dem jeder Modelle und Datens√§tze teilen und erforschen kann. Er bietet Versionierung, Metriken, Visualisierungen und andere Funktionen, die eine einfache Zusammenarbeit mit anderen erm√∂glichen.\n",
    "\n",
    "Sie k√∂nnen hier alle Deep Reinforcement Learning-Modelle sehen, die hier verf√ºgbar sindüëâ https://huggingface.co/models?pipeline_tag=reinforcement-learning&sort=downloads\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cygWLPGsEQ0m"
   },
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "\n",
    "from huggingface_sb3 import load_from_hub, package_to_hub\n",
    "from huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRqRuRUl8CsB"
   },
   "source": [
    "## Verstehen Sie das Gymnasium und wie es funktioniert ü§ñ\n",
    "\n",
    "üèã Die Bibliothek, die unsere Umgebung enth√§lt, hei√üt Gymnasium.\n",
    "**Sie werden Gymnasium in Deep Reinforcement Learning h√§ufig verwenden.\n",
    "\n",
    "Gymnasium ist die **neue Version der Gym-Bibliothek** [die von der Farama Foundation gepflegt wird] (https://farama.org/).\n",
    "\n",
    "Die Gymnasium-Bibliothek bietet zwei Dinge:\n",
    "\n",
    "- Eine Schnittstelle, die es erlaubt, **RL-Umgebungen** zu erstellen.\n",
    "- Eine **Sammlung von Umgebungen** (gym-control, atari, box2D...).\n",
    "\n",
    "Schauen wir uns ein Beispiel an, aber erinnern wir uns zun√§chst an die RL-Schleife.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/RL_process_game.jpg\" alt=\"Der RL-Prozess\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-TzNN0bQ_j-3"
   },
   "source": [
    "Bei jedem Schritt:\n",
    "- Unser Agent erh√§lt einen **Zustand (S0)** von der **Umgebung** - wir erhalten das erste Bild unseres Spiels (Umgebung).\n",
    "- Basierend auf diesem **Zustand (S0),** f√ºhrt der Agent eine **Aktion (A0)** aus - unser Agent bewegt sich nach rechts.\n",
    "- Die Umgebung geht in einen **neuen** **Zustand (S1)** √ºber - neues Bild.\n",
    "- Die Umgebung gibt dem Agenten eine **Belohnung (R1)** - wir sind nicht tot *(Positive Belohnung +1)*.\n",
    "\n",
    "\n",
    "Mit Gymnasium:\n",
    "\n",
    "1Ô∏è‚É£ Wir erstellen unsere Umgebung mit `gymnasium.make()`\n",
    "\n",
    "2Ô∏è‚É£ Wir setzen die Umgebung mit `observation = env.reset()` auf ihren Ausgangszustand zur√ºck.\n",
    "\n",
    "Bei jedem Schritt:\n",
    "\n",
    "3Ô∏è‚É£ Holen Sie sich eine Aktion mit unserem Modell (in unserem Beispiel nehmen wir eine zuf√§llige Aktion)\n",
    "\n",
    "4Ô∏è‚É£ Mit `env.step(action)` f√ºhren wir diese Aktion in der Umgebung aus und erhalten\n",
    "- Beobachtung\": Der neue Zustand (st+1)\n",
    "- `Belohnung`: Die Belohnung, die wir nach dem Ausf√ºhren der Aktion erhalten\n",
    "- Beendet`: Zeigt an, ob die Episode beendet wurde (der Agent hat den Endzustand erreicht)\n",
    "- Abgeschnitten\": Mit dieser neuen Version eingef√ºhrt, zeigt es ein Zeitlimit an oder wenn ein Agent zum Beispiel die Grenzen der Umgebung verl√§sst.\n",
    "- `Info`: Ein W√∂rterbuch, das zus√§tzliche Informationen liefert (abh√§ngig von der Umgebung).\n",
    "\n",
    "F√ºr weitere Erkl√§rungen siehe dies üëâ https://gymnasium.farama.org/api/env/#gymnasium.Env.step\n",
    "\n",
    "Wenn die Episode beendet ist:\n",
    "- Wir setzen die Umgebung mit `observation = env.reset()` auf ihren Ausgangszustand zur√ºck.\n",
    "\n",
    "**Schauen wir uns ein Beispiel an!** Achten Sie darauf, den Code zu lesen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w7vOFlpA_ONz"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# First, we create our environment called LunarLander-v2\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "# Then we reset this environment\n",
    "observation, info = env.reset()\n",
    "\n",
    "for _ in range(20):\n",
    "  # Take a random action\n",
    "  action = env.action_space.sample()\n",
    "  print(\"Action taken:\", action)\n",
    "\n",
    "  # Do this action in the environment and get\n",
    "  # next_state, reward, terminated, truncated and info\n",
    "  observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "  # If the game is terminated (in our case we land, crashed) or truncated (timeout)\n",
    "  if terminated or truncated:\n",
    "      # Reset the environment\n",
    "      print(\"Environment is reset\")\n",
    "      observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XIrKGGSlENZB"
   },
   "source": [
    "## Die LunarLander-Umgebung erstellen üåõ und verstehen, wie sie funktioniert\n",
    "\n",
    "### [Die Umgebung üéÆ](https://gymnasium.farama.org/environments/box2d/lunar_lander/)\n",
    "\n",
    "In diesem ersten Tutorial trainieren wir unseren Agenten, einen [Lunar Lander](https://gymnasium.farama.org/environments/box2d/lunar_lander/), **um korrekt auf dem Mond zu landen**. Dazu muss der Agent lernen, **seine Geschwindigkeit und Position (horizontal, vertikal und im Winkel) anzupassen, um korrekt zu landen**.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "üí° Eine gute Angewohnheit, wenn man anf√§ngt, eine Umgebung zu benutzen, ist es, ihre Dokumentation zu √ºberpr√ºfen.\n",
    "\n",
    "üëâ https://gymnasium.farama.org/environments/box2d/lunar_lander/\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "poLBgRocF9aT"
   },
   "source": [
    "Schauen wir mal, wie die Umwelt aussieht:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZNPG0g_UGCfh"
   },
   "outputs": [],
   "source": [
    "# We create our environment with gym.make(\"<name_of_the_environment>\")\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "env.reset()\n",
    "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
    "print(\"Observation Space Shape\", env.observation_space.shape)\n",
    "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2MXc15qFE0M9"
   },
   "source": [
    "Mit \"Observation Space Shape (8,)\" sehen wir, dass die Beobachtung ein Vektor der Gr√∂√üe 8 ist, wobei jeder Wert verschiedene Informationen √ºber den Lander enth√§lt:\n",
    "- Horizontale Pad-Koordinate (x)\n",
    "- Vertikale Pad-Koordinate (y)\n",
    "- Horizontale Geschwindigkeit (x)\n",
    "- Vertikale Geschwindigkeit (y)\n",
    "- Winkel\n",
    "- Geschwindigkeit im Winkel\n",
    "- Wenn der Kontaktpunkt des linken Beins das Land ber√ºhrt hat (boolesch)\n",
    "- Wenn der Kontaktpunkt des rechten Beins das Land ber√ºhrt hat (boolesch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "We5WqOBGLoSm"
   },
   "outputs": [],
   "source": [
    "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
    "print(\"Action Space Shape\", env.action_space.n)\n",
    "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MyxXwkI2Magx"
   },
   "source": [
    "Der Aktionsraum (die Menge der m√∂glichen Aktionen, die der Agent ausf√ºhren kann) ist diskret mit 4 verf√ºgbaren Aktionen üéÆ:\n",
    "\n",
    "- Aktion 0: Nichts tun,\n",
    "- Aktion 1: Linken Orientierungsmotor z√ºnden,\n",
    "- Aktion 2: Z√ºndung des Hauptmotors,\n",
    "- Aktion 3: Rechtes Orientierungstriebwerk z√ºnden.\n",
    "\n",
    "Belohnungsfunktion (die Funktion, die bei jedem Zeitschritt eine Belohnung gibt) üí∞:\n",
    "\n",
    "Nach jedem Schritt wird eine Belohnung gew√§hrt. Die Gesamtbelohnung einer Episode ist die **Summe der Belohnungen f√ºr alle Schritte innerhalb dieser Episode**.\n",
    "\n",
    "F√ºr jeden Schritt gibt es eine Belohnung:\n",
    "\n",
    "- Sie erh√∂ht/verringert sich, je n√§her/weiter der Lander an der Landeplattform ist.\n",
    "- Sie erh√∂ht/verringert sich, je langsamer/schneller sich der Lander bewegt.\n",
    "- Wird verringert, je st√§rker der Lander geneigt ist (Winkel nicht horizontal).\n",
    "- Erh√∂ht sich um 10 Punkte f√ºr jedes Bein, das den Boden ber√ºhrt.\n",
    "- Verringert sich um 0,03 Punkte pro Frame, in dem ein Seitentriebwerk gez√ºndet wird.\n",
    "- Verringert sich um 0,3 Punkte pro Frame, in dem das Haupttriebwerk gez√ºndet wird.\n",
    "\n",
    "Die Episode erh√§lt eine **zus√§tzliche Belohnung von -100 bzw. +100 Punkten f√ºr einen Absturz bzw. eine sichere Landung**.\n",
    "\n",
    "Eine Episode gilt als **gel√∂st, wenn sie mindestens 200 Punkte erreicht hat.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dFD9RAFjG8aq"
   },
   "source": [
    "#### Vektorisierte Umgebung\n",
    "\n",
    "- Wir erstellen eine vektorisierte Umgebung (eine Methode zum Stapeln mehrerer unabh√§ngiger Umgebungen zu einer einzigen Umgebung) mit 16 Umgebungen, damit **wir w√§hrend des Trainings vielf√§ltigere Erfahrungen machen k√∂nnen**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "99hqQ_etEy1N"
   },
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "env = make_vec_env('LunarLander-v2', n_envs=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VgrE86r5E5IK"
   },
   "source": [
    "## Das Modell erstellen ü§ñ\n",
    "- Wir haben unsere Umgebung untersucht und das Problem verstanden: **F√§hig sein, den Lunar Lander korrekt auf dem Landeplatz zu landen, indem wir das linke und rechte Triebwerk sowie das Hauptausrichtungs-Triebwerk steuern**. Jetzt erstellen wir den Algorithmus, den wir zur L√∂sung dieses Problems verwenden werden üöÄ.\n",
    "\n",
    "- Dazu verwenden wir unsere erste Deep-RL-Bibliothek, [Stable Baselines3 (SB3)] (https://stable-baselines3.readthedocs.io/en/master/).\n",
    "\n",
    "- SB3 ist eine Reihe von **zuverl√§ssigen Implementierungen von Verst√§rkungslernalgorithmen in PyTorch**.\n",
    "\n",
    "---\n",
    "\n",
    "üí° Eine gute Angewohnheit bei der Verwendung einer neuen Bibliothek ist es, sich zuerst in die Dokumentation zu vertiefen: https://stable-baselines3.readthedocs.io/en/master/ und dann einige Tutorials auszuprobieren.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HLlClRW37Q7e"
   },
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/sb3.png\" alt=\"Stabile Grundlinien3\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HV4yiUM_9_Ka"
   },
   "source": [
    "Um dieses Problem zu l√∂sen, werden wir SB3 **PPO** verwenden. [PPO (auch bekannt als Proximal Policy Optimization) ist einer der SOTA (State of the Art) Deep Reinforcement Learning-Algorithmen, die Sie in diesem Kurs kennenlernen werden] (https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#example%5D).\n",
    "\n",
    "PPO ist eine Kombination aus:\n",
    "- *Value-based Reinforcement Learning-Methode*: Lernen einer Action-Value-Funktion, die uns die **wertvollste zu ergreifende Aktion bei einem Zustand und einer Aktion** liefert.\n",
    "- Methode des policybasierten Verst√§rkungslernens*: Lernen einer Policy, die uns eine **Wahrscheinlichkeitsverteilung √ºber Aktionen** liefert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5qL_4HeIOrEJ"
   },
   "source": [
    "Stable-Baselines3 ist einfach einzurichten:\n",
    "\n",
    "1Ô∏è‚É£ Sie **erstellen Ihre Umgebung** (in unserem Fall wurde das oben gemacht)\n",
    "\n",
    "2Ô∏è‚É£ Sie definieren das **Modell, das Sie verwenden m√∂chten, und instanziieren dieses Modell** `model = PPO(\"MlpPolicy\")`\n",
    "\n",
    "3Ô∏è‚É£ Sie **trainieren den Agenten** mit `model.learn` und legen die Anzahl der Trainingszeitschritte fest\n",
    "\n",
    "```\n",
    "# Umgebung erstellen\n",
    "env = gym.make('LunarLander-v2')\n",
    "\n",
    "# Den Agenten instanziieren\n",
    "model = PPO('MlpPolicy', env, verbose=1)\n",
    "# Den Agenten trainieren\n",
    "model.learn(total_timesteps=int(2e5))\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nxI6hT1GE4-A"
   },
   "outputs": [],
   "source": [
    "# TODO: Define a PPO MlpPolicy architecture\n",
    "# We use MultiLayerPerceptron (MLPPolicy) because the input is a vector,\n",
    "# if we had frames as input we would use CnnPolicy\n",
    "model ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QAN7B0_HCVZC"
   },
   "source": [
    "#### L√∂sung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "543OHYDfcjK4"
   },
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "# We added some parameters to accelerate the training\n",
    "model = PPO(\n",
    "    policy = 'MlpPolicy',\n",
    "    env = env,\n",
    "    n_steps = 1024,\n",
    "    batch_size = 64,\n",
    "    n_epochs = 4,\n",
    "    gamma = 0.999,\n",
    "    gae_lambda = 0.98,\n",
    "    ent_coef = 0.01,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ClJJk88yoBUi"
   },
   "source": [
    "## Trainieren Sie den PPO-Agenten üèÉ.\n",
    "- Lassen Sie uns unseren Agenten f√ºr 1.000.000 Zeitschritte trainieren, vergessen Sie nicht, die GPU auf Colab zu verwenden. Es wird ungef√§hr ~20min dauern, aber Sie k√∂nnen auch weniger Zeitschritte verwenden, wenn Sie es nur ausprobieren wollen.\n",
    "- Machen Sie w√§hrend des Trainings eine ‚òï Pause, die Sie sich verdient haben ü§ó."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qKnYkNiVp89p"
   },
   "outputs": [],
   "source": [
    "# TODO: Train it for 1,000 timesteps\n",
    "\n",
    "# TODO: Specify file name for model and save the model to file\n",
    "model_name = \"ppo-LunarLander-v2\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1bQzQ-QcE3zo"
   },
   "source": [
    "#### L√∂sung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "poBCy9u_csyR"
   },
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "# Train it for 1,000,000 timesteps\n",
    "model.learn(total_timesteps=1000000)\n",
    "# Save the model\n",
    "model_name = \"ppo-LunarLander-v2\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BY_HuedOoISR"
   },
   "source": [
    "## Bewerten Sie den Agenten üìà.\n",
    "- Denken Sie daran, die Umgebung in einen [Monitor](https://stable-baselines3.readthedocs.io/en/master/common/monitor.html) zu verpacken.\n",
    "- Nun, da unser Lunar Lander-Agent trainiert ist üöÄ, m√ºssen wir seine Leistung **pr√ºfen**.\n",
    "- Stable-Baselines3 bietet daf√ºr eine Methode: `evaluate_policy`.\n",
    "- Um diesen Teil auszuf√ºllen, m√ºssen Sie [in der Dokumentation nachsehen] (https://stable-baselines3.readthedocs.io/en/master/guide/examples.html#basic-usage-training-saving-loading)\n",
    "- Im n√§chsten Schritt werden wir sehen, **wie Sie Ihren Agenten automatisch bewerten und freigeben k√∂nnen, um in einer Rangliste zu konkurrieren, aber jetzt machen wir es erst einmal selbst**\n",
    "\n",
    "\n",
    "üí° Wenn Sie Ihren Agenten bewerten, sollten Sie nicht Ihre Trainingsumgebung verwenden, sondern eine Bewertungsumgebung erstellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yRpno0glsADy"
   },
   "outputs": [],
   "source": [
    "# TODO: Evaluate the agent\n",
    "# Create a new environment for evaluation\n",
    "eval_env =\n",
    "\n",
    "# Evaluate the model with 10 evaluation episodes and deterministic=True\n",
    "mean_reward, std_reward =\n",
    "\n",
    "# Print the results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BqPKw3jt_pG5"
   },
   "source": [
    "#### L√∂sung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zpz8kHlt_a_m"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "eval_env = Monitor(gym.make(\"LunarLander-v2\"))\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
    "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "reBhoODwcXfr"
   },
   "source": [
    "- In meinem Fall habe ich nach dem Training von 1 Million Schritten eine mittlere Belohnung von 200,20 +/- 20,80 erhalten, was bedeutet, dass unser Mondlande-Agent bereit ist, auf dem Mond zu landen üåõü•≥."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generiere ein gif von deinem Mondlandung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import imageio\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\")\n",
    "images = []\n",
    "obs, info = env.reset()\n",
    "img = env.render()\n",
    "for i in range(350):\n",
    "    images.append(img)\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, _, _ ,_, _ = env.step(action)\n",
    "    img = env.render()\n",
    "\n",
    "imageio.mimsave('lander_ppo.gif', [np.array(img) for i, img in enumerate(images) if i%2 == 0], duration=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lander_ppo.gif','rb') as f:\n",
    "    display(Image(data=f.read(), format='png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IK_kR78NoNb2"
   },
   "source": [
    "## Ver√∂ffentliche unser trainiertes Modell auf dem Hub üî•\n",
    "Da wir nun gesehen haben, dass wir nach dem Training gute Ergebnisse erzielt haben, k√∂nnen wir unser trainiertes Modell mit einer Zeile Code auf dem Hub ü§ó ver√∂ffentlichen.\n",
    "\n",
    "üìö Die Dokumentation der Bibliotheken üëâ https://github.com/huggingface/huggingface_sb3/tree/main#hugging-face--x-stable-baselines3-v20\n",
    "\n",
    "Hier ist ein Beispiel f√ºr eine Model Card (mit Space Invaders):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gs-Ew7e1gXN3"
   },
   "source": [
    "Mit `package_to_hub` **werten Sie aus, zeichnen ein Replay auf, generieren eine Modellkarte Ihres Agenten und schieben sie zum Hub**.\n",
    "\n",
    "This way:\n",
    "- Sie k√∂nnen **unsere Arbeit vorf√ºhren** üî•.\n",
    "- Sie k√∂nnen **Ihren Agenten beim Spielen visualisieren** üëÄ\n",
    "- Du kannst **einen Agenten mit der Community teilen, den andere benutzen k√∂nnen** üíæ\n",
    "- Sie k√∂nnen **auf eine Bestenliste üèÜ zugreifen, um zu sehen, wie gut Ihr Agent im Vergleich zu Ihren Klassenkameraden abschneidet** üëâ https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JquRrWytA6eo"
   },
   "source": [
    "Um Ihr Modell mit der Gemeinschaft teilen zu k√∂nnen, sind drei weitere Schritte erforderlich:\n",
    "\n",
    "1Ô∏è‚É£ (Falls noch nicht geschehen) erstelle ein Konto auf Hugging Face ‚û° https://huggingface.co/join\n",
    "\n",
    "2Ô∏è‚É£ Melde dich an und speichere dann dein Authentifizierungs-Token von der Hugging Face-Website.\n",
    "- Erstellen Sie ein neues Token (https://huggingface.co/settings/tokens) **mit Schreibrolle**\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/create-token.jpg\" alt=\"HF-Token erstellen\">\n",
    "\n",
    "- Kopieren Sie das Token\n",
    "- F√ºhren Sie die Zelle unten aus und f√ºgen Sie das Token ein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GZiFBBlzxzxY"
   },
   "outputs": [],
   "source": [
    "notebook_login()\n",
    "!git config --global credential.helper store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_tsf2uv0g_4p"
   },
   "source": [
    "Wenn Sie kein Google Colab oder ein Jupyter Notebook verwenden m√∂chten, m√ºssen Sie stattdessen diesen Befehl verwenden: `huggingface-cli login`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FGNh9VsZok0i"
   },
   "source": [
    "3Ô∏è‚É£ Wir sind jetzt bereit, unseren trainierten Agenten mit der Funktion `package_to_hub()` an den ü§ó Hub üî• zu √ºbertragen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ay24l6bqFF18"
   },
   "source": [
    "F√ºllen wir die Funktion `package_to_hub`:\n",
    "- model\": unser trainiertes Modell.\n",
    "- model_name\": der Name des trainierten Modells, den wir in \"model_save\" definiert haben\n",
    "- model_architecture`: die verwendete Modellarchitektur, in unserem Fall PPO\n",
    "- env_id`: der Name der Umgebung, in unserem Fall `LunarLander-v2`\n",
    "- eval_env`: die in eval_env definierte Auswertungsumgebung\n",
    "- repo_id`: der Name des Hugging Face Hub Repository, das erstellt/aktualisiert werden soll `(repo_id = {Benutzername}/{repo_name})`\n",
    "\n",
    "üí° **Ein guter Name ist {username}/{model_architecture}-{env_id}**\n",
    "\n",
    "- `commit_message`: Nachricht der √úbergabe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I2E--IJu8JYq"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "from huggingface_sb3 import package_to_hub\n",
    "\n",
    "# PLACE the variables you've just defined two cells above\n",
    "# Define the name of the environment\n",
    "env_id = \"LunarLander-v2\"\n",
    "\n",
    "# TODO: Define the model architecture we used\n",
    "model_architecture = \"PPO\"\n",
    "\n",
    "## Define a repo_id\n",
    "## repo_id is the id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2\n",
    "## CHANGE WITH YOUR REPO ID\n",
    "repo_id = \"ThomasSimonini/ppo-LunarLander-v2\" # Change with your repo id, you can't push with mine üòÑ\n",
    "\n",
    "## Define the commit message\n",
    "commit_message = \"Upload PPO LunarLander-v2 trained agent\"\n",
    "\n",
    "# Create the evaluation env and set the render_mode=\"rgb_array\"\n",
    "eval_env = DummyVecEnv([lambda: gym.make(env_id, render_mode=\"rgb_array\")])\n",
    "\n",
    "# PLACE the package_to_hub function you've just filled here\n",
    "package_to_hub(model=model, # Our trained model\n",
    "               model_name=model_name, # The name of our trained model\n",
    "               model_architecture=model_architecture, # The model architecture we used: in our case PPO\n",
    "               env_id=env_id, # Name of the environment\n",
    "               eval_env=eval_env, # Evaluation Environment\n",
    "               repo_id=repo_id, # id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2\n",
    "               commit_message=commit_message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T79AEAWEFIxz"
   },
   "source": [
    "Herzlichen Gl√ºckwunsch ü•≥ Sie haben gerade Ihren ersten Deep Reinforcement Learning-Agenten trainiert und hochgeladen. Das obige Skript sollte einen Link zu einem Modell-Repository wie https://huggingface.co/osanseviero/test_sb3 angezeigt haben. Wenn Sie auf diesen Link gehen, k√∂nnen Sie:\n",
    "* Auf der rechten Seite eine Videovorschau Ihres Agenten sehen.\n",
    "* Klicken Sie auf \"Dateien und Versionen\", um alle Dateien im Repository zu sehen.\n",
    "* Klicken Sie auf \"Use in stable-baselines3\", um einen Codeausschnitt zu erhalten, der zeigt, wie das Modell geladen wird.\n",
    "* Eine Modellkarte (Datei `README.md`), die eine Beschreibung des Modells enth√§lt\n",
    "\n",
    "Unter der Haube verwendet der Hub git-basierte Repositories (keine Sorge, wenn Sie nicht wissen, was git ist), was bedeutet, dass Sie das Modell mit neuen Versionen aktualisieren k√∂nnen, wenn Sie experimentieren und Ihren Agenten verbessern.\n",
    "\n",
    "Vergleiche die Ergebnisse deines LunarLander-v2 mit deinen Klassenkameraden mit Hilfe des Leaderboards üèÜ üëâ https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9nWnuQHRfFRa"
   },
   "source": [
    "## Laden Sie ein gespeichertes LunarLander-Modell aus dem Hub ü§ó.\n",
    "Danke an [ironbar](https://github.com/ironbar) f√ºr den Beitrag.\n",
    "\n",
    "Das Laden eines gespeicherten Modells aus dem Hub ist wirklich einfach.\n",
    "\n",
    "Gehen Sie auf https://huggingface.co/models?library=stable-baselines3, um die Liste aller gespeicherten Stable-baselines3-Modelle zu sehen.\n",
    "1. Sie w√§hlen eines aus und kopieren seine repo_id\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit1/copy-id.png\" alt=\"Copy-id\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hNPLJF2bfiUw"
   },
   "source": [
    "2. Dann m√ºssen wir nur load_from_hub mit verwenden:\n",
    "- Die Repo_id\n",
    "- dem Dateinamen: dem gespeicherten Modell innerhalb des Repo und seiner Erweiterung (*.zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhb9-NtsinKB"
   },
   "source": [
    "Da das Modell, das ich vom Hub herunterlade, mit Gym (der fr√ºheren Version von Gymnasium) trainiert wurde, m√ºssen wir shimmy installieren, ein API-Konvertierungstool, das uns helfen wird, die Umgebung korrekt auszuf√ºhren.\n",
    "\n",
    "Shimmy Dokumentation: https://github.com/Farama-Foundation/Shimmy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "03WI-bkci1kH"
   },
   "outputs": [],
   "source": [
    "!pip install shimmy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oj8PSGHJfwz3"
   },
   "outputs": [],
   "source": [
    "from huggingface_sb3 import load_from_hub\n",
    "repo_id = \"Classroom-workshop/assignment2-omar\" # The repo_id\n",
    "filename = \"ppo-LunarLander-v2.zip\" # The model filename.zip\n",
    "\n",
    "# When the model was trained on Python 3.8 the pickle protocol is 5\n",
    "# But Python 3.6, 3.7 use protocol 4\n",
    "# In order to get compatibility we need to:\n",
    "# 1. Install pickle5 (we done it at the beginning of the colab)\n",
    "# 2. Create a custom empty object we pass as parameter to PPO.load()\n",
    "custom_objects = {\n",
    "            \"learning_rate\": 0.0,\n",
    "            \"lr_schedule\": lambda _: 0.0,\n",
    "            \"clip_range\": lambda _: 0.0,\n",
    "}\n",
    "\n",
    "checkpoint = load_from_hub(repo_id, filename)\n",
    "model = PPO.load(checkpoint, custom_objects=custom_objects, print_system_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fs0Y-qgPgLUf"
   },
   "source": [
    "Lassen Sie uns dieses Mittel bewerten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PAEVwK-aahfx"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "eval_env = Monitor(gym.make(\"LunarLander-v2\"))\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
    "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQAwLnYFPk-s"
   },
   "source": [
    "## Einige zus√§tzliche Herausforderungen üèÜ\n",
    "Die beste Art zu lernen **ist, Dinge selbst auszuprobieren**! Wie Sie gesehen haben, ist der derzeitige Agent nicht besonders gut. Als ersten Vorschlag k√∂nnen Sie f√ºr mehr Schritte trainieren. Bei 1.000.000 Schritten haben wir tolle Ergebnisse gesehen!\n",
    "\n",
    "In der [Rangliste] (https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) findest du deine Agenten. K√∂nnen Sie sich an die Spitze setzen?\n",
    "\n",
    "Hier sind einige Ideen, um dies zu erreichen:\n",
    "* Trainieren Sie mehr Schritte\n",
    "* Probieren Sie verschiedene Hyperparameter f√ºr `PPO` aus. Sie k√∂nnen sie unter https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#parameters sehen.\n",
    "* √úberpr√ºfen Sie die [Stable-Baselines3-Dokumentation] (https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html) und versuchen Sie ein anderes Modell wie DQN.\n",
    "**Pushen Sie Ihr neu trainiertes Modell** auf den Hub üî•.\n",
    "\n",
    "**Vergleiche die Ergebnisse deines LunarLander-v2 mit deinen Klassenkameraden** mit Hilfe der [Bestenliste](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) üèÜ\n",
    "\n",
    "Ist dir die Mondlandung zu langweilig? Versuche, **die Umgebung zu ver√§ndern**, warum nicht MountainCar-v0, CartPole-v1 oder CarRacing-v0 benutzen? Schau dir an, wie sie funktionieren [mit Hilfe der Gym-Dokumentation] (https://www.gymlibrary.dev/) und hab Spa√ü üéâ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9lM95-dvmif8"
   },
   "source": [
    "________________________________________________________________________\n",
    "Herzlichen Gl√ºckwunsch zum Abschluss dieses Kapitels! Das war das gr√∂√üte, **und es gab eine Menge Informationen.**\n",
    "\n",
    "Wenn du dich immer noch verwirrt f√ºhlst mit all diesen Elementen...das ist v√∂llig normal! **So ging es mir und allen Leuten, die RL studiert haben.\n",
    "\n",
    "Nimm dir Zeit, um den Stoff wirklich zu **verstehen, bevor du weitermachst und die zus√§tzlichen Herausforderungen versuchst**. Es ist wichtig, diese Elemente zu beherrschen und eine solide Grundlage zu haben.\n",
    "\n",
    "Nat√ºrlich werden wir im Laufe des Kurses tiefer in diese Konzepte eintauchen, aber **es ist besser, wenn man sie jetzt schon gut versteht, bevor man in die n√§chsten Kapitel eintaucht**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BjLhT70TEZIn"
   },
   "source": [
    "Beim n√§chsten Mal, in der Bonuseinheit 1, trainierst du Huggy, den Hund, um den Stock zu holen.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit1/huggy.jpg\" alt=\"Huggy\"/>\n",
    "\n",
    "## Lernt weiter, bleibt toll ü§ó."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "QAN7B0_HCVZC",
    "BqPKw3jt_pG5"
   ],
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.9.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "ed7f8024e43d3b8f5ca3c5e1a8151ab4d136b3ecee1e3fd59e0766ccc55e1b10"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
