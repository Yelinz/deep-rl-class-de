{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njb_ProuHiOe"
   },
   "source": [
    "# Unit 1: Trainiere deinen ersten Deep Reinforcement Learning Agent 🤖.\n",
    "\n",
    "![Cover](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/thumbnail.jpg)\n",
    "\n",
    "In diesem Notizbuch werden Sie Ihren **ersten Deep Reinforcement Learning-Agenten** trainieren, einen Lunar Lander-Agenten, der lernen soll, **richtig auf dem Mond zu landen 🌕**. Verwenden Sie [Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/), eine Deep Reinforcement Learning-Bibliothek, teilen Sie sie mit der Community und experimentieren Sie mit verschiedenen Konfigurationen\n",
    "\n",
    "⬇️ Hier ist ein Beispiel dafür, was **man in nur wenigen Minuten erreichen kann** ⬇️\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PF46MwbZD00b"
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<video controls autoplay><source src=\"https://huggingface.co/sb3/ppo-LunarLander-v2/resolve/main/replay.mp4\" type=\"video/mp4\"></video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x7oR6R-ZIbeS"
   },
   "source": [
    "### Die Umwelt 🎮\n",
    "\n",
    "- [LunarLander-v2](https://gymnasium.farama.org/environments/box2d/lunar_lander/)\n",
    "\n",
    "### Die verwendete Bibliothek 📚\n",
    "\n",
    "- [Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OwEcFHe9RRZW"
   },
   "source": [
    "Wir versuchen ständig, unsere Anleitungen zu verbessern. **Wenn Sie also Probleme in diesem Notizbuch** finden, öffnen Sie bitte [ein Problem im Github Repo](https://github.com/huggingface/deep-rl-class/issues)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4i6tjI2tHQ8j"
   },
   "source": [
    "## Ziele dieses Notizbuchs 🏆\n",
    "\n",
    "Am Ende des Notizbuchs werden Sie:\n",
    "\n",
    "- In der Lage sein, **Gymnasium**, die Umgebungsbibliothek, zu benutzen.\n",
    "- In der Lage sein, **Stable-Baselines3**, die Deep Reinforcement Learning-Bibliothek, zu verwenden.\n",
    "- In der Lage sein, **Ihren trainierten Agenten auf den Hub** zu pushen, mit einer schönen Videowiedergabe und einem Bewertungsergebnis 🔥.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ff-nyJdzJPND"
   },
   "source": [
    "## Dieses Notebook ist aus dem Deep Reinforcement Learning Kurs\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/deep-rl-course-illustration.jpg\" alt=\"Deep RL Course illustration\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6p5HnEefISCB"
   },
   "source": [
    "In diesem kostenlosen Kurs lernen Sie:\n",
    "\n",
    "- 📖 Deep Reinforcement Learning in **Theorie und Praxis** studieren.\n",
    "- 🧑‍💻 Lernen Sie, **berühmte Deep RL-Bibliotheken** wie Stable Baselines3, RL Baselines3 Zoo, CleanRL und Sample Factory 2.0 zu verwenden.\n",
    "- 🤖 Trainieren Sie **Agenten in einzigartigen Umgebungen**.\n",
    "- 🎓 **Erwerben Sie ein Abschlusszertifikat**, wenn Sie 80% der Aufgaben erfüllen.\n",
    "\n",
    "Und mehr!\n",
    "\n",
    "Prüfen Sie 📚 den Lehrplan 👉 https://simoninithomas.github.io/deep-rl-course\n",
    "\n",
    "Vergessen Sie nicht, sich **<a href=\"http://eepurl.com/ic5ZUD\">für den Kurs anzumelden</a>** (wir sammeln Ihre E-Mail, um Ihnen **die Links zu schicken, wenn die einzelnen Einheiten veröffentlicht werden, und Sie über die Herausforderungen und Aktualisierungen zu informieren).**\n",
    "\n",
    "Der beste Weg, um in Kontakt zu bleiben und Fragen zu stellen, ist **unserem Discord-Server** beizutreten, um sich mit der Community und uns auszutauschen 👉🏻 https://discord.gg/ydHrjt3WP5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-mo_6rXIjRi"
   },
   "source": [
    "## Voraussetzungen 🏗️\n",
    "\n",
    "Bevor Sie sich mit dem Notebook beschäftigen, müssen Sie:\n",
    "\n",
    "🔲 📝 **[Einheit 0 lesen](https://huggingface.co/deep-rl-course/unit0/introduction)**, die Ihnen alle **Informationen über den Kurs gibt und Ihnen beim Einstieg hilft** 🤗\n",
    "\n",
    "🔲 📚 **Entwickeln Sie ein Verständnis für die Grundlagen des Verstärkungslernens** (MC, TD, Belohnungshypothese...), indem Sie [Einheit 1](https://huggingface.co/deep-rl-course/unit1/introduction) lesen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HoeqMnr5LuYE"
   },
   "source": [
    "## Eine kleine Zusammenfassung von Deep Reinforcement Learning 📚\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/RL_process_game.jpg\" alt=\"Der RL-Prozess\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcQYx9ynaFMD"
   },
   "source": [
    "Lassen Sie uns kurz rekapitulieren, was wir in der ersten Einheit gelernt haben:\n",
    "\n",
    "- Verstärkungslernen ist ein **computergestützter Ansatz zum Lernen aus Handlungen**. Wir bauen einen Agenten, der von der Umwelt lernt, indem er **durch Versuch und Irrtum** mit ihr interagiert und Belohnungen (negativ oder positiv) als Feedback erhält.\n",
    "\n",
    "- Das Ziel eines jeden RL-Agenten ist die **Maximierung seiner erwarteten kumulativen Belohnung** (auch erwarteter Ertrag genannt), da RL auf der _Belohnungshypothese_ basiert, die besagt, dass alle Ziele als Maximierung einer erwarteten kumulativen Belohnung beschrieben werden können.\n",
    "\n",
    "- Der RL-Prozess ist eine **Schleife, die eine Folge von Zustand, Aktion, Belohnung und nächstem Zustand** ausgibt.\n",
    "\n",
    "- Um die erwartete kumulative Belohnung (erwartete Rendite) zu berechnen, **diskontieren wir die Belohnungen**: Die Belohnungen, die früher (zu Beginn des Spiels) eintreten, sind wahrscheinlicher, da sie besser vorhersehbar sind als die langfristige zukünftige Belohnung.\n",
    "\n",
    "- Um ein RL-Problem zu lösen, wollen Sie **eine optimale Strategie** finden; die Strategie ist das \"Gehirn\" Ihrer KI, das uns sagt, welche Aktion wir in einem bestimmten Zustand ergreifen sollen. Die optimale Strategie ist diejenige, die Ihnen die Aktionen liefert, die den erwarteten Gewinn maximieren.\n",
    "\n",
    "Es gibt **zwei** Möglichkeiten, die optimale Strategie zu finden:\n",
    "\n",
    "- Durch **direktes Trainieren Ihrer Strategie**: Strategie-basierte Methoden.\n",
    "- Durch **Trainieren einer Wertfunktion**, die uns den erwarteten Ertrag angibt, den der Agent in jedem Zustand erhält, und Verwendung dieser Funktion zur Festlegung unserer Strategie: wertbasierte Methoden.\n",
    "\n",
    "- Schließlich haben wir über Deep RL gesprochen, weil **wir tiefe neuronale Netze einsetzen, um die zu ergreifenden Maßnahmen (policy-basiert) oder den Wert eines Zustands (wertbasiert) zu schätzen, daher der Name \"deep\" **."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qDploC3jSH99"
   },
   "source": [
    "# Trainieren wir unseren ersten Deep Reinforcement Learning-Agenten und laden wir ihn in den Hub hoch 🚀.\n",
    "\n",
    "## Get a certificate 🎓\n",
    "\n",
    "Um dieses Hands-On für den [Zertifizierungsprozess] (https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process) zu validieren, müssen Sie Ihr trainiertes Modell zum Hub hochladen und **ein Ergebnis von >= 200** erhalten.\n",
    "\n",
    "Um Ihr Ergebnis zu finden, gehen Sie zur [Bestenliste](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) und suchen Sie Ihr Modell, **das Ergebnis = mean_reward - std of reward**\n",
    "\n",
    "Weitere Informationen über den Zertifizierungsprozess finden Sie in diesem Abschnitt 👉 https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HqzznTzhNfAC"
   },
   "source": [
    "## Die GPU einstellen 💪\n",
    "\n",
    "- Um **das Training des Agenten zu beschleunigen, werden wir einen Grafikprozessor** verwenden. Gehen Sie dazu auf \"Runtime > Change Runtime type\".\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step1.jpg\" alt=\"GPU Schritt 1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38HBd3t1SHJ8"
   },
   "source": [
    "- Hardware-Beschleuniger > GPU\".\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step2.jpg\" alt=\"GPU Schritt 2\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jeDAH0h0EBiG"
   },
   "source": [
    "## Abhängigkeiten installieren und einen virtuellen Bildschirm erstellen 🔽.\n",
    "\n",
    "Der erste Schritt ist die Installation der Abhängigkeiten, wir werden mehrere installieren.\n",
    "\n",
    "- `gymnasium[box2d]`: Enthält die LunarLander-v2-Umgebung 🌛.\n",
    "- `stable-baselines3[extra]`: Die Deep Reinforcement Learning-Bibliothek.\n",
    "- `huggingface_sb3`: Zusätzlicher Code für Stable-baselines3 zum Laden und Hochladen von Modellen aus dem Hugging Face 🤗 Hub.\n",
    "\n",
    "Um die Dinge zu vereinfachen, haben wir ein Skript erstellt, das all diese Abhängigkeiten installiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yQIGLPDkGhgG"
   },
   "outputs": [],
   "source": [
    "!apt install swig cmake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9XaULfDZDvrC"
   },
   "outputs": [],
   "source": [
    "!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BEKeXQJsQCYm"
   },
   "source": [
    "Während der Arbeit mit dem Notebook müssen wir ein Replay-Video erstellen. Dazu benötigen wir mit colab **einen virtuellen Bildschirm, um die Umgebung zu rendern** (und somit die Bilder aufzunehmen).\n",
    "\n",
    "Daher wird die folgende Zelle die Bibliotheken für virtuelle Bildschirme installieren und einen virtuellen Bildschirm erstellen und ausführen 🖥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j5f2cGkdP-mb"
   },
   "outputs": [],
   "source": [
    "!sudo apt-get update\n",
    "!sudo apt-get install -y python3-opengl\n",
    "!apt install ffmpeg\n",
    "!apt install xvfb\n",
    "!pip3 install pyvirtualdisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCwBTAwAW9JJ"
   },
   "source": [
    "Um sicherzustellen, dass die neu installierten Bibliotheken verwendet werden, **ist es manchmal erforderlich, die Laufzeit des Notebooks neu zu starten**. Die nächste Zelle wird die **Laufzeitumgebung zum Absturz bringen, so dass Sie eine neue Verbindung herstellen und den Code von hier aus ausführen müssen**. Dank dieses Tricks **können wir unseren virtuellen Bildschirm ausführen**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cYvkbef7XEMi"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BE5JWP5rQIKf"
   },
   "outputs": [],
   "source": [
    "# Virtual display\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "virtual_display = Display(visible=0, size=(1400, 900))\n",
    "virtual_display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wrgpVFqyENVf"
   },
   "source": [
    "## Importieren Sie die Pakete 📦\n",
    "\n",
    "Eine zusätzliche Bibliothek, die wir importieren, ist huggingface_hub **um trainierte Modelle aus dem Hub hoch- und herunterladen zu können**.\n",
    "\n",
    "\n",
    "Der Hugging Face Hub 🤗 funktioniert als zentraler Ort, an dem jeder Modelle und Datensätze teilen und erforschen kann. Er bietet Versionierung, Metriken, Visualisierungen und andere Funktionen, die eine einfache Zusammenarbeit mit anderen ermöglichen.\n",
    "\n",
    "Sie können hier alle Deep Reinforcement Learning-Modelle sehen, die hier verfügbar sind👉 https://huggingface.co/models?pipeline_tag=reinforcement-learning&sort=downloads\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cygWLPGsEQ0m"
   },
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "\n",
    "from huggingface_sb3 import load_from_hub, package_to_hub\n",
    "from huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRqRuRUl8CsB"
   },
   "source": [
    "## Verstehen Sie das Gymnasium und wie es funktioniert 🤖\n",
    "\n",
    "🏋 Die Bibliothek, die unsere Umgebung enthält, heißt Gymnasium.\n",
    "**Sie werden Gymnasium in Deep Reinforcement Learning häufig verwenden.\n",
    "\n",
    "Gymnasium ist die **neue Version der Gym-Bibliothek** [die von der Farama Foundation gepflegt wird] (https://farama.org/).\n",
    "\n",
    "Die Gymnasium-Bibliothek bietet zwei Dinge:\n",
    "\n",
    "- Eine Schnittstelle, die es erlaubt, **RL-Umgebungen** zu erstellen.\n",
    "- Eine **Sammlung von Umgebungen** (gym-control, atari, box2D...).\n",
    "\n",
    "Schauen wir uns ein Beispiel an, aber erinnern wir uns zunächst an die RL-Schleife.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/RL_process_game.jpg\" alt=\"Der RL-Prozess\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-TzNN0bQ_j-3"
   },
   "source": [
    "Bei jedem Schritt:\n",
    "- Unser Agent erhält einen **Zustand (S0)** von der **Umgebung** - wir erhalten das erste Bild unseres Spiels (Umgebung).\n",
    "- Basierend auf diesem **Zustand (S0),** führt der Agent eine **Aktion (A0)** aus - unser Agent bewegt sich nach rechts.\n",
    "- Die Umgebung geht in einen **neuen** **Zustand (S1)** über - neues Bild.\n",
    "- Die Umgebung gibt dem Agenten eine **Belohnung (R1)** - wir sind nicht tot *(Positive Belohnung +1)*.\n",
    "\n",
    "\n",
    "Mit Gymnasium:\n",
    "\n",
    "1️⃣ Wir erstellen unsere Umgebung mit `gymnasium.make()`\n",
    "\n",
    "2️⃣ Wir setzen die Umgebung mit `observation = env.reset()` auf ihren Ausgangszustand zurück.\n",
    "\n",
    "Bei jedem Schritt:\n",
    "\n",
    "3️⃣ Holen Sie sich eine Aktion mit unserem Modell (in unserem Beispiel nehmen wir eine zufällige Aktion)\n",
    "\n",
    "4️⃣ Mit `env.step(action)` führen wir diese Aktion in der Umgebung aus und erhalten\n",
    "- Beobachtung\": Der neue Zustand (st+1)\n",
    "- `Belohnung`: Die Belohnung, die wir nach dem Ausführen der Aktion erhalten\n",
    "- Beendet`: Zeigt an, ob die Episode beendet wurde (der Agent hat den Endzustand erreicht)\n",
    "- Abgeschnitten\": Mit dieser neuen Version eingeführt, zeigt es ein Zeitlimit an oder wenn ein Agent zum Beispiel die Grenzen der Umgebung verlässt.\n",
    "- `Info`: Ein Wörterbuch, das zusätzliche Informationen liefert (abhängig von der Umgebung).\n",
    "\n",
    "Für weitere Erklärungen siehe dies 👉 https://gymnasium.farama.org/api/env/#gymnasium.Env.step\n",
    "\n",
    "Wenn die Episode beendet ist:\n",
    "- Wir setzen die Umgebung mit `observation = env.reset()` auf ihren Ausgangszustand zurück.\n",
    "\n",
    "**Schauen wir uns ein Beispiel an!** Achten Sie darauf, den Code zu lesen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w7vOFlpA_ONz"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# First, we create our environment called LunarLander-v2\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "# Then we reset this environment\n",
    "observation, info = env.reset()\n",
    "\n",
    "for _ in range(20):\n",
    "  # Take a random action\n",
    "  action = env.action_space.sample()\n",
    "  print(\"Action taken:\", action)\n",
    "\n",
    "  # Do this action in the environment and get\n",
    "  # next_state, reward, terminated, truncated and info\n",
    "  observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "  # If the game is terminated (in our case we land, crashed) or truncated (timeout)\n",
    "  if terminated or truncated:\n",
    "      # Reset the environment\n",
    "      print(\"Environment is reset\")\n",
    "      observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XIrKGGSlENZB"
   },
   "source": [
    "## Die LunarLander-Umgebung erstellen 🌛 und verstehen, wie sie funktioniert\n",
    "\n",
    "### [Die Umgebung 🎮](https://gymnasium.farama.org/environments/box2d/lunar_lander/)\n",
    "\n",
    "In diesem ersten Tutorial trainieren wir unseren Agenten, einen [Lunar Lander](https://gymnasium.farama.org/environments/box2d/lunar_lander/), **um korrekt auf dem Mond zu landen**. Dazu muss der Agent lernen, **seine Geschwindigkeit und Position (horizontal, vertikal und im Winkel) anzupassen, um korrekt zu landen**.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "💡 Eine gute Angewohnheit, wenn man anfängt, eine Umgebung zu benutzen, ist es, ihre Dokumentation zu überprüfen.\n",
    "\n",
    "👉 https://gymnasium.farama.org/environments/box2d/lunar_lander/\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "poLBgRocF9aT"
   },
   "source": [
    "Schauen wir mal, wie die Umwelt aussieht:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZNPG0g_UGCfh"
   },
   "outputs": [],
   "source": [
    "# We create our environment with gym.make(\"<name_of_the_environment>\")\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "env.reset()\n",
    "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
    "print(\"Observation Space Shape\", env.observation_space.shape)\n",
    "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2MXc15qFE0M9"
   },
   "source": [
    "Mit \"Observation Space Shape (8,)\" sehen wir, dass die Beobachtung ein Vektor der Größe 8 ist, wobei jeder Wert verschiedene Informationen über den Lander enthält:\n",
    "- Horizontale Pad-Koordinate (x)\n",
    "- Vertikale Pad-Koordinate (y)\n",
    "- Horizontale Geschwindigkeit (x)\n",
    "- Vertikale Geschwindigkeit (y)\n",
    "- Winkel\n",
    "- Geschwindigkeit im Winkel\n",
    "- Wenn der Kontaktpunkt des linken Beins das Land berührt hat (boolesch)\n",
    "- Wenn der Kontaktpunkt des rechten Beins das Land berührt hat (boolesch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "We5WqOBGLoSm"
   },
   "outputs": [],
   "source": [
    "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
    "print(\"Action Space Shape\", env.action_space.n)\n",
    "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MyxXwkI2Magx"
   },
   "source": [
    "Der Aktionsraum (die Menge der möglichen Aktionen, die der Agent ausführen kann) ist diskret mit 4 verfügbaren Aktionen 🎮:\n",
    "\n",
    "- Aktion 0: Nichts tun,\n",
    "- Aktion 1: Linken Orientierungsmotor zünden,\n",
    "- Aktion 2: Zündung des Hauptmotors,\n",
    "- Aktion 3: Rechtes Orientierungstriebwerk zünden.\n",
    "\n",
    "Belohnungsfunktion (die Funktion, die bei jedem Zeitschritt eine Belohnung gibt) 💰:\n",
    "\n",
    "Nach jedem Schritt wird eine Belohnung gewährt. Die Gesamtbelohnung einer Episode ist die **Summe der Belohnungen für alle Schritte innerhalb dieser Episode**.\n",
    "\n",
    "Für jeden Schritt gibt es eine Belohnung:\n",
    "\n",
    "- Sie erhöht/verringert sich, je näher/weiter der Lander an der Landeplattform ist.\n",
    "- Sie erhöht/verringert sich, je langsamer/schneller sich der Lander bewegt.\n",
    "- Wird verringert, je stärker der Lander geneigt ist (Winkel nicht horizontal).\n",
    "- Erhöht sich um 10 Punkte für jedes Bein, das den Boden berührt.\n",
    "- Verringert sich um 0,03 Punkte pro Frame, in dem ein Seitentriebwerk gezündet wird.\n",
    "- Verringert sich um 0,3 Punkte pro Frame, in dem das Haupttriebwerk gezündet wird.\n",
    "\n",
    "Die Episode erhält eine **zusätzliche Belohnung von -100 bzw. +100 Punkten für einen Absturz bzw. eine sichere Landung**.\n",
    "\n",
    "Eine Episode gilt als **gelöst, wenn sie mindestens 200 Punkte erreicht hat.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dFD9RAFjG8aq"
   },
   "source": [
    "#### Vektorisierte Umgebung\n",
    "\n",
    "- Wir erstellen eine vektorisierte Umgebung (eine Methode zum Stapeln mehrerer unabhängiger Umgebungen zu einer einzigen Umgebung) mit 16 Umgebungen, damit **wir während des Trainings vielfältigere Erfahrungen machen können**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "99hqQ_etEy1N"
   },
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "env = make_vec_env('LunarLander-v2', n_envs=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VgrE86r5E5IK"
   },
   "source": [
    "## Das Modell erstellen 🤖\n",
    "- Wir haben unsere Umgebung untersucht und das Problem verstanden: **Fähig sein, den Lunar Lander korrekt auf dem Landeplatz zu landen, indem wir das linke und rechte Triebwerk sowie das Hauptausrichtungs-Triebwerk steuern**. Jetzt erstellen wir den Algorithmus, den wir zur Lösung dieses Problems verwenden werden 🚀.\n",
    "\n",
    "- Dazu verwenden wir unsere erste Deep-RL-Bibliothek, [Stable Baselines3 (SB3)] (https://stable-baselines3.readthedocs.io/en/master/).\n",
    "\n",
    "- SB3 ist eine Reihe von **zuverlässigen Implementierungen von Verstärkungslernalgorithmen in PyTorch**.\n",
    "\n",
    "---\n",
    "\n",
    "💡 Eine gute Angewohnheit bei der Verwendung einer neuen Bibliothek ist es, sich zuerst in die Dokumentation zu vertiefen: https://stable-baselines3.readthedocs.io/en/master/ und dann einige Tutorials auszuprobieren.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HLlClRW37Q7e"
   },
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/sb3.png\" alt=\"Stabile Grundlinien3\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HV4yiUM_9_Ka"
   },
   "source": [
    "Um dieses Problem zu lösen, werden wir SB3 **PPO** verwenden. [PPO (auch bekannt als Proximal Policy Optimization) ist einer der SOTA (State of the Art) Deep Reinforcement Learning-Algorithmen, die Sie in diesem Kurs kennenlernen werden] (https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#example%5D).\n",
    "\n",
    "PPO ist eine Kombination aus:\n",
    "- *Value-based Reinforcement Learning-Methode*: Lernen einer Action-Value-Funktion, die uns die **wertvollste zu ergreifende Aktion bei einem Zustand und einer Aktion** liefert.\n",
    "- Methode des politikbasierten Verstärkungslernens*: Lernen einer Politik, die uns eine **Wahrscheinlichkeitsverteilung über Aktionen** liefert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5qL_4HeIOrEJ"
   },
   "source": [
    "Stable-Baselines3 ist einfach einzurichten:\n",
    "\n",
    "1️⃣ Sie **erstellen Ihre Umgebung** (in unserem Fall wurde das oben gemacht)\n",
    "\n",
    "2️⃣ Sie definieren das **Modell, das Sie verwenden möchten, und instanziieren dieses Modell** `model = PPO(\"MlpPolicy\")`\n",
    "\n",
    "3️⃣ Sie **trainieren den Agenten** mit `model.learn` und legen die Anzahl der Trainingszeitschritte fest\n",
    "\n",
    "```\n",
    "# Umgebung erstellen\n",
    "env = gym.make('LunarLander-v2')\n",
    "\n",
    "# Den Agenten instanziieren\n",
    "model = PPO('MlpPolicy', env, verbose=1)\n",
    "# Den Agenten trainieren\n",
    "model.learn(total_timesteps=int(2e5))\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nxI6hT1GE4-A"
   },
   "outputs": [],
   "source": [
    "# TODO: Define a PPO MlpPolicy architecture\n",
    "# We use MultiLayerPerceptron (MLPPolicy) because the input is a vector,\n",
    "# if we had frames as input we would use CnnPolicy\n",
    "model ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QAN7B0_HCVZC"
   },
   "source": [
    "#### Lösung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "543OHYDfcjK4"
   },
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "# We added some parameters to accelerate the training\n",
    "model = PPO(\n",
    "    policy = 'MlpPolicy',\n",
    "    env = env,\n",
    "    n_steps = 1024,\n",
    "    batch_size = 64,\n",
    "    n_epochs = 4,\n",
    "    gamma = 0.999,\n",
    "    gae_lambda = 0.98,\n",
    "    ent_coef = 0.01,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ClJJk88yoBUi"
   },
   "source": [
    "## Trainieren Sie den PPO-Agenten 🏃.\n",
    "- Lassen Sie uns unseren Agenten für 1.000.000 Zeitschritte trainieren, vergessen Sie nicht, die GPU auf Colab zu verwenden. Es wird ungefähr ~20min dauern, aber Sie können auch weniger Zeitschritte verwenden, wenn Sie es nur ausprobieren wollen.\n",
    "- Machen Sie während des Trainings eine ☕ Pause, die Sie sich verdient haben 🤗."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qKnYkNiVp89p"
   },
   "outputs": [],
   "source": [
    "# TODO: Train it for 1,000,000 timesteps\n",
    "\n",
    "# TODO: Specify file name for model and save the model to file\n",
    "model_name = \"ppo-LunarLander-v2\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1bQzQ-QcE3zo"
   },
   "source": [
    "#### Lösung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "poBCy9u_csyR"
   },
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "# Train it for 1,000,000 timesteps\n",
    "model.learn(total_timesteps=1000000)\n",
    "# Save the model\n",
    "model_name = \"ppo-LunarLander-v2\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BY_HuedOoISR"
   },
   "source": [
    "## Bewerten Sie den Agenten 📈.\n",
    "- Denken Sie daran, die Umgebung in einen [Monitor](https://stable-baselines3.readthedocs.io/en/master/common/monitor.html) zu verpacken.\n",
    "- Nun, da unser Lunar Lander-Agent trainiert ist 🚀, müssen wir seine Leistung **prüfen**.\n",
    "- Stable-Baselines3 bietet dafür eine Methode: `evaluate_policy`.\n",
    "- Um diesen Teil auszufüllen, müssen Sie [in der Dokumentation nachsehen] (https://stable-baselines3.readthedocs.io/en/master/guide/examples.html#basic-usage-training-saving-loading)\n",
    "- Im nächsten Schritt werden wir sehen, **wie Sie Ihren Agenten automatisch bewerten und freigeben können, um in einer Rangliste zu konkurrieren, aber jetzt machen wir es erst einmal selbst**\n",
    "\n",
    "\n",
    "💡 Wenn Sie Ihren Agenten bewerten, sollten Sie nicht Ihre Trainingsumgebung verwenden, sondern eine Bewertungsumgebung erstellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yRpno0glsADy"
   },
   "outputs": [],
   "source": [
    "# TODO: Evaluate the agent\n",
    "# Create a new environment for evaluation\n",
    "eval_env =\n",
    "\n",
    "# Evaluate the model with 10 evaluation episodes and deterministic=True\n",
    "mean_reward, std_reward =\n",
    "\n",
    "# Print the results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BqPKw3jt_pG5"
   },
   "source": [
    "#### Lösung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zpz8kHlt_a_m"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "eval_env = Monitor(gym.make(\"LunarLander-v2\"))\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
    "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "reBhoODwcXfr"
   },
   "source": [
    "- In meinem Fall habe ich nach dem Training von 1 Million Schritten eine mittlere Belohnung von 200,20 +/- 20,80 erhalten, was bedeutet, dass unser Mondlande-Agent bereit ist, auf dem Mond zu landen 🌛🥳."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IK_kR78NoNb2"
   },
   "source": [
    "## Veröffentliche unser trainiertes Modell auf dem Hub 🔥\n",
    "Da wir nun gesehen haben, dass wir nach dem Training gute Ergebnisse erzielt haben, können wir unser trainiertes Modell mit einer Zeile Code auf dem Hub 🤗 veröffentlichen.\n",
    "\n",
    "📚 Die Dokumentation der Bibliotheken 👉 https://github.com/huggingface/huggingface_sb3/tree/main#hugging-face--x-stable-baselines3-v20\n",
    "\n",
    "Hier ist ein Beispiel für eine Model Card (mit Space Invaders):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gs-Ew7e1gXN3"
   },
   "source": [
    "Mit `package_to_hub` **werten Sie aus, zeichnen ein Replay auf, generieren eine Modellkarte Ihres Agenten und schieben sie zum Hub**.\n",
    "\n",
    "This way:\n",
    "- Sie können **unsere Arbeit vorführen** 🔥.\n",
    "- Sie können **Ihren Agenten beim Spielen visualisieren** 👀\n",
    "- Du kannst **einen Agenten mit der Community teilen, den andere benutzen können** 💾\n",
    "- Sie können **auf eine Bestenliste 🏆 zugreifen, um zu sehen, wie gut Ihr Agent im Vergleich zu Ihren Klassenkameraden abschneidet** 👉 https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JquRrWytA6eo"
   },
   "source": [
    "Um Ihr Modell mit der Gemeinschaft teilen zu können, sind drei weitere Schritte erforderlich:\n",
    "\n",
    "1️⃣ (Falls noch nicht geschehen) erstelle ein Konto auf Hugging Face ➡ https://huggingface.co/join\n",
    "\n",
    "2️⃣ Melde dich an und speichere dann dein Authentifizierungs-Token von der Hugging Face-Website.\n",
    "- Erstellen Sie ein neues Token (https://huggingface.co/settings/tokens) **mit Schreibrolle**\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/create-token.jpg\" alt=\"HF-Token erstellen\">\n",
    "\n",
    "- Kopieren Sie das Token\n",
    "- Führen Sie die Zelle unten aus und fügen Sie das Token ein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GZiFBBlzxzxY"
   },
   "outputs": [],
   "source": [
    "notebook_login()\n",
    "!git config --global credential.helper store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_tsf2uv0g_4p"
   },
   "source": [
    "Wenn Sie kein Google Colab oder ein Jupyter Notebook verwenden möchten, müssen Sie stattdessen diesen Befehl verwenden: `huggingface-cli login`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FGNh9VsZok0i"
   },
   "source": [
    "3️⃣ Wir sind jetzt bereit, unseren trainierten Agenten mit der Funktion `package_to_hub()` an den 🤗 Hub 🔥 zu übertragen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ay24l6bqFF18"
   },
   "source": [
    "Füllen wir die Funktion `package_to_hub`:\n",
    "- model\": unser trainiertes Modell.\n",
    "- model_name\": der Name des trainierten Modells, den wir in \"model_save\" definiert haben\n",
    "- model_architecture`: die verwendete Modellarchitektur, in unserem Fall PPO\n",
    "- env_id`: der Name der Umgebung, in unserem Fall `LunarLander-v2`\n",
    "- eval_env`: die in eval_env definierte Auswertungsumgebung\n",
    "- repo_id`: der Name des Hugging Face Hub Repository, das erstellt/aktualisiert werden soll `(repo_id = {Benutzername}/{repo_name})`\n",
    "\n",
    "💡 **Ein guter Name ist {username}/{model_architecture}-{env_id}**\n",
    "\n",
    "- `commit_message`: Nachricht der Übergabe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JPG7ofdGIHN8"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "from huggingface_sb3 import package_to_hub\n",
    "\n",
    "## TODO: Define a repo_id\n",
    "## repo_id is the id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2\n",
    "repo_id =\n",
    "\n",
    "# TODO: Define the name of the environment\n",
    "env_id =\n",
    "\n",
    "# Create the evaluation env and set the render_mode=\"rgb_array\"\n",
    "eval_env = DummyVecEnv([lambda: Monitor(gym.make(env_id, render_mode=\"rgb_array\"))])\n",
    "\n",
    "\n",
    "# TODO: Define the model architecture we used\n",
    "model_architecture = \"\"\n",
    "\n",
    "## TODO: Define the commit message\n",
    "commit_message = \"\"\n",
    "\n",
    "# method save, evaluate, generate a model card and record a replay video of your agent before pushing the repo to the hub\n",
    "package_to_hub(model=model, # Our trained model\n",
    "               model_name=model_name, # The name of our trained model\n",
    "               model_architecture=model_architecture, # The model architecture we used: in our case PPO\n",
    "               env_id=env_id, # Name of the environment\n",
    "               eval_env=eval_env, # Evaluation Environment\n",
    "               repo_id=repo_id, # id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2\n",
    "               commit_message=commit_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Avf6gufJBGMw"
   },
   "source": [
    "#### Lösung\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I2E--IJu8JYq"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "from huggingface_sb3 import package_to_hub\n",
    "\n",
    "# PLACE the variables you've just defined two cells above\n",
    "# Define the name of the environment\n",
    "env_id = \"LunarLander-v2\"\n",
    "\n",
    "# TODO: Define the model architecture we used\n",
    "model_architecture = \"PPO\"\n",
    "\n",
    "## Define a repo_id\n",
    "## repo_id is the id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2\n",
    "## CHANGE WITH YOUR REPO ID\n",
    "repo_id = \"ThomasSimonini/ppo-LunarLander-v2\" # Change with your repo id, you can't push with mine 😄\n",
    "\n",
    "## Define the commit message\n",
    "commit_message = \"Upload PPO LunarLander-v2 trained agent\"\n",
    "\n",
    "# Create the evaluation env and set the render_mode=\"rgb_array\"\n",
    "eval_env = DummyVecEnv([lambda: gym.make(env_id, render_mode=\"rgb_array\")])\n",
    "\n",
    "# PLACE the package_to_hub function you've just filled here\n",
    "package_to_hub(model=model, # Our trained model\n",
    "               model_name=model_name, # The name of our trained model\n",
    "               model_architecture=model_architecture, # The model architecture we used: in our case PPO\n",
    "               env_id=env_id, # Name of the environment\n",
    "               eval_env=eval_env, # Evaluation Environment\n",
    "               repo_id=repo_id, # id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2\n",
    "               commit_message=commit_message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T79AEAWEFIxz"
   },
   "source": [
    "Herzlichen Glückwunsch 🥳 Sie haben gerade Ihren ersten Deep Reinforcement Learning-Agenten trainiert und hochgeladen. Das obige Skript sollte einen Link zu einem Modell-Repository wie https://huggingface.co/osanseviero/test_sb3 angezeigt haben. Wenn Sie auf diesen Link gehen, können Sie:\n",
    "* Auf der rechten Seite eine Videovorschau Ihres Agenten sehen.\n",
    "* Klicken Sie auf \"Dateien und Versionen\", um alle Dateien im Repository zu sehen.\n",
    "* Klicken Sie auf \"Use in stable-baselines3\", um einen Codeausschnitt zu erhalten, der zeigt, wie das Modell geladen wird.\n",
    "* Eine Modellkarte (Datei `README.md`), die eine Beschreibung des Modells enthält\n",
    "\n",
    "Unter der Haube verwendet der Hub git-basierte Repositories (keine Sorge, wenn Sie nicht wissen, was git ist), was bedeutet, dass Sie das Modell mit neuen Versionen aktualisieren können, wenn Sie experimentieren und Ihren Agenten verbessern.\n",
    "\n",
    "Vergleiche die Ergebnisse deines LunarLander-v2 mit deinen Klassenkameraden mit Hilfe des Leaderboards 🏆 👉 https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9nWnuQHRfFRa"
   },
   "source": [
    "## Laden Sie ein gespeichertes LunarLander-Modell aus dem Hub 🤗.\n",
    "Danke an [ironbar](https://github.com/ironbar) für den Beitrag.\n",
    "\n",
    "Das Laden eines gespeicherten Modells aus dem Hub ist wirklich einfach.\n",
    "\n",
    "Gehen Sie auf https://huggingface.co/models?library=stable-baselines3, um die Liste aller gespeicherten Stable-baselines3-Modelle zu sehen.\n",
    "1. Sie wählen eines aus und kopieren seine repo_id\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit1/copy-id.png\" alt=\"Copy-id\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hNPLJF2bfiUw"
   },
   "source": [
    "2. Dann müssen wir nur load_from_hub mit verwenden:\n",
    "- Die Repo_id\n",
    "- dem Dateinamen: dem gespeicherten Modell innerhalb des Repo und seiner Erweiterung (*.zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhb9-NtsinKB"
   },
   "source": [
    "Da das Modell, das ich vom Hub herunterlade, mit Gym (der früheren Version von Gymnasium) trainiert wurde, müssen wir shimmy installieren, ein API-Konvertierungstool, das uns helfen wird, die Umgebung korrekt auszuführen.\n",
    "\n",
    "Shimmy Dokumentation: https://github.com/Farama-Foundation/Shimmy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "03WI-bkci1kH"
   },
   "outputs": [],
   "source": [
    "!pip install shimmy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oj8PSGHJfwz3"
   },
   "outputs": [],
   "source": [
    "from huggingface_sb3 import load_from_hub\n",
    "repo_id = \"Classroom-workshop/assignment2-omar\" # The repo_id\n",
    "filename = \"ppo-LunarLander-v2.zip\" # The model filename.zip\n",
    "\n",
    "# When the model was trained on Python 3.8 the pickle protocol is 5\n",
    "# But Python 3.6, 3.7 use protocol 4\n",
    "# In order to get compatibility we need to:\n",
    "# 1. Install pickle5 (we done it at the beginning of the colab)\n",
    "# 2. Create a custom empty object we pass as parameter to PPO.load()\n",
    "custom_objects = {\n",
    "            \"learning_rate\": 0.0,\n",
    "            \"lr_schedule\": lambda _: 0.0,\n",
    "            \"clip_range\": lambda _: 0.0,\n",
    "}\n",
    "\n",
    "checkpoint = load_from_hub(repo_id, filename)\n",
    "model = PPO.load(checkpoint, custom_objects=custom_objects, print_system_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fs0Y-qgPgLUf"
   },
   "source": [
    "Lassen Sie uns dieses Mittel bewerten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PAEVwK-aahfx"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "eval_env = Monitor(gym.make(\"LunarLander-v2\"))\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
    "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQAwLnYFPk-s"
   },
   "source": [
    "## Einige zusätzliche Herausforderungen 🏆\n",
    "Die beste Art zu lernen **ist, Dinge selbst auszuprobieren**! Wie Sie gesehen haben, ist der derzeitige Agent nicht besonders gut. Als ersten Vorschlag können Sie für mehr Schritte trainieren. Bei 1.000.000 Schritten haben wir tolle Ergebnisse gesehen!\n",
    "\n",
    "In der [Rangliste] (https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) findest du deine Agenten. Können Sie sich an die Spitze setzen?\n",
    "\n",
    "Hier sind einige Ideen, um dies zu erreichen:\n",
    "* Trainieren Sie mehr Schritte\n",
    "* Probieren Sie verschiedene Hyperparameter für `PPO` aus. Sie können sie unter https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#parameters sehen.\n",
    "* Überprüfen Sie die [Stable-Baselines3-Dokumentation] (https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html) und versuchen Sie ein anderes Modell wie DQN.\n",
    "**Pushen Sie Ihr neu trainiertes Modell** auf den Hub 🔥.\n",
    "\n",
    "**Vergleiche die Ergebnisse deines LunarLander-v2 mit deinen Klassenkameraden** mit Hilfe der [Bestenliste](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) 🏆\n",
    "\n",
    "Ist dir die Mondlandung zu langweilig? Versuche, **die Umgebung zu verändern**, warum nicht MountainCar-v0, CartPole-v1 oder CarRacing-v0 benutzen? Schau dir an, wie sie funktionieren [mit Hilfe der Gym-Dokumentation] (https://www.gymlibrary.dev/) und hab Spaß 🎉."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9lM95-dvmif8"
   },
   "source": [
    "________________________________________________________________________\n",
    "Herzlichen Glückwunsch zum Abschluss dieses Kapitels! Das war das größte, **und es gab eine Menge Informationen.**\n",
    "\n",
    "Wenn du dich immer noch verwirrt fühlst mit all diesen Elementen...das ist völlig normal! **So ging es mir und allen Leuten, die RL studiert haben.\n",
    "\n",
    "Nimm dir Zeit, um den Stoff wirklich zu **verstehen, bevor du weitermachst und die zusätzlichen Herausforderungen versuchst**. Es ist wichtig, diese Elemente zu beherrschen und eine solide Grundlage zu haben.\n",
    "\n",
    "Natürlich werden wir im Laufe des Kurses tiefer in diese Konzepte eintauchen, aber **es ist besser, wenn man sie jetzt schon gut versteht, bevor man in die nächsten Kapitel eintaucht**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BjLhT70TEZIn"
   },
   "source": [
    "Beim nächsten Mal, in der Bonuseinheit 1, trainierst du Huggy, den Hund, um den Stock zu holen.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit1/huggy.jpg\" alt=\"Huggy\"/>\n",
    "\n",
    "## Lernt weiter, bleibt toll 🤗."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "QAN7B0_HCVZC",
    "BqPKw3jt_pG5"
   ],
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.9.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "ed7f8024e43d3b8f5ca3c5e1a8151ab4d136b3ecee1e3fd59e0766ccc55e1b10"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
