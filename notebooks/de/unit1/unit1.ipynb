{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njb_ProuHiOe"
   },
   "source": [
    "# Unit 1: Trainiere deinen ersten Deep Reinforcement Learning Agent 🤖.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/thumbnail.jpg\" alt=\"Illustration_1\" width=\"50%\">\n",
    "\n",
    "In diesem Notizbuch werden Sie Ihren **ersten Deep Reinforcement Learning-Agenten** trainieren, einen Lunar Lander-Agenten, der lernen soll, **richtig auf dem Mond zu landen 🌕**. Verwenden Sie [Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/), eine Deep Reinforcement Learning-Bibliothek, teilen Sie sie mit der Community und experimentieren Sie mit verschiedenen Konfigurationen\n",
    "\n",
    "⬇️ Hier ist ein Beispiel dafür, was **man in nur wenigen Minuten erreichen kann** ⬇️\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PF46MwbZD00b"
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<video controls autoplay><source src=\"https://huggingface.co/sb3/ppo-LunarLander-v2/resolve/main/replay.mp4\" type=\"video/mp4\"></video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x7oR6R-ZIbeS"
   },
   "source": [
    "### Die Umgebung 🎮\n",
    "\n",
    "- [LunarLander-v2](https://gymnasium.farama.org/environments/box2d/lunar_lander/)\n",
    "\n",
    "### Die verwendete Bibliothek 📚\n",
    "\n",
    "- [Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OwEcFHe9RRZW"
   },
   "source": [
    "Wir versuchen ständig, unsere Anleitungen zu verbessern. **Wenn Sie also Probleme in diesem Notizbuch** finden, öffnen Sie bitte [ein Problem im Github Repo](https://github.com/huggingface/deep-rl-class/issues)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4i6tjI2tHQ8j"
   },
   "source": [
    "## Ziele dieses Notizbuchs 🏆\n",
    "\n",
    "Am Ende des Notizbuchs werden Sie:\n",
    "\n",
    "- In der Lage sein, **Gymnasium**, die Umgebungsbibliothek, zu benutzen.\n",
    "- In der Lage sein, **Stable-Baselines3**, die Deep Reinforcement Learning-Bibliothek, zu verwenden.\n",
    "- In der Lage sein, **Ihren trainierten Agenten auf den Hub** zu pushen, mit einer schönen Videowiedergabe und einem Bewertungsergebnis 🔥.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ff-nyJdzJPND"
   },
   "source": [
    "## Dieses Notebook ist aus dem Deep Reinforcement Learning Kurs\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/deep-rl-course-illustration.jpg\" alt=\"Deep RL Course illustration\" width=\"30%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6p5HnEefISCB"
   },
   "source": [
    "In diesem kostenlosen Kurs lernen Sie:\n",
    "\n",
    "- 📖 Deep Reinforcement Learning in **Theorie und Praxis** studieren.\n",
    "- 🧑‍💻 Lernen Sie, **berühmte Deep RL-Bibliotheken** wie Stable Baselines3, RL Baselines3 Zoo, CleanRL und Sample Factory 2.0 zu verwenden.\n",
    "- 🤖 Trainieren Sie **Agenten in einzigartigen Umgebungen**.\n",
    "- 🎓 **Erwerben Sie ein Abschlusszertifikat**, wenn Sie 80% der Aufgaben erfüllen.\n",
    "\n",
    "Und mehr!\n",
    "\n",
    "Prüfen Sie 📚 den Lehrplan 👉 https://simoninithomas.github.io/deep-rl-course\n",
    "\n",
    "Vergessen Sie nicht, sich **<a href=\"http://eepurl.com/ic5ZUD\">für den Kurs anzumelden</a>** (wir sammeln Ihre E-Mail, um Ihnen **die Links zu schicken, wenn die einzelnen Einheiten veröffentlicht werden, und Sie über die Herausforderungen und Aktualisierungen zu informieren).**\n",
    "\n",
    "Der beste Weg, um in Kontakt zu bleiben und Fragen zu stellen, ist **unserem Discord-Server** beizutreten, um sich mit der Community und uns auszutauschen 👉🏻 https://discord.gg/ydHrjt3WP5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KI und Reinforcement Learning\n",
    "Wie Reinforcement Learning (RL) im zusammenhang mit KI und deren anderen Feldern zusammenhängt\n",
    "\n",
    "<img src=\"https://i.imgur.com/xGGaw0k.png\" alt=\"KI Felder\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-mo_6rXIjRi"
   },
   "source": [
    "# Was ist Reinforcement Learning?\n",
    "\n",
    "Um Reinforcement Learning zu verstehen, sollten wir mit dem großen Bild beginnen.\n",
    "\n",
    "## Das große Bild\n",
    "\n",
    "Die Idee hinter Reinforcement Learning ist, dass ein Agent (eine KI) von der Umwelt lernt, indem er **mit ihr interagiert** (durch Versuch und Irrtum) und **Belohnungen** (negativ oder positiv) als Rückmeldung für ausgeführte Aktionen erhält.\n",
    "\n",
    "Das Lernen aus Interaktionen mit der Umwelt **entsteht aus unseren natürlichen Erfahrungen**.\n",
    "\n",
    "Stellen Sie sich zum Beispiel vor, Sie setzen Ihren kleinen Bruder vor ein Videospiel, das er noch nie gespielt hat, geben ihm einen Controller und lassen ihn allein.\n",
    "\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/Illustration_1.jpg\" alt=\"Illustration_1\" width=\"100%\">\n",
    "\n",
    "Ihr Bruder interagiert mit der Umgebung (dem Videospiel), indem er die richtige Taste (Aktion) drückt. Er hat eine Münze bekommen, das ist eine +1 Belohnung. Das ist positiv, er hat gerade verstanden, dass er in diesem Spiel **die Münzen bekommen muss.**\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/Illustration_2.jpg\" alt=\"Illustration_2\" width=\"100%\">\n",
    "\n",
    "Aber dann **drückt er wieder die rechte Taste** und berührt einen Gegner. Er ist gerade gestorben, also ist das eine -1 Belohnung.\n",
    "\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/Illustration_3.jpg\" alt=\"Abbildung_3\" width=\"100%\">\n",
    "\n",
    "Indem er durch Ausprobieren mit seiner Umgebung interagiert, versteht dein kleiner Bruder, dass er in dieser Umgebung **Münzen bekommen, aber die Feinde meiden muss**.\n",
    "\n",
    "**Ohne jegliche Aufsicht** wird das Kind das Spiel immer besser beherrschen.\n",
    "\n",
    "So lernen Menschen und Tiere, **durch Interaktion.** Das Verstärkungslernen ist nur ein **computergestützter Ansatz des Lernens aus Handlungen.**\n",
    "\n",
    "\n",
    "### Eine formale Definition\n",
    "\n",
    "Wir können nun eine formale Definition vornehmen:\n",
    "\n",
    "Verstärkungslernen ist ein Rahmen für die Lösung von Steuerungsaufgaben (auch Entscheidungsprobleme genannt) durch den Aufbau von Agenten, die von der Umwelt lernen, indem sie mit ihr durch Versuch und Irrtum interagieren und Belohnungen (positiv oder negativ) als einzigartiges Feedback erhalten.\n",
    "\n",
    "Aber wie funktioniert Reinforcement Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Das Reinforcement Learning Framework\n",
    "\n",
    "## Der RL-Prozess\n",
    "<figure>\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/RL_process.jpg\" alt=\"The RL process\" width=\"100%\">\n",
    "<figcaption>The RL Process: a loop of state, action, reward and next state</figcaption>\n",
    "<figcaption>Source: <a href=\"http://incompleteideas.net/book/RLbook2020.pdf\">Reinforcement Learning: An Introduction, Richard Sutton and Andrew G. Barto</a></figcaption>\n",
    "</figure>\n",
    "\n",
    "Um den RL-Prozess zu verstehen, stellen wir uns einen Agenten vor, der lernt, ein Plattformspiel zu spielen:\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/RL_process_game.jpg\" alt=\"Der RL-Prozess\" width=\"100%\">\n",
    "\n",
    "- Unser Agent erhält den **Zustand \\\\(S_0\\\\)** von der **Umgebung** - wir erhalten den ersten Frame unseres Spiels (Umgebung).\n",
    "- Basierend auf diesem **Zustand \\\\(S_0\\\\),** führt der Agent eine **Aktion \\\\(A_0\\\\)** aus - unser Agent wird sich nach rechts bewegen.\n",
    "- Die Umgebung geht in einen **neuen** **Zustand \\\\(S_1\\\\)** über - neues Bild.\n",
    "- Die Umgebung gibt dem Agenten eine **Belohnung \\\\(R_1\\\\)** - wir sind nicht tot *(Positive Belohnung +1)*.\n",
    "\n",
    "Diese RL-Schleife gibt eine Sequenz von **Zustand, Aktion, Belohnung und nächstem Zustand** aus.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/sars.jpg\" alt=\"State, Action, Reward, Next State\" width=\"100%\">\n",
    "\n",
    "Das Ziel des Agenten ist die _Maximierung_ seiner kumulativen Belohnung, die **erwartete Rendite genannt wird.**\n",
    "\n",
    "## Die Belohnungshypothese: die zentrale Idee des Reinforcement Learning\n",
    "\n",
    "⇒ Warum ist das Ziel des Agenten die Maximierung des erwarteten Gewinns?\n",
    "\n",
    "Weil RL auf der **Belohnungshypothese** basiert, die besagt, dass alle Ziele als **Maximierung des erwarteten Ertrags** (erwartete kumulative Belohnung) beschrieben werden können.\n",
    "\n",
    "Aus diesem Grund versuchen wir beim Reinforcement Learning, **das beste Verhalten** zu erlernen, das **die erwartete kumulative Belohnung** maximiert.\n",
    "\n",
    "\n",
    "## Markov-Eigenschaft\n",
    "\n",
    "In der Literatur wird der RL-Prozess als **Markov Decision Process** (MDP) bezeichnet.\n",
    "\n",
    "Wir werden in den folgenden Einheiten erneut über die Markov-Eigenschaft sprechen. Die Markov-Eigenschaft impliziert, dass unser Agent **nur den aktuellen Zustand benötigt, um zu entscheiden**, welche Aktion er ausführen soll, und **nicht die Geschichte aller Zustände und Aktionen**, die er zuvor ausgeführt hat.\n",
    "\n",
    "## Raum für Beobachtungen/Zustände\n",
    "\n",
    "Beobachtungen/Zustände sind die **Informationen, die unser Agent aus der Umgebung erhält.** Im Falle eines Videospiels kann es sich um einen Frame (einen Screenshot) handeln. Im Falle des Handelsagenten kann es sich um den Wert einer bestimmten Aktie usw. handeln.\n",
    "\n",
    "Es muss jedoch zwischen *Beobachtung* und *Zustand* unterschieden werden:\n",
    "\n",
    "- *Zustand s*: ist **eine vollständige Beschreibung des Zustands der Welt** (es gibt keine versteckten Informationen). In einer vollständig beobachteten Umgebung.\n",
    "\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/chess.jpg\" alt=\"Chess\">\n",
    "<figcaption>In chess game, we receive a state from the environment since we have access to the whole check board information.</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "Bei einer Schachpartie haben wir Zugang zu den Informationen des gesamten Brettes, wir erhalten also einen Zustand aus der Umgebung. Mit anderen Worten, die Umgebung wird vollständig beobachtet.\n",
    "\n",
    "- *Beobachtung o*: ist eine **teilweise Beschreibung des Zustands.** In einer teilweise beobachteten Umgebung.\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/mario.jpg\" alt=\"Mario\">\n",
    "<figcaption>In Super Mario Bros. sehen wir nur den Teil des Levels, der sich in der Nähe des Spielers befindet, also erhalten wir eine Beobachtung.</figcaption>\n",
    "</figure>\n",
    "\n",
    "In Super Mario Bros. sehen wir nur den Teil des Levels, der sich in der Nähe des Spielers befindet, also erhalten wir eine Beobachtung.\n",
    "\n",
    "In Super Mario Bros. befinden wir uns in einer teilweise beobachteten Umgebung. Wir erhalten eine Beobachtung, **weil wir nur einen Teil des Levels sehen.**\n",
    "\n",
    "In diesem Kurs verwenden wir den Begriff \"Zustand\", um sowohl den Zustand als auch die Beobachtung zu bezeichnen, aber wir werden die Unterscheidung in den Implementierungen machen.\n",
    "\n",
    "Um es noch einmal zusammenzufassen:\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/obs_space_recap.jpg\" alt=\"Obs space recap\" width=\"100%\">\n",
    "\n",
    "\n",
    "## Action Space\n",
    "\n",
    "Der Aktionsraum ist die Menge **aller möglichen Aktionen in einer Umgebung.**\n",
    "\n",
    "Die Aktionen können aus einem *diskreten* oder *kontinuierlichen Raum* stammen:\n",
    "\n",
    "- *Diskreter Raum*: die Anzahl der möglichen Aktionen ist **unendlich**.\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/mario.jpg\" alt=\"Mario\">\n",
    "<figcaption>In Super Mario Bros, we have only 4 possible actions: left, right, up (jumping) and down (crouching).</figcaption>\n",
    "\n",
    "</figure>\n",
    "\n",
    "Auch in Super Mario Bros. haben wir eine endliche Anzahl von Aktionen, da wir nur 4 Richtungen haben.\n",
    "\n",
    "- *Kontinuierlicher Raum*: die Anzahl der möglichen Aktionen ist **unendlich**.\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/self_driving_car.jpg\" alt=\"Self Driving Car\">\n",
    "<figcaption>A Self Driving Car agent has an infinite number of possible actions since it can turn left 20°, 21,1°, 21,2°, honk, turn right 20°…\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Um es noch einmal zusammenzufassen:\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/action_space.jpg\" alt=\"Action space recap\" width=\"100%\">\n",
    "\n",
    "Die Berücksichtigung dieser Informationen ist von entscheidender Bedeutung, weil sie **bei der Wahl des RL-Algorithmus in der Zukunft von Bedeutung sein werden.\n",
    "\n",
    "## Belohnungen und die Diskontierung\n",
    "\n",
    "Die Belohnung ist im RL von grundlegender Bedeutung, weil sie **die einzige Rückmeldung** für den Agenten ist. Dank ihr weiß unser Agent, **ob die durchgeführte Aktion gut war oder nicht**.\n",
    "\n",
    "Die kumulative Belohnung zu jedem Zeitschritt **t** kann wie folgt geschrieben werden:\n",
    "<figure>\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/rewards_1.jpg\" alt=\"Rewards\">\n",
    "<figcaption>The cumulative reward equals the sum of all rewards in the sequence.\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Was gleichbedeutend ist mit:\n",
    "\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/rewards_2.jpg\" alt=\"Rewards\">\n",
    "<figcaption>The cumulative reward = rt+1 (rt+k+1 = rt+0+1 = rt+1)+ rt+2 (rt+k+1 = rt+1+1 = rt+2) + ...\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "In der Realität können wir sie jedoch **nicht einfach so addieren.** Die Belohnungen, die früher (zu Beginn des Spiels) kommen, **sind wahrscheinlicher**, da sie besser vorhersehbar sind als die langfristige zukünftige Belohnung.\n",
    "\n",
    "Nehmen wir an, dein Agent ist diese kleine Maus, die sich in jedem Zeitschritt um ein Plättchen bewegen kann, und dein Gegner ist die Katze (die sich ebenfalls bewegen kann). Das Ziel der Maus ist es, **die maximale Menge an Käse zu essen, bevor sie von der Katze gefressen wird**.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/rewards_3.jpg\" alt=\"Belohnungen\" width=\"100%\">\n",
    "\n",
    "Wie wir im Diagramm sehen können, **ist es wahrscheinlicher, den Käse in unserer Nähe zu essen als den Käse in der Nähe der Katze** (je näher wir der Katze sind, desto gefährlicher ist sie).\n",
    "\n",
    "Folglich wird **die Belohnung in der Nähe der Katze, auch wenn sie größer ist (mehr Käse), stärker abgewertet**, da wir nicht sicher sind, ob wir sie essen können.\n",
    "\n",
    "Um die Belohnungen zu diskontieren, gehen wir wie folgt vor:\n",
    "\n",
    "1. Wir definieren einen Abzinsungssatz namens Gamma. **Er muss zwischen 0 und 1 liegen**. Meistens liegt er zwischen **0,95 und 0,99**.\n",
    "- Je größer das Gamma, desto kleiner der Abschlag. Das bedeutet, dass unser Agent **sich mehr um die langfristige Belohnung kümmert**.\n",
    "- Andererseits, je kleiner das Gamma, desto größer der Abschlag. Das bedeutet, dass unser **Agent sich mehr um die kurzfristige Belohnung (den nächsten Käse) kümmert.**\n",
    "\n",
    "2. Dann wird jede Belohnung um gamma mit dem Exponenten des Zeitschritts abgezinst. Je größer der Zeitschritt, desto näher kommt die Katze, **so dass die zukünftige Belohnung immer unwahrscheinlicher wird.**\n",
    "\n",
    "Unsere diskontierte erwartete kumulative Belohnung ist:\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/rewards_4.jpg\" alt=\"Rewards\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Der Exploration/Exploitation-Tradeoff\n",
    "\n",
    "Bevor wir uns mit den verschiedenen Methoden zur Lösung von Reinforcement-Learning-Problemen befassen, müssen wir noch ein weiteres sehr wichtiges Thema behandeln: Der *Exploration/Exploitation Trade-off*.\n",
    "\n",
    "- *Exploration* ist das Erforschen der Umgebung durch das Ausprobieren zufälliger Aktionen, um **mehr Informationen über die Umgebung zu finden.**\n",
    "- *Ausbeutung* ist die **Ausbeutung bekannter Informationen zur Maximierung der Belohnung**.\n",
    "\n",
    "Denken Sie daran, dass das Ziel unseres RL-Agenten darin besteht, die erwartete kumulative Belohnung zu maximieren. Wir können jedoch **in eine häufige Falle tappen**.\n",
    "\n",
    "Nehmen wir ein Beispiel:\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/exp_1.jpg\" alt=\"Exploration\" width=\"100%\">\n",
    "\n",
    "In diesem Spiel kann unsere Maus eine **unendliche Anzahl von kleinen Käsen** (je +1) haben. Aber am oberen Ende des Labyrinths gibt es eine gigantische Menge Käse (+1000).\n",
    "\n",
    "Wenn wir uns jedoch nur auf die Ausbeutung konzentrieren, wird unser Agent niemals die gigantische Käsesumme erreichen. Stattdessen wird er nur **die nächstgelegene Quelle von Belohnungen** ausbeuten, selbst wenn diese Quelle klein ist (Ausbeutung).\n",
    "\n",
    "Wenn unser Agent jedoch ein wenig erkundet, kann er **die große Belohnung** (den Haufen des großen Käses) entdecken.\n",
    "\n",
    "Dies nennen wir den Kompromiss zwischen Erkundung und Ausbeutung. We need to balance how much we **explore the environment** and how much we **exploit what we know about the environment.**\n",
    "\n",
    "Deshalb müssen wir **eine Regel definieren, die uns hilft, mit diesem Kompromiss umzugehen**. Wir werden die verschiedenen Möglichkeiten, damit umzugehen, in den nächsten Einheiten sehen.\n",
    "\n",
    "Wenn es immer noch verwirrend ist, **denke an ein echtes Problem: die Wahl eines Restaurants:**\n",
    "\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/exp_2.jpg\" alt=\"Exploration\">\n",
    "<figcaption>Source: <a href=\"https://inst.eecs.berkeley.edu/~cs188/sp20/assets/lecture/lec15_6up.pdf\"> Berkley AI Course</a>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "- *Ausbeutung*: Man geht jeden Tag in das gleiche Restaurant, von dem man weiß, dass es gut ist, und **geht das Risiko ein, ein anderes, besseres Restaurant zu verpassen*.\n",
    "- *Exploration*: Probieren Sie Restaurants aus, in denen Sie noch nie waren, mit dem Risiko, eine schlechte Erfahrung zu machen, **aber mit der wahrscheinlichen Chance auf eine fantastische Erfahrung.**\n",
    "\n",
    "To recap:\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/expexpltradeoff.jpg\" alt=\"Exploration Exploitation Tradeoff\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zwei Hauptansätze zur Lösung von RL-Problemen \n",
    "\n",
    "Mit anderen Worten, wie bauen wir einen RL-Agenten, der **die Aktionen auswählen kann, die seine erwartete kumulative Belohnung maximieren?**\n",
    "\n",
    "## Die Policy π: das Gehirn des Agenten\n",
    "\n",
    "Die Policy **π** ist das **Gehirn unseres Agenten**, sie ist die Funktion, die uns sagt, welche **Aktion wir angesichts des Zustands, in dem wir uns befinden, ausführen sollen.** Sie **definiert also das Verhalten des Agenten** zu einem bestimmten Zeitpunkt.\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/policy_1.jpg\" alt=\"Policy\" />\n",
    "<figcaption>Think of policy as the brain of our agent, the function that will tell us the action to take given a state</figcaption>\n",
    "</figure>\n",
    "\n",
    "Diese Policy **ist die Funktion, die wir lernen wollen**, unser Ziel ist es, die optimale Policy π\\* zu finden, die Policy, die **den erwarteten Ertrag** maximiert, wenn der Agent danach handelt. Dieses π\\* finden wir **durch Training**.\n",
    "\n",
    "Es gibt zwei Ansätze, um unseren Agenten darauf zu trainieren, diese optimale Strategie π\\* zu finden:\n",
    "\n",
    "- **Direkt,** indem wir dem Agenten beibringen, zu lernen, welche **Handlung er angesichts des aktuellen Zustands ergreifen sollte,**: **Policy-Based Methods.**\n",
    "- Indirekt **lernen die Agenten, welcher Zustand wertvoller ist** und ergreifen dann die Maßnahmen, die **zu den wertvolleren Zuständen** führen: Wertbasierte Methoden.\n",
    "\n",
    "## Policy basierte Methoden\n",
    "\n",
    "Bei richtlinienbasierten Methoden **lernen wir direkt eine Richtlinienfunktion**.\n",
    "\n",
    "Diese Funktion definiert eine Abbildung von jedem Zustand auf die beste entsprechende Aktion. Alternativ kann sie auch **eine Wahrscheinlichkeitsverteilung über die Menge der möglichen Aktionen in diesem Zustand** definieren.\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/policy_2.jpg\" alt=\"Policy\" />\n",
    "<figcaption>As we can see here, the policy (deterministic) <b>directly indicates the action to take for each step.</b></figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "Es gibt zwei Arten von Policy:\n",
    "\n",
    "\n",
    "- *Deterministisch*: eine Policy in einem bestimmten Zustand **liefert immer die gleiche Aktion**.\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/policy_3.jpg\" alt=\"Policy\"/>\n",
    "<figcaption>action = policy(state)</figcaption>\n",
    "</figure>\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/policy_4.jpg\" alt=\"Policy\" width=\"100%\"/>\n",
    "\n",
    "- *Stochastisch*: gibt **eine Wahrscheinlichkeitsverteilung über Aktionen aus.\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/policy_5.jpg\" alt=\"Policy\"/>\n",
    "<figcaption>Policy(Aktionen | Zustand) = Wahrscheinlichkeitsverteilung über die Menge der Aktionen bei aktuellem Zustand</figcaption>\n",
    "</figure>\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/policy-based.png\" alt=\"Policy Based\"/>\n",
    "<figcaption>Ausgehend von einem Anfangszustand gibt unsere stochastische Strategie Wahrscheinlichkeitsverteilungen über die möglichen Aktionen in diesem Zustand aus.</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "Wenn wir rekapitulieren:\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/pbm_1.jpg\" alt=\"Pbm recap\" width=\"100%\" />\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/pbm_2.jpg\" alt=\"Pbm rekapitulieren\" width=\"100%\" />\n",
    "\n",
    "\n",
    "## Wertbasierte Methoden\n",
    "\n",
    "Bei wertbasierten Methoden lernen wir statt einer Policyfunktion eine **Wertfunktion**, die einen Zustand auf den erwarteten Wert **dieses Zustands abbildet.**\n",
    "\n",
    "Der Wert eines Zustands ist der **erwartete diskontierte Ertrag**, den der Agent erhalten kann, wenn er **in diesem Zustand beginnt und dann gemäß unserer Strategie handelt**.\n",
    "\n",
    "\"Gemäß unserer Policy handeln\" bedeutet lediglich, dass unsere Policy **\"den Zustand mit dem höchsten Wert anstrebt\".**\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/value_1.jpg\" alt=\"Wertbasierte RL\" width=\"100%\" />\n",
    "\n",
    "Hier sehen wir, dass unsere Wertfunktion **für jeden möglichen Zustand Werte definiert**.\n",
    "\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/value_2.jpg\" alt=\"Value based RL\"/>\n",
    "\n",
    "Dank unserer Wertfunktion wird unsere Strategie bei jedem Schritt den Zustand mit dem größten Wert auswählen, der durch die Wertfunktion definiert ist: -7, dann -6, dann -5 (und so weiter), um das Ziel zu erreichen.\n",
    "\n",
    "Wenn wir rekapitulieren:\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/vbm_1.jpg\" alt=\"Vbm recap\" width=\"100%\" />\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/vbm_2.jpg\" alt=\"Vbm recap\" width=\"100%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HoeqMnr5LuYE"
   },
   "source": [
    "## Eine kleine Zusammenfassung von Deep Reinforcement Learning 📚\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/RL_process_game.jpg\" alt=\"Der RL-Prozess\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcQYx9ynaFMD"
   },
   "source": [
    "Lassen Sie uns kurz rekapitulieren, was wir in der ersten Einheit gelernt haben:\n",
    "\n",
    "- Verstärkungslernen ist ein **computergestützter Ansatz zum Lernen aus Handlungen**. Wir bauen einen Agenten, der von der Umwelt lernt, indem er **durch Versuch und Irrtum** mit ihr interagiert und Belohnungen (negativ oder positiv) als Feedback erhält.\n",
    "\n",
    "- Das Ziel eines jeden RL-Agenten ist die **Maximierung seiner erwarteten kumulativen Belohnung** (auch erwarteter Ertrag genannt), da RL auf der _Belohnungshypothese_ basiert, die besagt, dass alle Ziele als Maximierung einer erwarteten kumulativen Belohnung beschrieben werden können.\n",
    "\n",
    "- Der RL-Prozess ist eine **Schleife, die eine Folge von Zustand, Aktion, Belohnung und nächstem Zustand** ausgibt.\n",
    "\n",
    "- Um die erwartete kumulative Belohnung (erwartete Rendite) zu berechnen, **diskontieren wir die Belohnungen**: Die Belohnungen, die früher (zu Beginn des Spiels) eintreten, sind wahrscheinlicher, da sie besser vorhersehbar sind als die langfristige zukünftige Belohnung.\n",
    "\n",
    "- Um ein RL-Problem zu lösen, wollen Sie **eine optimale Strategie** finden; die Strategie ist das \"Gehirn\" Ihrer KI, das uns sagt, welche Aktion wir in einem bestimmten Zustand ergreifen sollen. Die optimale Strategie ist diejenige, die Ihnen die Aktionen liefert, die den erwarteten Gewinn maximieren.\n",
    "\n",
    "Es gibt **zwei** Möglichkeiten, die optimale Strategie zu finden:\n",
    "1. Durch **direktes Trainieren Ihrer Strategie**: Strategie-basierte Methoden.\n",
    "2. Durch **Trainieren einer Wertfunktion**, die uns den erwarteten Ertrag angibt, den der Agent in jedem Zustand erhält, und Verwendung dieser Funktion zur Festlegung unserer Strategie: wertbasierte Methoden.\n",
    "\n",
    "\n",
    "- Schließlich haben wir über Deep RL gesprochen, weil **wir tiefe neuronale Netze einsetzen, um die zu ergreifenden Maßnahmen (policy-basiert) oder den Wert eines Zustands (wertbasiert) zu schätzen, daher der Name \"deep\" **."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qDploC3jSH99"
   },
   "source": [
    "# Trainieren wir unseren ersten Deep Reinforcement Learning-Agenten und laden wir ihn in den Hub hoch 🚀.\n",
    "\n",
    "## Get a certificate 🎓\n",
    "\n",
    "Um dieses Hands-On für den [Zertifizierungsprozess] (https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process) zu validieren, müssen Sie Ihr trainiertes Modell zum Hub hochladen und **ein Ergebnis von >= 200** erhalten.\n",
    "\n",
    "Um Ihr Ergebnis zu finden, gehen Sie zur [Bestenliste](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) und suchen Sie Ihr Modell, **das Ergebnis = mean_reward - std of reward**\n",
    "\n",
    "Weitere Informationen über den Zertifizierungsprozess finden Sie in diesem Abschnitt 👉 https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jeDAH0h0EBiG"
   },
   "source": [
    "## Abhängigkeiten installieren und einen virtuellen Bildschirm erstellen 🔽.\n",
    "\n",
    "Der erste Schritt ist die Installation der Abhängigkeiten, wir werden mehrere installieren.\n",
    "\n",
    "- `gymnasium[box2d]`: Enthält die LunarLander-v2-Umgebung 🌛.\n",
    "- `stable-baselines3[extra]`: Die Deep Reinforcement Learning-Bibliothek.\n",
    "- `huggingface_sb3`: Zusätzlicher Code für Stable-baselines3 zum Laden und Hochladen von Modellen aus dem Hugging Face 🤗 Hub.\n",
    "\n",
    "Um die Dinge zu vereinfachen, haben wir ein Skript erstellt, das all diese Abhängigkeiten installiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yQIGLPDkGhgG"
   },
   "outputs": [],
   "source": [
    "!apt install swig cmake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9XaULfDZDvrC"
   },
   "outputs": [],
   "source": [
    "!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BEKeXQJsQCYm"
   },
   "source": [
    "Während der Arbeit mit dem Notebook müssen wir ein Replay-Video erstellen. Dazu benötigen wir mit colab **einen virtuellen Bildschirm, um die Umgebung zu rendern** (und somit die Bilder aufzunehmen).\n",
    "\n",
    "Daher wird die folgende Zelle die Bibliotheken für virtuelle Bildschirme installieren und einen virtuellen Bildschirm erstellen und ausführen 🖥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j5f2cGkdP-mb"
   },
   "outputs": [],
   "source": [
    "!sudo apt-get update\n",
    "!sudo apt-get install -y python3-opengl\n",
    "!apt install ffmpeg\n",
    "!apt install xvfb\n",
    "!pip3 install pyvirtualdisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCwBTAwAW9JJ"
   },
   "source": [
    "Um sicherzustellen, dass die neu installierten Bibliotheken verwendet werden, **ist es manchmal erforderlich, die Laufzeit des Notebooks neu zu starten**. Die nächste Zelle wird die **Laufzeitumgebung zum Absturz bringen, so dass Sie eine neue Verbindung herstellen und den Code von hier aus ausführen müssen**. Dank dieses Tricks **können wir unseren virtuellen Bildschirm ausführen**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cYvkbef7XEMi"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BE5JWP5rQIKf"
   },
   "outputs": [],
   "source": [
    "# Virtual display\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "virtual_display = Display(visible=0, size=(1400, 900))\n",
    "virtual_display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wrgpVFqyENVf"
   },
   "source": [
    "## Importieren Sie die Pakete 📦\n",
    "\n",
    "Eine zusätzliche Bibliothek, die wir importieren, ist huggingface_hub **um trainierte Modelle aus dem Hub hoch- und herunterladen zu können**.\n",
    "\n",
    "\n",
    "Der Hugging Face Hub 🤗 funktioniert als zentraler Ort, an dem jeder Modelle und Datensätze teilen und erforschen kann. Er bietet Versionierung, Metriken, Visualisierungen und andere Funktionen, die eine einfache Zusammenarbeit mit anderen ermöglichen.\n",
    "\n",
    "Sie können hier alle Deep Reinforcement Learning-Modelle sehen, die hier verfügbar sind👉 https://huggingface.co/models?pipeline_tag=reinforcement-learning&sort=downloads\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cygWLPGsEQ0m"
   },
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "\n",
    "from huggingface_sb3 import load_from_hub, package_to_hub\n",
    "from huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRqRuRUl8CsB"
   },
   "source": [
    "## Verstehen Sie das Gymnasium und wie es funktioniert 🤖\n",
    "\n",
    "🏋 Die Bibliothek, die unsere Umgebung enthält, heißt Gymnasium.\n",
    "**Sie werden Gymnasium in Deep Reinforcement Learning häufig verwenden.\n",
    "\n",
    "Gymnasium ist die **neue Version der Gym-Bibliothek** [die von der Farama Foundation gepflegt wird] (https://farama.org/).\n",
    "\n",
    "Die Gymnasium-Bibliothek bietet zwei Dinge:\n",
    "\n",
    "- Eine Schnittstelle, die es erlaubt, **RL-Umgebungen** zu erstellen.\n",
    "- Eine **Sammlung von Umgebungen** (gym-control, atari, box2D...).\n",
    "\n",
    "Schauen wir uns ein Beispiel an, aber erinnern wir uns zunächst an die RL-Schleife.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/RL_process_game.jpg\" alt=\"Der RL-Prozess\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-TzNN0bQ_j-3"
   },
   "source": [
    "Bei jedem Schritt:\n",
    "- Unser Agent erhält einen **Zustand (S0)** von der **Umgebung** - wir erhalten das erste Bild unseres Spiels (Umgebung).\n",
    "- Basierend auf diesem **Zustand (S0),** führt der Agent eine **Aktion (A0)** aus - unser Agent bewegt sich nach rechts.\n",
    "- Die Umgebung geht in einen **neuen** **Zustand (S1)** über - neues Bild.\n",
    "- Die Umgebung gibt dem Agenten eine **Belohnung (R1)** - wir sind nicht tot *(Positive Belohnung +1)*.\n",
    "\n",
    "\n",
    "Mit Gymnasium:\n",
    "\n",
    "1️⃣ Wir erstellen unsere Umgebung mit `gymnasium.make()`\n",
    "\n",
    "2️⃣ Wir setzen die Umgebung mit `observation = env.reset()` auf ihren Ausgangszustand zurück.\n",
    "\n",
    "Bei jedem Schritt:\n",
    "\n",
    "3️⃣ Holen Sie sich eine Aktion mit unserem Modell (in unserem Beispiel nehmen wir eine zufällige Aktion)\n",
    "\n",
    "4️⃣ Mit `env.step(action)` führen wir diese Aktion in der Umgebung aus und erhalten\n",
    "- Beobachtung\": Der neue Zustand (st+1)\n",
    "- `Belohnung`: Die Belohnung, die wir nach dem Ausführen der Aktion erhalten\n",
    "- Beendet`: Zeigt an, ob die Episode beendet wurde (der Agent hat den Endzustand erreicht)\n",
    "- Abgeschnitten\": Mit dieser neuen Version eingeführt, zeigt es ein Zeitlimit an oder wenn ein Agent zum Beispiel die Grenzen der Umgebung verlässt.\n",
    "- `Info`: Ein Wörterbuch, das zusätzliche Informationen liefert (abhängig von der Umgebung).\n",
    "\n",
    "Für weitere Erklärungen siehe dies 👉 https://gymnasium.farama.org/api/env/#gymnasium.Env.step\n",
    "\n",
    "Wenn die Episode beendet ist:\n",
    "- Wir setzen die Umgebung mit `observation = env.reset()` auf ihren Ausgangszustand zurück.\n",
    "\n",
    "**Schauen wir uns ein Beispiel an!** Achten Sie darauf, den Code zu lesen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w7vOFlpA_ONz"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# First, we create our environment called LunarLander-v2\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "# Then we reset this environment\n",
    "observation, info = env.reset()\n",
    "\n",
    "for _ in range(20):\n",
    "  # Take a random action\n",
    "  action = env.action_space.sample()\n",
    "  print(\"Action taken:\", action)\n",
    "\n",
    "  # Do this action in the environment and get\n",
    "  # next_state, reward, terminated, truncated and info\n",
    "  observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "  # If the game is terminated (in our case we land, crashed) or truncated (timeout)\n",
    "  if terminated or truncated:\n",
    "      # Reset the environment\n",
    "      print(\"Environment is reset\")\n",
    "      observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XIrKGGSlENZB"
   },
   "source": [
    "## Die LunarLander-Umgebung erstellen 🌛 und verstehen, wie sie funktioniert\n",
    "\n",
    "### [Die Umgebung 🎮](https://gymnasium.farama.org/environments/box2d/lunar_lander/)\n",
    "\n",
    "In diesem ersten Tutorial trainieren wir unseren Agenten, einen [Lunar Lander](https://gymnasium.farama.org/environments/box2d/lunar_lander/), **um korrekt auf dem Mond zu landen**. Dazu muss der Agent lernen, **seine Geschwindigkeit und Position (horizontal, vertikal und im Winkel) anzupassen, um korrekt zu landen**.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "💡 Eine gute Angewohnheit, wenn man anfängt, eine Umgebung zu benutzen, ist es, ihre Dokumentation zu überprüfen.\n",
    "\n",
    "👉 https://gymnasium.farama.org/environments/box2d/lunar_lander/\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "poLBgRocF9aT"
   },
   "source": [
    "Schauen wir mal, wie die Umwelt aussieht:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZNPG0g_UGCfh"
   },
   "outputs": [],
   "source": [
    "# We create our environment with gym.make(\"<name_of_the_environment>\")\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "env.reset()\n",
    "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
    "print(\"Observation Space Shape\", env.observation_space.shape)\n",
    "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2MXc15qFE0M9"
   },
   "source": [
    "Mit \"Observation Space Shape (8,)\" sehen wir, dass die Beobachtung ein Vektor der Größe 8 ist, wobei jeder Wert verschiedene Informationen über den Lander enthält:\n",
    "- Horizontale Pad-Koordinate (x)\n",
    "- Vertikale Pad-Koordinate (y)\n",
    "- Horizontale Geschwindigkeit (x)\n",
    "- Vertikale Geschwindigkeit (y)\n",
    "- Winkel\n",
    "- Geschwindigkeit im Winkel\n",
    "- Wenn der Kontaktpunkt des linken Beins das Land berührt hat (boolesch)\n",
    "- Wenn der Kontaktpunkt des rechten Beins das Land berührt hat (boolesch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "We5WqOBGLoSm"
   },
   "outputs": [],
   "source": [
    "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
    "print(\"Action Space Shape\", env.action_space.n)\n",
    "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MyxXwkI2Magx"
   },
   "source": [
    "Der Aktionsraum (die Menge der möglichen Aktionen, die der Agent ausführen kann) ist diskret mit 4 verfügbaren Aktionen 🎮:\n",
    "\n",
    "- Aktion 0: Nichts tun,\n",
    "- Aktion 1: Linken Orientierungsmotor zünden,\n",
    "- Aktion 2: Zündung des Hauptmotors,\n",
    "- Aktion 3: Rechtes Orientierungstriebwerk zünden.\n",
    "\n",
    "Belohnungsfunktion (die Funktion, die bei jedem Zeitschritt eine Belohnung gibt) 💰:\n",
    "\n",
    "Nach jedem Schritt wird eine Belohnung gewährt. Die Gesamtbelohnung einer Episode ist die **Summe der Belohnungen für alle Schritte innerhalb dieser Episode**.\n",
    "\n",
    "Für jeden Schritt gibt es eine Belohnung:\n",
    "\n",
    "- Sie erhöht/verringert sich, je näher/weiter der Lander an der Landeplattform ist.\n",
    "- Sie erhöht/verringert sich, je langsamer/schneller sich der Lander bewegt.\n",
    "- Wird verringert, je stärker der Lander geneigt ist (Winkel nicht horizontal).\n",
    "- Erhöht sich um 10 Punkte für jedes Bein, das den Boden berührt.\n",
    "- Verringert sich um 0,03 Punkte pro Frame, in dem ein Seitentriebwerk gezündet wird.\n",
    "- Verringert sich um 0,3 Punkte pro Frame, in dem das Haupttriebwerk gezündet wird.\n",
    "\n",
    "Die Episode erhält eine **zusätzliche Belohnung von -100 bzw. +100 Punkten für einen Absturz bzw. eine sichere Landung**.\n",
    "\n",
    "Eine Episode gilt als **gelöst, wenn sie mindestens 200 Punkte erreicht hat.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dFD9RAFjG8aq"
   },
   "source": [
    "#### Vektorisierte Umgebung\n",
    "\n",
    "- Wir erstellen eine vektorisierte Umgebung (eine Methode zum Stapeln mehrerer unabhängiger Umgebungen zu einer einzigen Umgebung) mit 16 Umgebungen, damit **wir während des Trainings vielfältigere Erfahrungen machen können**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "99hqQ_etEy1N"
   },
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "env = make_vec_env('LunarLander-v2', n_envs=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VgrE86r5E5IK"
   },
   "source": [
    "## Das Modell erstellen 🤖\n",
    "- Wir haben unsere Umgebung untersucht und das Problem verstanden: **Fähig sein, den Lunar Lander korrekt auf dem Landeplatz zu landen, indem wir das linke und rechte Triebwerk sowie das Hauptausrichtungs-Triebwerk steuern**. Jetzt erstellen wir den Algorithmus, den wir zur Lösung dieses Problems verwenden werden 🚀.\n",
    "\n",
    "- Dazu verwenden wir unsere erste Deep-RL-Bibliothek, [Stable Baselines3 (SB3)] (https://stable-baselines3.readthedocs.io/en/master/).\n",
    "\n",
    "- SB3 ist eine Reihe von **zuverlässigen Implementierungen von Verstärkungslernalgorithmen in PyTorch**.\n",
    "\n",
    "---\n",
    "\n",
    "💡 Eine gute Angewohnheit bei der Verwendung einer neuen Bibliothek ist es, sich zuerst in die Dokumentation zu vertiefen: https://stable-baselines3.readthedocs.io/en/master/ und dann einige Tutorials auszuprobieren.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HLlClRW37Q7e"
   },
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/sb3.png\" alt=\"Stabile Grundlinien3\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HV4yiUM_9_Ka"
   },
   "source": [
    "Um dieses Problem zu lösen, werden wir SB3 **PPO** verwenden. [PPO (auch bekannt als Proximal Policy Optimization) ist einer der SOTA (State of the Art) Deep Reinforcement Learning-Algorithmen, die Sie in diesem Kurs kennenlernen werden] (https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#example%5D).\n",
    "\n",
    "PPO ist eine Kombination aus:\n",
    "- *Value-based Reinforcement Learning-Methode*: Lernen einer Action-Value-Funktion, die uns die **wertvollste zu ergreifende Aktion bei einem Zustand und einer Aktion** liefert.\n",
    "- Methode des policybasierten Verstärkungslernens*: Lernen einer Policy, die uns eine **Wahrscheinlichkeitsverteilung über Aktionen** liefert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5qL_4HeIOrEJ"
   },
   "source": [
    "Stable-Baselines3 ist einfach einzurichten:\n",
    "\n",
    "1️⃣ Sie **erstellen Ihre Umgebung** (in unserem Fall wurde das oben gemacht)\n",
    "\n",
    "2️⃣ Sie definieren das **Modell, das Sie verwenden möchten, und instanziieren dieses Modell** `model = PPO(\"MlpPolicy\")`\n",
    "\n",
    "3️⃣ Sie **trainieren den Agenten** mit `model.learn` und legen die Anzahl der Trainingszeitschritte fest\n",
    "\n",
    "```\n",
    "# Umgebung erstellen\n",
    "env = gym.make('LunarLander-v2')\n",
    "\n",
    "# Den Agenten instanziieren\n",
    "model = PPO('MlpPolicy', env, verbose=1)\n",
    "# Den Agenten trainieren\n",
    "model.learn(total_timesteps=int(2e5))\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nxI6hT1GE4-A"
   },
   "outputs": [],
   "source": [
    "# TODO: Define a PPO MlpPolicy architecture\n",
    "# We use MultiLayerPerceptron (MLPPolicy) because the input is a vector,\n",
    "# if we had frames as input we would use CnnPolicy\n",
    "model ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QAN7B0_HCVZC"
   },
   "source": [
    "#### Lösung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "543OHYDfcjK4"
   },
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "# We added some parameters to accelerate the training\n",
    "model = PPO(\n",
    "    policy = 'MlpPolicy',\n",
    "    env = env,\n",
    "    n_steps = 1024,\n",
    "    batch_size = 64,\n",
    "    n_epochs = 4,\n",
    "    gamma = 0.999,\n",
    "    gae_lambda = 0.98,\n",
    "    ent_coef = 0.01,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ClJJk88yoBUi"
   },
   "source": [
    "## Trainieren Sie den PPO-Agenten 🏃.\n",
    "- Lassen Sie uns unseren Agenten für 1.000.000 Zeitschritte trainieren, vergessen Sie nicht, die GPU auf Colab zu verwenden. Es wird ungefähr ~20min dauern, aber Sie können auch weniger Zeitschritte verwenden, wenn Sie es nur ausprobieren wollen.\n",
    "- Machen Sie während des Trainings eine ☕ Pause, die Sie sich verdient haben 🤗."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qKnYkNiVp89p"
   },
   "outputs": [],
   "source": [
    "# TODO: Train it for 1,000 timesteps\n",
    "\n",
    "# TODO: Specify file name for model and save the model to file\n",
    "model_name = \"ppo-LunarLander-v2\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1bQzQ-QcE3zo"
   },
   "source": [
    "#### Lösung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "poBCy9u_csyR"
   },
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "# Train it for 1,000,000 timesteps\n",
    "model.learn(total_timesteps=1000000)\n",
    "# Save the model\n",
    "model_name = \"ppo-LunarLander-v2\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BY_HuedOoISR"
   },
   "source": [
    "## Bewerten Sie den Agenten 📈.\n",
    "- Denken Sie daran, die Umgebung in einen [Monitor](https://stable-baselines3.readthedocs.io/en/master/common/monitor.html) zu verpacken.\n",
    "- Nun, da unser Lunar Lander-Agent trainiert ist 🚀, müssen wir seine Leistung **prüfen**.\n",
    "- Stable-Baselines3 bietet dafür eine Methode: `evaluate_policy`.\n",
    "- Um diesen Teil auszufüllen, müssen Sie [in der Dokumentation nachsehen] (https://stable-baselines3.readthedocs.io/en/master/guide/examples.html#basic-usage-training-saving-loading)\n",
    "- Im nächsten Schritt werden wir sehen, **wie Sie Ihren Agenten automatisch bewerten und freigeben können, um in einer Rangliste zu konkurrieren, aber jetzt machen wir es erst einmal selbst**\n",
    "\n",
    "\n",
    "💡 Wenn Sie Ihren Agenten bewerten, sollten Sie nicht Ihre Trainingsumgebung verwenden, sondern eine Bewertungsumgebung erstellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yRpno0glsADy"
   },
   "outputs": [],
   "source": [
    "# TODO: Evaluate the agent\n",
    "# Create a new environment for evaluation\n",
    "eval_env =\n",
    "\n",
    "# Evaluate the model with 10 evaluation episodes and deterministic=True\n",
    "mean_reward, std_reward =\n",
    "\n",
    "# Print the results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BqPKw3jt_pG5"
   },
   "source": [
    "#### Lösung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zpz8kHlt_a_m"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "eval_env = Monitor(gym.make(\"LunarLander-v2\"))\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
    "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "reBhoODwcXfr"
   },
   "source": [
    "- In meinem Fall habe ich nach dem Training von 1 Million Schritten eine mittlere Belohnung von 200,20 +/- 20,80 erhalten, was bedeutet, dass unser Mondlande-Agent bereit ist, auf dem Mond zu landen 🌛🥳."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generiere ein gif von deinem Mondlandung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import imageio\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\")\n",
    "images = []\n",
    "obs, info = env.reset()\n",
    "img = env.render()\n",
    "for i in range(350):\n",
    "    images.append(img)\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, _, _ ,_, _ = env.step(action)\n",
    "    img = env.render()\n",
    "\n",
    "imageio.mimsave('lander_ppo.gif', [np.array(img) for i, img in enumerate(images) if i%2 == 0], duration=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lander_ppo.gif','rb') as f:\n",
    "    display(Image(data=f.read(), format='png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IK_kR78NoNb2"
   },
   "source": [
    "## Veröffentliche unser trainiertes Modell auf dem Hub 🔥\n",
    "Da wir nun gesehen haben, dass wir nach dem Training gute Ergebnisse erzielt haben, können wir unser trainiertes Modell mit einer Zeile Code auf dem Hub 🤗 veröffentlichen.\n",
    "\n",
    "📚 Die Dokumentation der Bibliotheken 👉 https://github.com/huggingface/huggingface_sb3/tree/main#hugging-face--x-stable-baselines3-v20\n",
    "\n",
    "Hier ist ein Beispiel für eine Model Card (mit Space Invaders):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gs-Ew7e1gXN3"
   },
   "source": [
    "Mit `package_to_hub` **werten Sie aus, zeichnen ein Replay auf, generieren eine Modellkarte Ihres Agenten und schieben sie zum Hub**.\n",
    "\n",
    "This way:\n",
    "- Sie können **unsere Arbeit vorführen** 🔥.\n",
    "- Sie können **Ihren Agenten beim Spielen visualisieren** 👀\n",
    "- Du kannst **einen Agenten mit der Community teilen, den andere benutzen können** 💾\n",
    "- Sie können **auf eine Bestenliste 🏆 zugreifen, um zu sehen, wie gut Ihr Agent im Vergleich zu Ihren Klassenkameraden abschneidet** 👉 https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JquRrWytA6eo"
   },
   "source": [
    "Um Ihr Modell mit der Gemeinschaft teilen zu können, sind drei weitere Schritte erforderlich:\n",
    "\n",
    "1️⃣ (Falls noch nicht geschehen) erstelle ein Konto auf Hugging Face ➡ https://huggingface.co/join\n",
    "\n",
    "2️⃣ Melde dich an und speichere dann dein Authentifizierungs-Token von der Hugging Face-Website.\n",
    "- Erstellen Sie ein neues Token (https://huggingface.co/settings/tokens) **mit Schreibrolle**\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/create-token.jpg\" alt=\"HF-Token erstellen\">\n",
    "\n",
    "- Kopieren Sie das Token\n",
    "- Führen Sie die Zelle unten aus und fügen Sie das Token ein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GZiFBBlzxzxY"
   },
   "outputs": [],
   "source": [
    "notebook_login()\n",
    "!git config --global credential.helper store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_tsf2uv0g_4p"
   },
   "source": [
    "Wenn Sie kein Google Colab oder ein Jupyter Notebook verwenden möchten, müssen Sie stattdessen diesen Befehl verwenden: `huggingface-cli login`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FGNh9VsZok0i"
   },
   "source": [
    "3️⃣ Wir sind jetzt bereit, unseren trainierten Agenten mit der Funktion `package_to_hub()` an den 🤗 Hub 🔥 zu übertragen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ay24l6bqFF18"
   },
   "source": [
    "Füllen wir die Funktion `package_to_hub`:\n",
    "- model\": unser trainiertes Modell.\n",
    "- model_name\": der Name des trainierten Modells, den wir in \"model_save\" definiert haben\n",
    "- model_architecture`: die verwendete Modellarchitektur, in unserem Fall PPO\n",
    "- env_id`: der Name der Umgebung, in unserem Fall `LunarLander-v2`\n",
    "- eval_env`: die in eval_env definierte Auswertungsumgebung\n",
    "- repo_id`: der Name des Hugging Face Hub Repository, das erstellt/aktualisiert werden soll `(repo_id = {Benutzername}/{repo_name})`\n",
    "\n",
    "💡 **Ein guter Name ist {username}/{model_architecture}-{env_id}**\n",
    "\n",
    "- `commit_message`: Nachricht der Übergabe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I2E--IJu8JYq"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "from huggingface_sb3 import package_to_hub\n",
    "\n",
    "# PLACE the variables you've just defined two cells above\n",
    "# Define the name of the environment\n",
    "env_id = \"LunarLander-v2\"\n",
    "\n",
    "# TODO: Define the model architecture we used\n",
    "model_architecture = \"PPO\"\n",
    "\n",
    "## Define a repo_id\n",
    "## repo_id is the id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2\n",
    "## CHANGE WITH YOUR REPO ID\n",
    "repo_id = \"ThomasSimonini/ppo-LunarLander-v2\" # Change with your repo id, you can't push with mine 😄\n",
    "\n",
    "## Define the commit message\n",
    "commit_message = \"Upload PPO LunarLander-v2 trained agent\"\n",
    "\n",
    "# Create the evaluation env and set the render_mode=\"rgb_array\"\n",
    "eval_env = DummyVecEnv([lambda: gym.make(env_id, render_mode=\"rgb_array\")])\n",
    "\n",
    "# PLACE the package_to_hub function you've just filled here\n",
    "package_to_hub(model=model, # Our trained model\n",
    "               model_name=model_name, # The name of our trained model\n",
    "               model_architecture=model_architecture, # The model architecture we used: in our case PPO\n",
    "               env_id=env_id, # Name of the environment\n",
    "               eval_env=eval_env, # Evaluation Environment\n",
    "               repo_id=repo_id, # id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2\n",
    "               commit_message=commit_message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T79AEAWEFIxz"
   },
   "source": [
    "Herzlichen Glückwunsch 🥳 Sie haben gerade Ihren ersten Deep Reinforcement Learning-Agenten trainiert und hochgeladen. Das obige Skript sollte einen Link zu einem Modell-Repository wie https://huggingface.co/osanseviero/test_sb3 angezeigt haben. Wenn Sie auf diesen Link gehen, können Sie:\n",
    "* Auf der rechten Seite eine Videovorschau Ihres Agenten sehen.\n",
    "* Klicken Sie auf \"Dateien und Versionen\", um alle Dateien im Repository zu sehen.\n",
    "* Klicken Sie auf \"Use in stable-baselines3\", um einen Codeausschnitt zu erhalten, der zeigt, wie das Modell geladen wird.\n",
    "* Eine Modellkarte (Datei `README.md`), die eine Beschreibung des Modells enthält\n",
    "\n",
    "Unter der Haube verwendet der Hub git-basierte Repositories (keine Sorge, wenn Sie nicht wissen, was git ist), was bedeutet, dass Sie das Modell mit neuen Versionen aktualisieren können, wenn Sie experimentieren und Ihren Agenten verbessern.\n",
    "\n",
    "Vergleiche die Ergebnisse deines LunarLander-v2 mit deinen Klassenkameraden mit Hilfe des Leaderboards 🏆 👉 https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9nWnuQHRfFRa"
   },
   "source": [
    "## Laden Sie ein gespeichertes LunarLander-Modell aus dem Hub 🤗.\n",
    "Danke an [ironbar](https://github.com/ironbar) für den Beitrag.\n",
    "\n",
    "Das Laden eines gespeicherten Modells aus dem Hub ist wirklich einfach.\n",
    "\n",
    "Gehen Sie auf https://huggingface.co/models?library=stable-baselines3, um die Liste aller gespeicherten Stable-baselines3-Modelle zu sehen.\n",
    "1. Sie wählen eines aus und kopieren seine repo_id\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit1/copy-id.png\" alt=\"Copy-id\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hNPLJF2bfiUw"
   },
   "source": [
    "2. Dann müssen wir nur load_from_hub mit verwenden:\n",
    "- Die Repo_id\n",
    "- dem Dateinamen: dem gespeicherten Modell innerhalb des Repo und seiner Erweiterung (*.zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhb9-NtsinKB"
   },
   "source": [
    "Da das Modell, das ich vom Hub herunterlade, mit Gym (der früheren Version von Gymnasium) trainiert wurde, müssen wir shimmy installieren, ein API-Konvertierungstool, das uns helfen wird, die Umgebung korrekt auszuführen.\n",
    "\n",
    "Shimmy Dokumentation: https://github.com/Farama-Foundation/Shimmy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "03WI-bkci1kH"
   },
   "outputs": [],
   "source": [
    "!pip install shimmy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oj8PSGHJfwz3"
   },
   "outputs": [],
   "source": [
    "from huggingface_sb3 import load_from_hub\n",
    "repo_id = \"Classroom-workshop/assignment2-omar\" # The repo_id\n",
    "filename = \"ppo-LunarLander-v2.zip\" # The model filename.zip\n",
    "\n",
    "# When the model was trained on Python 3.8 the pickle protocol is 5\n",
    "# But Python 3.6, 3.7 use protocol 4\n",
    "# In order to get compatibility we need to:\n",
    "# 1. Install pickle5 (we done it at the beginning of the colab)\n",
    "# 2. Create a custom empty object we pass as parameter to PPO.load()\n",
    "custom_objects = {\n",
    "            \"learning_rate\": 0.0,\n",
    "            \"lr_schedule\": lambda _: 0.0,\n",
    "            \"clip_range\": lambda _: 0.0,\n",
    "}\n",
    "\n",
    "checkpoint = load_from_hub(repo_id, filename)\n",
    "model = PPO.load(checkpoint, custom_objects=custom_objects, print_system_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fs0Y-qgPgLUf"
   },
   "source": [
    "Lassen Sie uns dieses Mittel bewerten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PAEVwK-aahfx"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "eval_env = Monitor(gym.make(\"LunarLander-v2\"))\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
    "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQAwLnYFPk-s"
   },
   "source": [
    "## Einige zusätzliche Herausforderungen 🏆\n",
    "Die beste Art zu lernen **ist, Dinge selbst auszuprobieren**! Wie Sie gesehen haben, ist der derzeitige Agent nicht besonders gut. Als ersten Vorschlag können Sie für mehr Schritte trainieren. Bei 1.000.000 Schritten haben wir tolle Ergebnisse gesehen!\n",
    "\n",
    "In der [Rangliste] (https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) findest du deine Agenten. Können Sie sich an die Spitze setzen?\n",
    "\n",
    "Hier sind einige Ideen, um dies zu erreichen:\n",
    "* Trainieren Sie mehr Schritte\n",
    "* Probieren Sie verschiedene Hyperparameter für `PPO` aus. Sie können sie unter https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#parameters sehen.\n",
    "* Überprüfen Sie die [Stable-Baselines3-Dokumentation] (https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html) und versuchen Sie ein anderes Modell wie DQN.\n",
    "**Pushen Sie Ihr neu trainiertes Modell** auf den Hub 🔥.\n",
    "\n",
    "**Vergleiche die Ergebnisse deines LunarLander-v2 mit deinen Klassenkameraden** mit Hilfe der [Bestenliste](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) 🏆\n",
    "\n",
    "Ist dir die Mondlandung zu langweilig? Versuche, **die Umgebung zu verändern**, warum nicht MountainCar-v0, CartPole-v1 oder CarRacing-v0 benutzen? Schau dir an, wie sie funktionieren [mit Hilfe der Gym-Dokumentation] (https://www.gymlibrary.dev/) und hab Spaß 🎉."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9lM95-dvmif8"
   },
   "source": [
    "________________________________________________________________________\n",
    "Herzlichen Glückwunsch zum Abschluss dieses Kapitels! Das war das größte, **und es gab eine Menge Informationen.**\n",
    "\n",
    "Wenn du dich immer noch verwirrt fühlst mit all diesen Elementen...das ist völlig normal! **So ging es mir und allen Leuten, die RL studiert haben.\n",
    "\n",
    "Nimm dir Zeit, um den Stoff wirklich zu **verstehen, bevor du weitermachst und die zusätzlichen Herausforderungen versuchst**. Es ist wichtig, diese Elemente zu beherrschen und eine solide Grundlage zu haben.\n",
    "\n",
    "Natürlich werden wir im Laufe des Kurses tiefer in diese Konzepte eintauchen, aber **es ist besser, wenn man sie jetzt schon gut versteht, bevor man in die nächsten Kapitel eintaucht**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BjLhT70TEZIn"
   },
   "source": [
    "Beim nächsten Mal, in der Bonuseinheit 1, trainierst du Huggy, den Hund, um den Stock zu holen.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit1/huggy.jpg\" alt=\"Huggy\"/>\n",
    "\n",
    "## Lernt weiter, bleibt toll 🤗."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "QAN7B0_HCVZC",
    "BqPKw3jt_pG5"
   ],
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.9.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "ed7f8024e43d3b8f5ca3c5e1a8151ab4d136b3ecee1e3fd59e0766ccc55e1b10"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
