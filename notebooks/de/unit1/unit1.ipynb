{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njb_ProuHiOe"
   },
   "source": [
    "# Unit 1: Trainiere deinen ersten Deep Reinforcement Learning Agent ü§ñ.\n",
    "\n",
    "![Cover](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/thumbnail.jpg)\n",
    "\n",
    "In diesem Notizbuch werden Sie Ihren **ersten Deep Reinforcement Learning-Agenten** trainieren, einen Lunar Lander-Agenten, der lernen soll, **richtig auf dem Mond zu landen üåï**. Verwenden Sie [Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/), eine Deep Reinforcement Learning-Bibliothek, teilen Sie sie mit der Community und experimentieren Sie mit verschiedenen Konfigurationen\n",
    "\n",
    "‚¨áÔ∏è Hier ist ein Beispiel daf√ºr, was **man in nur wenigen Minuten erreichen kann** ‚¨áÔ∏è\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PF46MwbZD00b"
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<video controls autoplay><source src=\"https://huggingface.co/sb3/ppo-LunarLander-v2/resolve/main/replay.mp4\" type=\"video/mp4\"></video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x7oR6R-ZIbeS"
   },
   "source": [
    "### Die Umwelt üéÆ\n",
    "\n",
    "- [LunarLander-v2](https://gymnasium.farama.org/environments/box2d/lunar_lander/)\n",
    "\n",
    "### Die verwendete Bibliothek üìö\n",
    "\n",
    "- [Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OwEcFHe9RRZW"
   },
   "source": [
    "Wir versuchen st√§ndig, unsere Anleitungen zu verbessern. **Wenn Sie also Probleme in diesem Notizbuch** finden, √∂ffnen Sie bitte [ein Problem im Github Repo](https://github.com/huggingface/deep-rl-class/issues)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4i6tjI2tHQ8j"
   },
   "source": [
    "## Ziele dieses Notizbuchs üèÜ\n",
    "\n",
    "Am Ende des Notizbuchs werden Sie:\n",
    "\n",
    "- In der Lage sein, **Gymnasium**, die Umgebungsbibliothek, zu benutzen.\n",
    "- In der Lage sein, **Stable-Baselines3**, die Deep Reinforcement Learning-Bibliothek, zu verwenden.\n",
    "- In der Lage sein, **Ihren trainierten Agenten auf den Hub** zu pushen, mit einer sch√∂nen Videowiedergabe und einem Bewertungsergebnis üî•.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ff-nyJdzJPND"
   },
   "source": [
    "## Dieses Notebook ist aus dem Deep Reinforcement Learning Kurs\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/deep-rl-course-illustration.jpg\" alt=\"Deep RL Course illustration\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6p5HnEefISCB"
   },
   "source": [
    "In diesem kostenlosen Kurs lernen Sie:\n",
    "\n",
    "- üìñ Deep Reinforcement Learning in **Theorie und Praxis** studieren.\n",
    "- üßë‚Äçüíª Lernen Sie, **ber√ºhmte Deep RL-Bibliotheken** wie Stable Baselines3, RL Baselines3 Zoo, CleanRL und Sample Factory 2.0 zu verwenden.\n",
    "- ü§ñ Trainieren Sie **Agenten in einzigartigen Umgebungen**.\n",
    "- üéì **Erwerben Sie ein Abschlusszertifikat**, wenn Sie 80% der Aufgaben erf√ºllen.\n",
    "\n",
    "Und mehr!\n",
    "\n",
    "Pr√ºfen Sie üìö den Lehrplan üëâ https://simoninithomas.github.io/deep-rl-course\n",
    "\n",
    "Vergessen Sie nicht, sich **<a href=\"http://eepurl.com/ic5ZUD\">f√ºr den Kurs anzumelden</a>** (wir sammeln Ihre E-Mail, um Ihnen **die Links zu schicken, wenn die einzelnen Einheiten ver√∂ffentlicht werden, und Sie √ºber die Herausforderungen und Aktualisierungen zu informieren).**\n",
    "\n",
    "Der beste Weg, um in Kontakt zu bleiben und Fragen zu stellen, ist **unserem Discord-Server** beizutreten, um sich mit der Community und uns auszutauschen üëâüèª https://discord.gg/ydHrjt3WP5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-mo_6rXIjRi"
   },
   "source": [
    "## Voraussetzungen üèóÔ∏è\n",
    "\n",
    "Bevor Sie sich mit dem Notebook besch√§ftigen, m√ºssen Sie:\n",
    "\n",
    "üî≤ üìù **[Einheit 0 lesen](https://huggingface.co/deep-rl-course/unit0/introduction)**, die Ihnen alle **Informationen √ºber den Kurs gibt und Ihnen beim Einstieg hilft** ü§ó\n",
    "\n",
    "üî≤ üìö **Entwickeln Sie ein Verst√§ndnis f√ºr die Grundlagen des Verst√§rkungslernens** (MC, TD, Belohnungshypothese...), indem Sie [Einheit 1](https://huggingface.co/deep-rl-course/unit1/introduction) lesen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HoeqMnr5LuYE"
   },
   "source": [
    "## Eine kleine Zusammenfassung von Deep Reinforcement Learning üìö\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/RL_process_game.jpg\" alt=\"Der RL-Prozess\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcQYx9ynaFMD"
   },
   "source": [
    "Lassen Sie uns kurz rekapitulieren, was wir in der ersten Einheit gelernt haben:\n",
    "\n",
    "- Verst√§rkungslernen ist ein **computergest√ºtzter Ansatz zum Lernen aus Handlungen**. Wir bauen einen Agenten, der von der Umwelt lernt, indem er **durch Versuch und Irrtum** mit ihr interagiert und Belohnungen (negativ oder positiv) als Feedback erh√§lt.\n",
    "\n",
    "- Das Ziel eines jeden RL-Agenten ist die **Maximierung seiner erwarteten kumulativen Belohnung** (auch erwarteter Ertrag genannt), da RL auf der _Belohnungshypothese_ basiert, die besagt, dass alle Ziele als Maximierung einer erwarteten kumulativen Belohnung beschrieben werden k√∂nnen.\n",
    "\n",
    "- Der RL-Prozess ist eine **Schleife, die eine Folge von Zustand, Aktion, Belohnung und n√§chstem Zustand** ausgibt.\n",
    "\n",
    "- Um die erwartete kumulative Belohnung (erwartete Rendite) zu berechnen, **diskontieren wir die Belohnungen**: Die Belohnungen, die fr√ºher (zu Beginn des Spiels) eintreten, sind wahrscheinlicher, da sie besser vorhersehbar sind als die langfristige zuk√ºnftige Belohnung.\n",
    "\n",
    "- Um ein RL-Problem zu l√∂sen, wollen Sie **eine optimale Strategie** finden; die Strategie ist das \"Gehirn\" Ihrer KI, das uns sagt, welche Aktion wir in einem bestimmten Zustand ergreifen sollen. Die optimale Strategie ist diejenige, die Ihnen die Aktionen liefert, die den erwarteten Gewinn maximieren.\n",
    "\n",
    "Es gibt **zwei** M√∂glichkeiten, die optimale Strategie zu finden:\n",
    "\n",
    "- Durch **direktes Trainieren Ihrer Strategie**: Strategie-basierte Methoden.\n",
    "- Durch **Trainieren einer Wertfunktion**, die uns den erwarteten Ertrag angibt, den der Agent in jedem Zustand erh√§lt, und Verwendung dieser Funktion zur Festlegung unserer Strategie: wertbasierte Methoden.\n",
    "\n",
    "- Schlie√ülich haben wir √ºber Deep RL gesprochen, weil **wir tiefe neuronale Netze einsetzen, um die zu ergreifenden Ma√ünahmen (policy-basiert) oder den Wert eines Zustands (wertbasiert) zu sch√§tzen, daher der Name \"deep\" **."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qDploC3jSH99"
   },
   "source": [
    "# Trainieren wir unseren ersten Deep Reinforcement Learning-Agenten und laden wir ihn in den Hub hoch üöÄ.\n",
    "\n",
    "## Get a certificate üéì\n",
    "\n",
    "Um dieses Hands-On f√ºr den [Zertifizierungsprozess] (https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process) zu validieren, m√ºssen Sie Ihr trainiertes Modell zum Hub hochladen und **ein Ergebnis von >= 200** erhalten.\n",
    "\n",
    "Um Ihr Ergebnis zu finden, gehen Sie zur [Bestenliste](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) und suchen Sie Ihr Modell, **das Ergebnis = mean_reward - std of reward**\n",
    "\n",
    "Weitere Informationen √ºber den Zertifizierungsprozess finden Sie in diesem Abschnitt üëâ https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HqzznTzhNfAC"
   },
   "source": [
    "## Die GPU einstellen üí™\n",
    "\n",
    "- Um **das Training des Agenten zu beschleunigen, werden wir einen Grafikprozessor** verwenden. Gehen Sie dazu auf \"Runtime > Change Runtime type\".\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step1.jpg\" alt=\"GPU Schritt 1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38HBd3t1SHJ8"
   },
   "source": [
    "- Hardware-Beschleuniger > GPU\".\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step2.jpg\" alt=\"GPU Schritt 2\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jeDAH0h0EBiG"
   },
   "source": [
    "## Abh√§ngigkeiten installieren und einen virtuellen Bildschirm erstellen üîΩ.\n",
    "\n",
    "Der erste Schritt ist die Installation der Abh√§ngigkeiten, wir werden mehrere installieren.\n",
    "\n",
    "- `gymnasium[box2d]`: Enth√§lt die LunarLander-v2-Umgebung üåõ.\n",
    "- `stable-baselines3[extra]`: Die Deep Reinforcement Learning-Bibliothek.\n",
    "- `huggingface_sb3`: Zus√§tzlicher Code f√ºr Stable-baselines3 zum Laden und Hochladen von Modellen aus dem Hugging Face ü§ó Hub.\n",
    "\n",
    "Um die Dinge zu vereinfachen, haben wir ein Skript erstellt, das all diese Abh√§ngigkeiten installiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yQIGLPDkGhgG"
   },
   "outputs": [],
   "source": [
    "!apt install swig cmake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9XaULfDZDvrC"
   },
   "outputs": [],
   "source": [
    "!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BEKeXQJsQCYm"
   },
   "source": [
    "W√§hrend der Arbeit mit dem Notebook m√ºssen wir ein Replay-Video erstellen. Dazu ben√∂tigen wir mit colab **einen virtuellen Bildschirm, um die Umgebung zu rendern** (und somit die Bilder aufzunehmen).\n",
    "\n",
    "Daher wird die folgende Zelle die Bibliotheken f√ºr virtuelle Bildschirme installieren und einen virtuellen Bildschirm erstellen und ausf√ºhren üñ•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j5f2cGkdP-mb"
   },
   "outputs": [],
   "source": [
    "!sudo apt-get update\n",
    "!sudo apt-get install -y python3-opengl\n",
    "!apt install ffmpeg\n",
    "!apt install xvfb\n",
    "!pip3 install pyvirtualdisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCwBTAwAW9JJ"
   },
   "source": [
    "Um sicherzustellen, dass die neu installierten Bibliotheken verwendet werden, **ist es manchmal erforderlich, die Laufzeit des Notebooks neu zu starten**. Die n√§chste Zelle wird die **Laufzeitumgebung zum Absturz bringen, so dass Sie eine neue Verbindung herstellen und den Code von hier aus ausf√ºhren m√ºssen**. Dank dieses Tricks **k√∂nnen wir unseren virtuellen Bildschirm ausf√ºhren**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cYvkbef7XEMi"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BE5JWP5rQIKf"
   },
   "outputs": [],
   "source": [
    "# Virtual display\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "virtual_display = Display(visible=0, size=(1400, 900))\n",
    "virtual_display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wrgpVFqyENVf"
   },
   "source": [
    "## Importieren Sie die Pakete üì¶\n",
    "\n",
    "Eine zus√§tzliche Bibliothek, die wir importieren, ist huggingface_hub **um trainierte Modelle aus dem Hub hoch- und herunterladen zu k√∂nnen**.\n",
    "\n",
    "\n",
    "Der Hugging Face Hub ü§ó funktioniert als zentraler Ort, an dem jeder Modelle und Datens√§tze teilen und erforschen kann. Er bietet Versionierung, Metriken, Visualisierungen und andere Funktionen, die eine einfache Zusammenarbeit mit anderen erm√∂glichen.\n",
    "\n",
    "Sie k√∂nnen hier alle Deep Reinforcement Learning-Modelle sehen, die hier verf√ºgbar sindüëâ https://huggingface.co/models?pipeline_tag=reinforcement-learning&sort=downloads\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cygWLPGsEQ0m"
   },
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "\n",
    "from huggingface_sb3 import load_from_hub, package_to_hub\n",
    "from huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRqRuRUl8CsB"
   },
   "source": [
    "## Verstehen Sie das Gymnasium und wie es funktioniert ü§ñ\n",
    "\n",
    "üèã Die Bibliothek, die unsere Umgebung enth√§lt, hei√üt Gymnasium.\n",
    "**Sie werden Gymnasium in Deep Reinforcement Learning h√§ufig verwenden.\n",
    "\n",
    "Gymnasium ist die **neue Version der Gym-Bibliothek** [die von der Farama Foundation gepflegt wird] (https://farama.org/).\n",
    "\n",
    "Die Gymnasium-Bibliothek bietet zwei Dinge:\n",
    "\n",
    "- Eine Schnittstelle, die es erlaubt, **RL-Umgebungen** zu erstellen.\n",
    "- Eine **Sammlung von Umgebungen** (gym-control, atari, box2D...).\n",
    "\n",
    "Schauen wir uns ein Beispiel an, aber erinnern wir uns zun√§chst an die RL-Schleife.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/RL_process_game.jpg\" alt=\"Der RL-Prozess\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-TzNN0bQ_j-3"
   },
   "source": [
    "Bei jedem Schritt:\n",
    "- Unser Agent erh√§lt einen **Zustand (S0)** von der **Umgebung** - wir erhalten das erste Bild unseres Spiels (Umgebung).\n",
    "- Basierend auf diesem **Zustand (S0),** f√ºhrt der Agent eine **Aktion (A0)** aus - unser Agent bewegt sich nach rechts.\n",
    "- Die Umgebung geht in einen **neuen** **Zustand (S1)** √ºber - neues Bild.\n",
    "- Die Umgebung gibt dem Agenten eine **Belohnung (R1)** - wir sind nicht tot *(Positive Belohnung +1)*.\n",
    "\n",
    "\n",
    "Mit Gymnasium:\n",
    "\n",
    "1Ô∏è‚É£ Wir erstellen unsere Umgebung mit `gymnasium.make()`\n",
    "\n",
    "2Ô∏è‚É£ Wir setzen die Umgebung mit `observation = env.reset()` auf ihren Ausgangszustand zur√ºck.\n",
    "\n",
    "Bei jedem Schritt:\n",
    "\n",
    "3Ô∏è‚É£ Holen Sie sich eine Aktion mit unserem Modell (in unserem Beispiel nehmen wir eine zuf√§llige Aktion)\n",
    "\n",
    "4Ô∏è‚É£ Mit `env.step(action)` f√ºhren wir diese Aktion in der Umgebung aus und erhalten\n",
    "- Beobachtung\": Der neue Zustand (st+1)\n",
    "- `Belohnung`: Die Belohnung, die wir nach dem Ausf√ºhren der Aktion erhalten\n",
    "- Beendet`: Zeigt an, ob die Episode beendet wurde (der Agent hat den Endzustand erreicht)\n",
    "- Abgeschnitten\": Mit dieser neuen Version eingef√ºhrt, zeigt es ein Zeitlimit an oder wenn ein Agent zum Beispiel die Grenzen der Umgebung verl√§sst.\n",
    "- `Info`: Ein W√∂rterbuch, das zus√§tzliche Informationen liefert (abh√§ngig von der Umgebung).\n",
    "\n",
    "F√ºr weitere Erkl√§rungen siehe dies üëâ https://gymnasium.farama.org/api/env/#gymnasium.Env.step\n",
    "\n",
    "Wenn die Episode beendet ist:\n",
    "- Wir setzen die Umgebung mit `observation = env.reset()` auf ihren Ausgangszustand zur√ºck.\n",
    "\n",
    "**Schauen wir uns ein Beispiel an!** Achten Sie darauf, den Code zu lesen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w7vOFlpA_ONz"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# First, we create our environment called LunarLander-v2\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "# Then we reset this environment\n",
    "observation, info = env.reset()\n",
    "\n",
    "for _ in range(20):\n",
    "  # Take a random action\n",
    "  action = env.action_space.sample()\n",
    "  print(\"Action taken:\", action)\n",
    "\n",
    "  # Do this action in the environment and get\n",
    "  # next_state, reward, terminated, truncated and info\n",
    "  observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "  # If the game is terminated (in our case we land, crashed) or truncated (timeout)\n",
    "  if terminated or truncated:\n",
    "      # Reset the environment\n",
    "      print(\"Environment is reset\")\n",
    "      observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XIrKGGSlENZB"
   },
   "source": [
    "## Die LunarLander-Umgebung erstellen üåõ und verstehen, wie sie funktioniert\n",
    "\n",
    "### [Die Umgebung üéÆ](https://gymnasium.farama.org/environments/box2d/lunar_lander/)\n",
    "\n",
    "In diesem ersten Tutorial trainieren wir unseren Agenten, einen [Lunar Lander](https://gymnasium.farama.org/environments/box2d/lunar_lander/), **um korrekt auf dem Mond zu landen**. Dazu muss der Agent lernen, **seine Geschwindigkeit und Position (horizontal, vertikal und im Winkel) anzupassen, um korrekt zu landen**.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "üí° Eine gute Angewohnheit, wenn man anf√§ngt, eine Umgebung zu benutzen, ist es, ihre Dokumentation zu √ºberpr√ºfen.\n",
    "\n",
    "üëâ https://gymnasium.farama.org/environments/box2d/lunar_lander/\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "poLBgRocF9aT"
   },
   "source": [
    "Schauen wir mal, wie die Umwelt aussieht:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZNPG0g_UGCfh"
   },
   "outputs": [],
   "source": [
    "# We create our environment with gym.make(\"<name_of_the_environment>\")\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "env.reset()\n",
    "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
    "print(\"Observation Space Shape\", env.observation_space.shape)\n",
    "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2MXc15qFE0M9"
   },
   "source": [
    "Mit \"Observation Space Shape (8,)\" sehen wir, dass die Beobachtung ein Vektor der Gr√∂√üe 8 ist, wobei jeder Wert verschiedene Informationen √ºber den Lander enth√§lt:\n",
    "- Horizontale Pad-Koordinate (x)\n",
    "- Vertikale Pad-Koordinate (y)\n",
    "- Horizontale Geschwindigkeit (x)\n",
    "- Vertikale Geschwindigkeit (y)\n",
    "- Winkel\n",
    "- Geschwindigkeit im Winkel\n",
    "- Wenn der Kontaktpunkt des linken Beins das Land ber√ºhrt hat (boolesch)\n",
    "- Wenn der Kontaktpunkt des rechten Beins das Land ber√ºhrt hat (boolesch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "We5WqOBGLoSm"
   },
   "outputs": [],
   "source": [
    "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
    "print(\"Action Space Shape\", env.action_space.n)\n",
    "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MyxXwkI2Magx"
   },
   "source": [
    "Der Aktionsraum (die Menge der m√∂glichen Aktionen, die der Agent ausf√ºhren kann) ist diskret mit 4 verf√ºgbaren Aktionen üéÆ:\n",
    "\n",
    "- Aktion 0: Nichts tun,\n",
    "- Aktion 1: Linken Orientierungsmotor z√ºnden,\n",
    "- Aktion 2: Z√ºndung des Hauptmotors,\n",
    "- Aktion 3: Rechtes Orientierungstriebwerk z√ºnden.\n",
    "\n",
    "Belohnungsfunktion (die Funktion, die bei jedem Zeitschritt eine Belohnung gibt) üí∞:\n",
    "\n",
    "Nach jedem Schritt wird eine Belohnung gew√§hrt. Die Gesamtbelohnung einer Episode ist die **Summe der Belohnungen f√ºr alle Schritte innerhalb dieser Episode**.\n",
    "\n",
    "F√ºr jeden Schritt gibt es eine Belohnung:\n",
    "\n",
    "- Sie erh√∂ht/verringert sich, je n√§her/weiter der Lander an der Landeplattform ist.\n",
    "- Sie erh√∂ht/verringert sich, je langsamer/schneller sich der Lander bewegt.\n",
    "- Wird verringert, je st√§rker der Lander geneigt ist (Winkel nicht horizontal).\n",
    "- Erh√∂ht sich um 10 Punkte f√ºr jedes Bein, das den Boden ber√ºhrt.\n",
    "- Verringert sich um 0,03 Punkte pro Frame, in dem ein Seitentriebwerk gez√ºndet wird.\n",
    "- Verringert sich um 0,3 Punkte pro Frame, in dem das Haupttriebwerk gez√ºndet wird.\n",
    "\n",
    "Die Episode erh√§lt eine **zus√§tzliche Belohnung von -100 bzw. +100 Punkten f√ºr einen Absturz bzw. eine sichere Landung**.\n",
    "\n",
    "Eine Episode gilt als **gel√∂st, wenn sie mindestens 200 Punkte erreicht hat.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dFD9RAFjG8aq"
   },
   "source": [
    "#### Vektorisierte Umgebung\n",
    "\n",
    "- Wir erstellen eine vektorisierte Umgebung (eine Methode zum Stapeln mehrerer unabh√§ngiger Umgebungen zu einer einzigen Umgebung) mit 16 Umgebungen, damit **wir w√§hrend des Trainings vielf√§ltigere Erfahrungen machen k√∂nnen**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "99hqQ_etEy1N"
   },
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "env = make_vec_env('LunarLander-v2', n_envs=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VgrE86r5E5IK"
   },
   "source": [
    "## Das Modell erstellen ü§ñ\n",
    "- Wir haben unsere Umgebung untersucht und das Problem verstanden: **F√§hig sein, den Lunar Lander korrekt auf dem Landeplatz zu landen, indem wir das linke und rechte Triebwerk sowie das Hauptausrichtungs-Triebwerk steuern**. Jetzt erstellen wir den Algorithmus, den wir zur L√∂sung dieses Problems verwenden werden üöÄ.\n",
    "\n",
    "- Dazu verwenden wir unsere erste Deep-RL-Bibliothek, [Stable Baselines3 (SB3)] (https://stable-baselines3.readthedocs.io/en/master/).\n",
    "\n",
    "- SB3 ist eine Reihe von **zuverl√§ssigen Implementierungen von Verst√§rkungslernalgorithmen in PyTorch**.\n",
    "\n",
    "---\n",
    "\n",
    "üí° Eine gute Angewohnheit bei der Verwendung einer neuen Bibliothek ist es, sich zuerst in die Dokumentation zu vertiefen: https://stable-baselines3.readthedocs.io/en/master/ und dann einige Tutorials auszuprobieren.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HLlClRW37Q7e"
   },
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/sb3.png\" alt=\"Stabile Grundlinien3\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HV4yiUM_9_Ka"
   },
   "source": [
    "Um dieses Problem zu l√∂sen, werden wir SB3 **PPO** verwenden. [PPO (auch bekannt als Proximal Policy Optimization) ist einer der SOTA (State of the Art) Deep Reinforcement Learning-Algorithmen, die Sie in diesem Kurs kennenlernen werden] (https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#example%5D).\n",
    "\n",
    "PPO ist eine Kombination aus:\n",
    "- *Value-based Reinforcement Learning-Methode*: Lernen einer Action-Value-Funktion, die uns die **wertvollste zu ergreifende Aktion bei einem Zustand und einer Aktion** liefert.\n",
    "- Methode des politikbasierten Verst√§rkungslernens*: Lernen einer Politik, die uns eine **Wahrscheinlichkeitsverteilung √ºber Aktionen** liefert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5qL_4HeIOrEJ"
   },
   "source": [
    "Stable-Baselines3 ist einfach einzurichten:\n",
    "\n",
    "1Ô∏è‚É£ Sie **erstellen Ihre Umgebung** (in unserem Fall wurde das oben gemacht)\n",
    "\n",
    "2Ô∏è‚É£ Sie definieren das **Modell, das Sie verwenden m√∂chten, und instanziieren dieses Modell** `model = PPO(\"MlpPolicy\")`\n",
    "\n",
    "3Ô∏è‚É£ Sie **trainieren den Agenten** mit `model.learn` und legen die Anzahl der Trainingszeitschritte fest\n",
    "\n",
    "```\n",
    "# Umgebung erstellen\n",
    "env = gym.make('LunarLander-v2')\n",
    "\n",
    "# Den Agenten instanziieren\n",
    "model = PPO('MlpPolicy', env, verbose=1)\n",
    "# Den Agenten trainieren\n",
    "model.learn(total_timesteps=int(2e5))\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nxI6hT1GE4-A"
   },
   "outputs": [],
   "source": [
    "# TODO: Define a PPO MlpPolicy architecture\n",
    "# We use MultiLayerPerceptron (MLPPolicy) because the input is a vector,\n",
    "# if we had frames as input we would use CnnPolicy\n",
    "model ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QAN7B0_HCVZC"
   },
   "source": [
    "#### L√∂sung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "543OHYDfcjK4"
   },
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "# We added some parameters to accelerate the training\n",
    "model = PPO(\n",
    "    policy = 'MlpPolicy',\n",
    "    env = env,\n",
    "    n_steps = 1024,\n",
    "    batch_size = 64,\n",
    "    n_epochs = 4,\n",
    "    gamma = 0.999,\n",
    "    gae_lambda = 0.98,\n",
    "    ent_coef = 0.01,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ClJJk88yoBUi"
   },
   "source": [
    "## Trainieren Sie den PPO-Agenten üèÉ.\n",
    "- Lassen Sie uns unseren Agenten f√ºr 1.000.000 Zeitschritte trainieren, vergessen Sie nicht, die GPU auf Colab zu verwenden. Es wird ungef√§hr ~20min dauern, aber Sie k√∂nnen auch weniger Zeitschritte verwenden, wenn Sie es nur ausprobieren wollen.\n",
    "- Machen Sie w√§hrend des Trainings eine ‚òï Pause, die Sie sich verdient haben ü§ó."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qKnYkNiVp89p"
   },
   "outputs": [],
   "source": [
    "# TODO: Train it for 1,000,000 timesteps\n",
    "\n",
    "# TODO: Specify file name for model and save the model to file\n",
    "model_name = \"ppo-LunarLander-v2\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1bQzQ-QcE3zo"
   },
   "source": [
    "#### L√∂sung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "poBCy9u_csyR"
   },
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "# Train it for 1,000,000 timesteps\n",
    "model.learn(total_timesteps=1000000)\n",
    "# Save the model\n",
    "model_name = \"ppo-LunarLander-v2\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BY_HuedOoISR"
   },
   "source": [
    "## Bewerten Sie den Agenten üìà.\n",
    "- Denken Sie daran, die Umgebung in einen [Monitor](https://stable-baselines3.readthedocs.io/en/master/common/monitor.html) zu verpacken.\n",
    "- Nun, da unser Lunar Lander-Agent trainiert ist üöÄ, m√ºssen wir seine Leistung **pr√ºfen**.\n",
    "- Stable-Baselines3 bietet daf√ºr eine Methode: `evaluate_policy`.\n",
    "- Um diesen Teil auszuf√ºllen, m√ºssen Sie [in der Dokumentation nachsehen] (https://stable-baselines3.readthedocs.io/en/master/guide/examples.html#basic-usage-training-saving-loading)\n",
    "- Im n√§chsten Schritt werden wir sehen, **wie Sie Ihren Agenten automatisch bewerten und freigeben k√∂nnen, um in einer Rangliste zu konkurrieren, aber jetzt machen wir es erst einmal selbst**\n",
    "\n",
    "\n",
    "üí° Wenn Sie Ihren Agenten bewerten, sollten Sie nicht Ihre Trainingsumgebung verwenden, sondern eine Bewertungsumgebung erstellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yRpno0glsADy"
   },
   "outputs": [],
   "source": [
    "# TODO: Evaluate the agent\n",
    "# Create a new environment for evaluation\n",
    "eval_env =\n",
    "\n",
    "# Evaluate the model with 10 evaluation episodes and deterministic=True\n",
    "mean_reward, std_reward =\n",
    "\n",
    "# Print the results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BqPKw3jt_pG5"
   },
   "source": [
    "#### L√∂sung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zpz8kHlt_a_m"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "eval_env = Monitor(gym.make(\"LunarLander-v2\"))\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
    "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "reBhoODwcXfr"
   },
   "source": [
    "- In meinem Fall habe ich nach dem Training von 1 Million Schritten eine mittlere Belohnung von 200,20 +/- 20,80 erhalten, was bedeutet, dass unser Mondlande-Agent bereit ist, auf dem Mond zu landen üåõü•≥."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IK_kR78NoNb2"
   },
   "source": [
    "## Ver√∂ffentliche unser trainiertes Modell auf dem Hub üî•\n",
    "Da wir nun gesehen haben, dass wir nach dem Training gute Ergebnisse erzielt haben, k√∂nnen wir unser trainiertes Modell mit einer Zeile Code auf dem Hub ü§ó ver√∂ffentlichen.\n",
    "\n",
    "üìö Die Dokumentation der Bibliotheken üëâ https://github.com/huggingface/huggingface_sb3/tree/main#hugging-face--x-stable-baselines3-v20\n",
    "\n",
    "Hier ist ein Beispiel f√ºr eine Model Card (mit Space Invaders):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gs-Ew7e1gXN3"
   },
   "source": [
    "Mit `package_to_hub` **werten Sie aus, zeichnen ein Replay auf, generieren eine Modellkarte Ihres Agenten und schieben sie zum Hub**.\n",
    "\n",
    "This way:\n",
    "- Sie k√∂nnen **unsere Arbeit vorf√ºhren** üî•.\n",
    "- Sie k√∂nnen **Ihren Agenten beim Spielen visualisieren** üëÄ\n",
    "- Du kannst **einen Agenten mit der Community teilen, den andere benutzen k√∂nnen** üíæ\n",
    "- Sie k√∂nnen **auf eine Bestenliste üèÜ zugreifen, um zu sehen, wie gut Ihr Agent im Vergleich zu Ihren Klassenkameraden abschneidet** üëâ https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JquRrWytA6eo"
   },
   "source": [
    "Um Ihr Modell mit der Gemeinschaft teilen zu k√∂nnen, sind drei weitere Schritte erforderlich:\n",
    "\n",
    "1Ô∏è‚É£ (Falls noch nicht geschehen) erstelle ein Konto auf Hugging Face ‚û° https://huggingface.co/join\n",
    "\n",
    "2Ô∏è‚É£ Melde dich an und speichere dann dein Authentifizierungs-Token von der Hugging Face-Website.\n",
    "- Erstellen Sie ein neues Token (https://huggingface.co/settings/tokens) **mit Schreibrolle**\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/create-token.jpg\" alt=\"HF-Token erstellen\">\n",
    "\n",
    "- Kopieren Sie das Token\n",
    "- F√ºhren Sie die Zelle unten aus und f√ºgen Sie das Token ein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GZiFBBlzxzxY"
   },
   "outputs": [],
   "source": [
    "notebook_login()\n",
    "!git config --global credential.helper store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_tsf2uv0g_4p"
   },
   "source": [
    "Wenn Sie kein Google Colab oder ein Jupyter Notebook verwenden m√∂chten, m√ºssen Sie stattdessen diesen Befehl verwenden: `huggingface-cli login`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FGNh9VsZok0i"
   },
   "source": [
    "3Ô∏è‚É£ Wir sind jetzt bereit, unseren trainierten Agenten mit der Funktion `package_to_hub()` an den ü§ó Hub üî• zu √ºbertragen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ay24l6bqFF18"
   },
   "source": [
    "F√ºllen wir die Funktion `package_to_hub`:\n",
    "- model\": unser trainiertes Modell.\n",
    "- model_name\": der Name des trainierten Modells, den wir in \"model_save\" definiert haben\n",
    "- model_architecture`: die verwendete Modellarchitektur, in unserem Fall PPO\n",
    "- env_id`: der Name der Umgebung, in unserem Fall `LunarLander-v2`\n",
    "- eval_env`: die in eval_env definierte Auswertungsumgebung\n",
    "- repo_id`: der Name des Hugging Face Hub Repository, das erstellt/aktualisiert werden soll `(repo_id = {Benutzername}/{repo_name})`\n",
    "\n",
    "üí° **Ein guter Name ist {username}/{model_architecture}-{env_id}**\n",
    "\n",
    "- `commit_message`: Nachricht der √úbergabe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JPG7ofdGIHN8"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "from huggingface_sb3 import package_to_hub\n",
    "\n",
    "## TODO: Define a repo_id\n",
    "## repo_id is the id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2\n",
    "repo_id =\n",
    "\n",
    "# TODO: Define the name of the environment\n",
    "env_id =\n",
    "\n",
    "# Create the evaluation env and set the render_mode=\"rgb_array\"\n",
    "eval_env = DummyVecEnv([lambda: Monitor(gym.make(env_id, render_mode=\"rgb_array\"))])\n",
    "\n",
    "\n",
    "# TODO: Define the model architecture we used\n",
    "model_architecture = \"\"\n",
    "\n",
    "## TODO: Define the commit message\n",
    "commit_message = \"\"\n",
    "\n",
    "# method save, evaluate, generate a model card and record a replay video of your agent before pushing the repo to the hub\n",
    "package_to_hub(model=model, # Our trained model\n",
    "               model_name=model_name, # The name of our trained model\n",
    "               model_architecture=model_architecture, # The model architecture we used: in our case PPO\n",
    "               env_id=env_id, # Name of the environment\n",
    "               eval_env=eval_env, # Evaluation Environment\n",
    "               repo_id=repo_id, # id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2\n",
    "               commit_message=commit_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Avf6gufJBGMw"
   },
   "source": [
    "#### L√∂sung\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I2E--IJu8JYq"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "from huggingface_sb3 import package_to_hub\n",
    "\n",
    "# PLACE the variables you've just defined two cells above\n",
    "# Define the name of the environment\n",
    "env_id = \"LunarLander-v2\"\n",
    "\n",
    "# TODO: Define the model architecture we used\n",
    "model_architecture = \"PPO\"\n",
    "\n",
    "## Define a repo_id\n",
    "## repo_id is the id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2\n",
    "## CHANGE WITH YOUR REPO ID\n",
    "repo_id = \"ThomasSimonini/ppo-LunarLander-v2\" # Change with your repo id, you can't push with mine üòÑ\n",
    "\n",
    "## Define the commit message\n",
    "commit_message = \"Upload PPO LunarLander-v2 trained agent\"\n",
    "\n",
    "# Create the evaluation env and set the render_mode=\"rgb_array\"\n",
    "eval_env = DummyVecEnv([lambda: gym.make(env_id, render_mode=\"rgb_array\")])\n",
    "\n",
    "# PLACE the package_to_hub function you've just filled here\n",
    "package_to_hub(model=model, # Our trained model\n",
    "               model_name=model_name, # The name of our trained model\n",
    "               model_architecture=model_architecture, # The model architecture we used: in our case PPO\n",
    "               env_id=env_id, # Name of the environment\n",
    "               eval_env=eval_env, # Evaluation Environment\n",
    "               repo_id=repo_id, # id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2\n",
    "               commit_message=commit_message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T79AEAWEFIxz"
   },
   "source": [
    "Herzlichen Gl√ºckwunsch ü•≥ Sie haben gerade Ihren ersten Deep Reinforcement Learning-Agenten trainiert und hochgeladen. Das obige Skript sollte einen Link zu einem Modell-Repository wie https://huggingface.co/osanseviero/test_sb3 angezeigt haben. Wenn Sie auf diesen Link gehen, k√∂nnen Sie:\n",
    "* Auf der rechten Seite eine Videovorschau Ihres Agenten sehen.\n",
    "* Klicken Sie auf \"Dateien und Versionen\", um alle Dateien im Repository zu sehen.\n",
    "* Klicken Sie auf \"Use in stable-baselines3\", um einen Codeausschnitt zu erhalten, der zeigt, wie das Modell geladen wird.\n",
    "* Eine Modellkarte (Datei `README.md`), die eine Beschreibung des Modells enth√§lt\n",
    "\n",
    "Unter der Haube verwendet der Hub git-basierte Repositories (keine Sorge, wenn Sie nicht wissen, was git ist), was bedeutet, dass Sie das Modell mit neuen Versionen aktualisieren k√∂nnen, wenn Sie experimentieren und Ihren Agenten verbessern.\n",
    "\n",
    "Vergleiche die Ergebnisse deines LunarLander-v2 mit deinen Klassenkameraden mit Hilfe des Leaderboards üèÜ üëâ https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9nWnuQHRfFRa"
   },
   "source": [
    "## Laden Sie ein gespeichertes LunarLander-Modell aus dem Hub ü§ó.\n",
    "Danke an [ironbar](https://github.com/ironbar) f√ºr den Beitrag.\n",
    "\n",
    "Das Laden eines gespeicherten Modells aus dem Hub ist wirklich einfach.\n",
    "\n",
    "Gehen Sie auf https://huggingface.co/models?library=stable-baselines3, um die Liste aller gespeicherten Stable-baselines3-Modelle zu sehen.\n",
    "1. Sie w√§hlen eines aus und kopieren seine repo_id\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit1/copy-id.png\" alt=\"Copy-id\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hNPLJF2bfiUw"
   },
   "source": [
    "2. Dann m√ºssen wir nur load_from_hub mit verwenden:\n",
    "- Die Repo_id\n",
    "- dem Dateinamen: dem gespeicherten Modell innerhalb des Repo und seiner Erweiterung (*.zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhb9-NtsinKB"
   },
   "source": [
    "Da das Modell, das ich vom Hub herunterlade, mit Gym (der fr√ºheren Version von Gymnasium) trainiert wurde, m√ºssen wir shimmy installieren, ein API-Konvertierungstool, das uns helfen wird, die Umgebung korrekt auszuf√ºhren.\n",
    "\n",
    "Shimmy Dokumentation: https://github.com/Farama-Foundation/Shimmy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "03WI-bkci1kH"
   },
   "outputs": [],
   "source": [
    "!pip install shimmy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oj8PSGHJfwz3"
   },
   "outputs": [],
   "source": [
    "from huggingface_sb3 import load_from_hub\n",
    "repo_id = \"Classroom-workshop/assignment2-omar\" # The repo_id\n",
    "filename = \"ppo-LunarLander-v2.zip\" # The model filename.zip\n",
    "\n",
    "# When the model was trained on Python 3.8 the pickle protocol is 5\n",
    "# But Python 3.6, 3.7 use protocol 4\n",
    "# In order to get compatibility we need to:\n",
    "# 1. Install pickle5 (we done it at the beginning of the colab)\n",
    "# 2. Create a custom empty object we pass as parameter to PPO.load()\n",
    "custom_objects = {\n",
    "            \"learning_rate\": 0.0,\n",
    "            \"lr_schedule\": lambda _: 0.0,\n",
    "            \"clip_range\": lambda _: 0.0,\n",
    "}\n",
    "\n",
    "checkpoint = load_from_hub(repo_id, filename)\n",
    "model = PPO.load(checkpoint, custom_objects=custom_objects, print_system_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fs0Y-qgPgLUf"
   },
   "source": [
    "Lassen Sie uns dieses Mittel bewerten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PAEVwK-aahfx"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "eval_env = Monitor(gym.make(\"LunarLander-v2\"))\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
    "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQAwLnYFPk-s"
   },
   "source": [
    "## Einige zus√§tzliche Herausforderungen üèÜ\n",
    "Die beste Art zu lernen **ist, Dinge selbst auszuprobieren**! Wie Sie gesehen haben, ist der derzeitige Agent nicht besonders gut. Als ersten Vorschlag k√∂nnen Sie f√ºr mehr Schritte trainieren. Bei 1.000.000 Schritten haben wir tolle Ergebnisse gesehen!\n",
    "\n",
    "In der [Rangliste] (https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) findest du deine Agenten. K√∂nnen Sie sich an die Spitze setzen?\n",
    "\n",
    "Hier sind einige Ideen, um dies zu erreichen:\n",
    "* Trainieren Sie mehr Schritte\n",
    "* Probieren Sie verschiedene Hyperparameter f√ºr `PPO` aus. Sie k√∂nnen sie unter https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#parameters sehen.\n",
    "* √úberpr√ºfen Sie die [Stable-Baselines3-Dokumentation] (https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html) und versuchen Sie ein anderes Modell wie DQN.\n",
    "**Pushen Sie Ihr neu trainiertes Modell** auf den Hub üî•.\n",
    "\n",
    "**Vergleiche die Ergebnisse deines LunarLander-v2 mit deinen Klassenkameraden** mit Hilfe der [Bestenliste](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) üèÜ\n",
    "\n",
    "Ist dir die Mondlandung zu langweilig? Versuche, **die Umgebung zu ver√§ndern**, warum nicht MountainCar-v0, CartPole-v1 oder CarRacing-v0 benutzen? Schau dir an, wie sie funktionieren [mit Hilfe der Gym-Dokumentation] (https://www.gymlibrary.dev/) und hab Spa√ü üéâ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9lM95-dvmif8"
   },
   "source": [
    "________________________________________________________________________\n",
    "Herzlichen Gl√ºckwunsch zum Abschluss dieses Kapitels! Das war das gr√∂√üte, **und es gab eine Menge Informationen.**\n",
    "\n",
    "Wenn du dich immer noch verwirrt f√ºhlst mit all diesen Elementen...das ist v√∂llig normal! **So ging es mir und allen Leuten, die RL studiert haben.\n",
    "\n",
    "Nimm dir Zeit, um den Stoff wirklich zu **verstehen, bevor du weitermachst und die zus√§tzlichen Herausforderungen versuchst**. Es ist wichtig, diese Elemente zu beherrschen und eine solide Grundlage zu haben.\n",
    "\n",
    "Nat√ºrlich werden wir im Laufe des Kurses tiefer in diese Konzepte eintauchen, aber **es ist besser, wenn man sie jetzt schon gut versteht, bevor man in die n√§chsten Kapitel eintaucht**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BjLhT70TEZIn"
   },
   "source": [
    "Beim n√§chsten Mal, in der Bonuseinheit 1, trainierst du Huggy, den Hund, um den Stock zu holen.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit1/huggy.jpg\" alt=\"Huggy\"/>\n",
    "\n",
    "## Lernt weiter, bleibt toll ü§ó."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "QAN7B0_HCVZC",
    "BqPKw3jt_pG5"
   ],
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.9.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "ed7f8024e43d3b8f5ca3c5e1a8151ab4d136b3ecee1e3fd59e0766ccc55e1b10"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
